"""
æœ¬æ–‡ä»¶ç”¨äºå°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼ŒåŒ…æ‹¬å¯¹è¯ã€æ‘˜è¦ã€åˆ†æä¸å‘é‡åŒ–ç­‰èƒ½åŠ›ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `AIService`: AI èƒ½åŠ›å°è£…ï¼ˆä¸»/å¤‡æ¨¡å‹åˆ‡æ¢ã€å¹¶å‘æ§åˆ¶ã€å¤±è´¥é™çº§ï¼‰
- `ai_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import asyncio
import json
import re
import logging
from typing import Any, AsyncIterator, Dict, List, Optional, Tuple

import aiohttp
from openai import AsyncOpenAI, APIStatusError, RateLimitError, APIConnectionError

from app.core.config import get_settings
from app.core.logger import setup_logger
from app.core.exceptions import AIConfigurationError
from app.utils.tools import normalize_regions_to_countries

settings = get_settings()
logger = setup_logger("AIService")


class AIService:
    """
    è¾“å…¥:
    - AI é…ç½®ï¼ˆä¸»/å¤‡æ¨¡å‹ã€å¹¶å‘é™åˆ¶ã€Embedding é…ç½®ï¼‰

    è¾“å‡º:
    - å¤§æ¨¡å‹å¯¹è¯/æ‘˜è¦/åˆ†æç»“æœï¼Œä»¥åŠå‘é‡åŒ–ç»“æœ

    ä½œç”¨:
    - å°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼Œæä¾›ç»Ÿä¸€ã€å¯é™çº§çš„è°ƒç”¨å…¥å£
    """

    def __init__(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå§‹åŒ–ä¸»/å¤‡é€šé“å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        """

        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)

    def reload_config(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - é‡æ–°åŠ è½½å…¨å±€é…ç½®ï¼ˆç”¨äºé…ç½®æ›´æ–°ååˆ·æ–°æœ¬åœ°å¼•ç”¨ï¼‰
        """
        global settings
        from app.core.config import get_settings
        settings = get_settings()
        
        # é‡æ–°åˆå§‹åŒ–ä¿¡å·é‡ï¼ˆå¹¶å‘é…ç½®å¯èƒ½æ”¹å˜ï¼‰
        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)
        logger.info("ğŸ”„ AIService é…ç½®å·²åˆ·æ–°")

    def _has_main_llm(self) -> bool:
        return bool((settings.MAIN_AI_API_KEY or "").strip()) and bool((settings.MAIN_AI_BASE_URL or "").strip()) and bool((settings.MAIN_AI_MODEL or "").strip())

    def _has_backup_llm(self) -> bool:
        return bool((settings.BACKUP_AI_API_KEY or "").strip()) and bool((settings.BACKUP_AI_BASE_URL or "").strip()) and bool((settings.BACKUP_AI_MODEL or "").strip())

    def _has_embedding(self) -> bool:
        return bool((settings.SILICONFLOW_API_KEY or "").strip()) and bool((settings.SILICONFLOW_BASE_URL or "").strip()) and bool((settings.EMBEDDING_MODEL or "").strip())

    def _iter_llm_routes(self, prefer_backup: bool) -> List[Dict[str, str]]:
        routes: List[Dict[str, str]] = []
        if prefer_backup:
            if self._has_backup_llm():
                routes.append(
                    {
                        "base_url": str(settings.BACKUP_AI_BASE_URL),
                        "api_key": str(settings.BACKUP_AI_API_KEY),
                        "model": str(settings.BACKUP_AI_MODEL),
                        "type": "backup",
                    }
                )
            if self._has_main_llm():
                routes.append(
                    {
                        "base_url": str(settings.MAIN_AI_BASE_URL),
                        "api_key": str(settings.MAIN_AI_API_KEY),
                        "model": str(settings.MAIN_AI_MODEL),
                        "type": "main",
                    }
                )
            return routes

        if self._has_main_llm():
            routes.append(
                {
                    "base_url": str(settings.MAIN_AI_BASE_URL),
                    "api_key": str(settings.MAIN_AI_API_KEY),
                    "model": str(settings.MAIN_AI_MODEL),
                    "type": "main",
                }
            )
        if self._has_backup_llm():
            routes.append(
                {
                    "base_url": str(settings.BACKUP_AI_BASE_URL),
                    "api_key": str(settings.BACKUP_AI_API_KEY),
                    "model": str(settings.BACKUP_AI_MODEL),
                    "type": "backup",
                }
            )
        return routes

    async def _call_llm(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: str,
        system: str = "",
        semaphore: asyncio.Semaphore | None = None,
    ) -> Optional[str]:
        """
        è¾“å…¥:
        - `client`: OpenAI å…¼å®¹å®¢æˆ·ç«¯
        - `model`: æ¨¡å‹åç§°
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system`: ç³»ç»Ÿæç¤ºè¯
        - `semaphore`: å¹¶å‘æ§åˆ¶ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹è¿”å›æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - ç»Ÿä¸€å°è£… LLM è°ƒç”¨ã€å¹¶å‘æ§åˆ¶ä¸å¼‚å¸¸å¤„ç†
        """

        try:
            extra_body = {}
            if "modelscope" in str(client.base_url):
                extra_body["enable_thinking"] = False

            if semaphore is None:
                if str(settings.MAIN_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.main_sem
                elif str(settings.BACKUP_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.backup_sem

            # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æç¤ºè¯
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ”µ [LLM è¯·æ±‚] æ¨¡å‹: {model}\nç³»ç»Ÿæç¤ºè¯: {system}\nç”¨æˆ·æç¤ºè¯: {prompt[:2000]}...")

            async def do_call():
                return await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=0.6,
                    timeout=120,
                    extra_body=extra_body if extra_body else None,
                )

            # é‡è¯•é€»è¾‘
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if semaphore:
                        async with semaphore:
                            response = await do_call()
                    else:
                        response = await do_call()
                    
                    content = response.choices[0].message.content
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"ğŸŸ¢ [LLM å“åº”] æ¨¡å‹: {model}\nå†…å®¹: {content[:2000]}...")
                    
                    if not content:
                         logger.warning(f"âš ï¸ AI è¿”å›å†…å®¹ä¸ºç©º ({model})")

                    return content
                
                except (RateLimitError, APIConnectionError) as e:
                    if attempt == max_retries - 1:
                        raise e
                    wait_time = 2 * (attempt + 1)
                    logger.warning(f"âš ï¸ AI è°ƒç”¨å—é™æˆ–ç½‘ç»œæ³¢åŠ¨ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯• ({attempt + 1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                
                except APIStatusError as e:
                    # 401: API Key æ— æ•ˆ - è‡´å‘½é”™è¯¯
                    if e.status_code == 401:
                        logger.error(f"âŒ AI è®¤è¯å¤±è´¥ (401) - API Key æ— æ•ˆ ({model}): {e}")
                        raise AIConfigurationError(f"AI API Key æ— æ•ˆ ({model})")

                    # 400 Bad Request é€šå¸¸æ„å‘³ç€å†…å®¹è¿‡æ»¤æˆ–å‚æ•°æ— æ•ˆ
                    if e.status_code == 400:
                        logger.warning(f"âŒ AI è¯·æ±‚è¢«æ‹’ç» (400) - å¯èƒ½è§¦å‘æ•æ„Ÿè¯è¿‡æ»¤ ({model}): {e}")
                        return None # æ•…éšœè½¬ç§»åˆ°ä¸‹ä¸€ä¸ªè·¯ç”±
                    
                    # æœåŠ¡ç«¯é”™è¯¯ï¼Œé‡è¯•å¯èƒ½æœ‰æ•ˆ
                    if e.status_code >= 500:
                        if attempt == max_retries - 1:
                            raise e
                        wait_time = 2 * (attempt + 1)
                        logger.warning(f"âš ï¸ AI æœåŠ¡ç«¯é”™è¯¯ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯•")
                        await asyncio.sleep(wait_time)
                    else:
                        raise e

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ AI è°ƒç”¨å¼‚å¸¸ ({model}): {e}")
            return None

    async def chat_completion(self, prompt: str, system_prompt: str = "", route_key: str = None) -> str:
        """
        è¾“å…¥:
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system_prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        - `route_key`: é…ç½®è·¯ç”±é”®ï¼ˆå¯é€‰ï¼Œå¦‚ "REPORT", "SUMMARY" ç­‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹å›å¤æ–‡æœ¬ï¼ˆä¿è¯è¿”å›å­—ç¬¦ä¸²ï¼‰

        ä½œç”¨:
        - æ‰§è¡Œä¸€æ¬¡å¯¹è¯è¡¥å…¨ï¼›æ ¹æ® route_key é€‰æ‹©ä¸»/å¤‡é€šé“ç­–ç•¥
        """
        prefer_backup = False
        if route_key:
            prefer_backup = self._get_prefer_backup(route_key)
        
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res

        return "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆè¯·å…ˆåœ¨ç®¡ç†é¡µå®Œå–„ AI é…ç½®ï¼‰"

    async def batch_evaluate_topic_quality(self, topics: List[Dict[str, str]], existing_topics: List[Dict[str, str]] = None) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - topics: ä¸“é¢˜åˆ—è¡¨ [{"name":..., "description":...}]
        - existing_topics: ç°æœ‰ä¸“é¢˜åˆ—è¡¨ [{"name":..., "description":...}] ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - ç»è¿‡ç­›é€‰çš„æœ‰æ•ˆä¸“é¢˜åˆ—è¡¨

        ä½œç”¨:
        - æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡ï¼Œè¿‡æ»¤æ‰è¿‡äºå®½æ³›ã€éå…·ä½“äº‹ä»¶ã€æˆ–çº¯ç²¹è¡Œä¸šè¶‹åŠ¿çš„ä¸“é¢˜
        - æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ä¸“é¢˜é‡å¤æˆ–å±äºç°æœ‰ä¸“é¢˜çš„å»¶ä¼¸
        """
        if not topics:
            return []

        logger.info(f"ğŸ¤– æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡: {len(topics)} ä¸ª")
        
        existing_info = ""
        if existing_topics:
            # é™åˆ¶ç°æœ‰ä¸“é¢˜æ•°é‡ä»¥é˜²æ­¢ Prompt è¿‡é•¿ï¼Œå–æœ€è¿‘ 50 ä¸ªå³å¯ï¼ˆå‡è®¾æŒ‰æ—¶é—´å€’åºæˆ–ç›¸å…³æ€§æ’åºï¼‰
            # è¿™é‡Œè°ƒç”¨è€…ä¼ å…¥çš„é€šå¸¸æ˜¯ active ä¸“é¢˜ï¼Œæ•°é‡å¯èƒ½è¾ƒå¤šï¼Œæˆªå–ä¸€ä¸‹æ¯”è¾ƒå®‰å…¨
            limit_existing = existing_topics[:50]
            existing_info = "ã€å·²å­˜åœ¨çš„ä¸“é¢˜åˆ—è¡¨ï¼ˆç”¨äºæŸ¥é‡å’Œåˆ¤æ–­å»¶ä¼¸å…³ç³»ï¼‰ã€‘:\n"
            for t in limit_existing:
                existing_info += f"- {t.get('name')}: {t.get('description')}\n"
            existing_info += "\n"

        # è·å–è´¨é‡ç­‰çº§ï¼Œé»˜è®¤ä¸º 3
        quality_level = getattr(settings, "TOPIC_QUALITY_LEVEL", 3)
        logger.info(f"ğŸ” ä¸“é¢˜å®¡æ ¸è´¨é‡ç­‰çº§: {quality_level}")

        criteria_map = {
            1: (
                "1. ã€åŸºç¡€äº‹ä»¶æ ‡å‡†ã€‘ï¼ˆæ»¡è¶³å³å¯ï¼‰ï¼š\n"
                "   - **çœŸå®æ€§**ï¼šå¿…é¡»æ˜¯çœŸå®å‘ç”Ÿçš„æ–°é—»äº‹ä»¶ï¼Œè€Œéè™šæ„æˆ–çº¯ç²¹çš„è§‚ç‚¹ã€‚\n"
                "   - **å…·ä½“æ€§**ï¼šæœ‰æ˜ç¡®çš„ä¸»ä½“å’ŒåŠ¨ä½œã€‚\n"
                "   - **æ’é™¤é¡¹**ï¼šæ’é™¤æå…¶çç¢çš„ä¸ªäººæ—¥å¸¸ï¼ˆå¦‚â€œæŸäººåƒäº†é¥­â€ï¼‰ã€‚å…è®¸åœ°æ–¹æ€§å°äº‹ä»¶ã€‚\n"
            ),
            2: (
                "1. ã€ä¸€èˆ¬äº‹ä»¶æ ‡å‡†ã€‘ï¼ˆæ»¡è¶³å³å¯ï¼‰ï¼š\n"
                "   - **å…¬å…±ä»·å€¼**ï¼šå…·æœ‰æ˜¾è‘—çš„å…¬å…±ä¿¡æ¯ä»·å€¼ï¼Œéçº¯ç²¹çš„ç§äººçäº‹ã€‚\n"
                "   - **å…·ä½“æ€§**ï¼šäº‹ä»¶æè¿°æ¸…æ™°ï¼Œéæ¨¡ç³Šçš„è¡Œä¸šæ¦‚å¿µã€‚\n"
                "   - **è®¨è®ºåº¦**ï¼šå¼•èµ·äº†ä¸€å®šèŒƒå›´å†…çš„è®¨è®ºæˆ–å…³æ³¨ï¼Œéæ— äººé—®æ´¥çš„ä¿¡æ¯ã€‚\n"
                "   - **æ’é™¤é¡¹**ï¼šä¸¥æ ¼æ’é™¤å•†ä¸šå¹¿å‘Šã€æå°èŒƒå›´çš„æ— å…³ç´§è¦äº‹æ•…ã€å¸¸è§„çš„æ—¥å¸¸ä¾‹è¡ŒæŠ¥é“ã€‚\n"
            ),
            3: (
                "1. ã€é‡å¤§äº‹ä»¶è¯„ä¼°æ ‡å‡†ã€‘ï¼ˆå¿…é¡»å…¨éƒ¨æ»¡è¶³ï¼‰ï¼š\n"
                "   - **å½±å“èŒƒå›´**ï¼šå…·æœ‰çœçº§ã€å…¨å›½çº§åˆ«çš„å½±å“åŠ›ï¼Œæˆ–åœ¨æ‰€å±è¡Œä¸šå†…å…·æœ‰é‡å¤§å½±å“ã€‚æ’é™¤ä»…å½±å“ä¸ªåˆ«å…¬å¸æˆ–å°åŒºçš„äº‹ä»¶ã€‚\n"
                "   - **æŒç»­æ€§**ï¼šå…·æœ‰æŒç»­å‘é…µçš„æ½œåŠ›ï¼Œä¸ä»…ä»…æ˜¯æ˜™èŠ±ä¸€ç°çš„ç¬é—´æ¶ˆæ¯ã€‚\n"
                "   - **ç¤¾ä¼šçƒ­åº¦**ï¼šå±äºå½“å‰ç¤¾ä¼šçƒ­ç‚¹ï¼Œå…·æœ‰å¹¿æ³›çš„å…¬ä¼—è®¨è®ºåº¦ã€‚\n"
                "   - **ç‰¹æ®Šæ”¾è¡Œ**ï¼šå¯¹äºæ¶‰åŠé‚»å›½å†²çªã€å¤–äº¤çº·äº‰ç­‰äº‹ä»¶ï¼ˆå¦‚â€œæ³°æŸ¬å†²çªâ€ã€â€œæœéŸ©å±€åŠ¿â€ï¼‰ï¼Œå±äº**å›½é™…æ”¿æ²»**èŒƒç•´ï¼Œæ— è®ºçƒ­åº¦å¦‚ä½•ï¼Œéƒ½åº”è§†ä¸ºå…·æœ‰å›½é™…å½±å“åŠ›çš„æ½œåŠ›äº‹ä»¶äºˆä»¥é€šè¿‡ã€‚\n"
            ),
            4: (
                "1. ã€é«˜å½±å“åŠ›äº‹ä»¶æ ‡å‡†ã€‘ï¼ˆå¿…é¡»å…¨éƒ¨æ»¡è¶³ï¼‰ï¼š\n"
                "   - **å…¨å›½/å›½é™…å½±å“**ï¼šå¿…é¡»æ˜¯å…¨å›½æ€§æˆ–å›½é™…æ€§çš„é‡å¤§æ–°é—»ã€‚åœ°æ–¹æ€§æ–°é—»ï¼ˆé™¤éå¼•å‘å…¨å›½å…³æ³¨ï¼‰ä¸€å¾‹ä¸äºˆé€šè¿‡ã€‚\n"
                "   - **é«˜çƒ­åº¦**ï¼šå¿…é¡»æ˜¯å½“å‰å…¬ä¼—è®¨è®ºçš„ç„¦ç‚¹ã€‚\n"
                "   - **æ·±åº¦**ï¼šäº‹ä»¶å…·æœ‰æ·±è¿œçš„ç¤¾ä¼šæˆ–è¡Œä¸šå½±å“ï¼Œéç®€å•çš„æ—¥å¸¸é€šæŠ¥ã€‚\n"
            ),
            5: (
                "1. ã€é¡¶çº§é‡å¤§äº‹ä»¶æ ‡å‡†ã€‘ï¼ˆå¿…é¡»å…¨éƒ¨æ»¡è¶³ï¼‰ï¼š\n"
                "   - **å†å²çº§/æˆ˜ç•¥çº§**ï¼šæ¶‰åŠå›½å®¶æ”¿ç­–é‡å¤§è°ƒæ•´ã€å›½é™…å…³ç³»é‡å¤§å˜åŒ–ã€æˆ–æ”¹å˜è¡Œä¸šæ ¼å±€çš„å†å²æ€§äº‹ä»¶ã€‚\n"
                "   - **æé«˜çƒ­åº¦**ï¼šå…¨ç½‘åˆ·å±çº§çš„è¶…çº§çƒ­ç‚¹ã€‚\n"
                "   - **æä¸¥ç­›é€‰**ï¼šå®ç¼ºæ¯‹æ»¥ï¼Œéæ­¤ç±»ç‰¹å¤§äº‹ä»¶ä¸€å¾‹ä¸äºˆé€šè¿‡ã€‚\n"
            )
        }
        
        # é»˜è®¤ä½¿ç”¨ç­‰çº§ 3
        criteria_text = criteria_map.get(quality_level, criteria_map[3])

        # æ„å»ºç¦æ­¢é¡¹
        forbidden_common = (
            "   - ä¸èƒ½ç”±å¤šä¸ªéå¼ºå…³è”ä¸»ä½“ç»„æˆçš„ä¸¤ä¸ªäº‹ä»¶åˆå¹¶åçš„çŸ­è¯­ï¼ˆå¦‚â€œæ—¥æ–¹å®˜å‘˜æ‹¥æ ¸è¨€è®ºåŠé–å›½ç¥ç¤¾äº‰è®®â€ï¼‰ã€‚\n"
            "3. ã€æŸ¥é‡ä¸å»¶ä¼¸ã€‘ï¼š\n"
            "   - å¦‚æœå¾…åˆ›å»ºä¸“é¢˜æ˜¯ç°æœ‰ä¸“é¢˜çš„å­é›†æˆ–é‡å¤ï¼Œæ ‡è®°ä¸ºæ— æ•ˆï¼ˆæˆ–å»ºè®®åˆå¹¶ï¼Œæ­¤å¤„ç›´æ¥è®¾ä¸º invalidï¼‰ã€‚\n"
        )
        
        forbidden_strict = (
            "2. ã€ç¦æ­¢ç”Ÿæˆçš„æƒ…å†µã€‘ï¼š\n"
            "   - **åœ°æ–¹æ€§æ—¥å¸¸äº‹åŠ¡**ï¼šå¦‚æŸåœ°å±…æ°‘çº çº·ã€æ™®é€šæ²»å®‰æ¡ˆä»¶ã€å°èŒƒå›´å¤©æ°”é¢„æŠ¥ã€‚\n"
            "   - **éå…¬å…±åˆ©ç›Šçš„å•†ä¸šè¡Œä¸º**ï¼šå¦‚æŸå…¬å¸å¸¸è§„ä¿ƒé”€ã€äº§å“å°ç‰ˆæœ¬æ›´æ–°ã€æ™®é€šäººäº‹å˜åŠ¨ã€‚\n"
            "   - **è¿‡äºå®½æ³›**ï¼šå¦‚â€œè¿‘æœŸç»æµå½¢åŠ¿â€ã€â€œç§‘æŠ€æ–°é—»æ±‡æ€»â€ç­‰éå…·ä½“äº‹ä»¶ã€‚\n"
        )
        
        forbidden_loose = (
            "2. ã€ç¦æ­¢ç”Ÿæˆçš„æƒ…å†µã€‘ï¼š\n"
            "   - **è¿‡äºå®½æ³›**ï¼šå¦‚â€œä»Šæ—¥æ–°é—»â€ã€â€œè´¢ç»å¿«è®¯â€ç­‰å®Œå…¨æ— å…·ä½“å†…å®¹çš„æ ‡é¢˜ã€‚\n"
            "   - **çº¯ç²¹å¹¿å‘Š**ï¼šæ— æ–°é—»ä»·å€¼çš„çº¯æ¨é”€å†…å®¹ã€‚\n"
        )

        if quality_level >= 3:
            forbidden_text = forbidden_strict + forbidden_common
        else:
            forbidden_text = forbidden_loose + forbidden_common

        system_prompt = (
            f"ä½ æ˜¯ä¸“é¢˜è´¨é‡å®¡æ ¸å‘˜ï¼ˆå½“å‰å®¡æ ¸ç­‰çº§ï¼š{quality_level}/5ï¼‰ã€‚è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹å¾…åˆ›å»ºçš„ä¸“é¢˜æ˜¯å¦ç¬¦åˆæ ‡å‡†ã€‚\n"
            f"åˆ¤å®šæ ‡å‡†ï¼š\n"
            f"{criteria_text}"
            f"{forbidden_text}"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'index' (æ•´æ•°), 'valid' (å¸ƒå°”å€¼), 'reason' (ç®€çŸ­ç†ç”±)ã€‚\n"
            "ä¾‹å¦‚ï¼š[{\"index\": 0, \"valid\": true, \"reason\": \"ç¬¦åˆæ ‡å‡†...\"}, {\"index\": 1, \"valid\": false, \"reason\": \"ä¸ç¬¦åˆ...\"}]"
        )
        
        user_prompt = f"{existing_info}å¾…å®¡æ ¸ä¸“é¢˜åˆ—è¡¨ï¼š\n"
        for i, t in enumerate(topics):
            user_prompt += f"[{i}] åç§°ï¼š{t.get('name')}\n    æè¿°ï¼š{t.get('description')}\n\n"
            
        prefer_backup = self._get_prefer_backup("TOPIC_EVAL")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            logger.warning("âš ï¸ ä¸“é¢˜è´¨é‡è¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤å…¨éƒ¨ä¿ç•™")
            return topics
            
        try:
            clean = res.replace("```json", "").replace("```", "").strip()
            # å°è¯•æå– JSON æ•°ç»„
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1:
                clean = clean[start : end + 1]
                
            results = json.loads(clean)
            
            valid_topics = []
            if isinstance(results, list):
                for item in results:
                    idx = item.get("index")
                    is_valid = item.get("valid")
                    reason = item.get("reason", "æ— ç†ç”±")
                    
                    if isinstance(idx, int) and 0 <= idx < len(topics):
                        topic_name = topics[idx].get("name")
                        if is_valid:
                            logger.info(f"   âœ… [é€šè¿‡] {topic_name}: {reason}")
                            valid_topics.append(topics[idx])
                        else:
                            logger.info(f"   âŒ [æ‹’ç»] {topic_name}: {reason}")
            else:
                # å¦‚æœç»“æ„é”™è¯¯åˆ™é™çº§å¤„ç†
                logger.warning("   âš ï¸ è´¨é‡è¯„ä¼°è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè§£æå¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰")
                return topics

            if len(valid_topics) < len(topics):
                removed_count = len(topics) - len(valid_topics)
                logger.info(f"ğŸ—‘ï¸ è¿‡æ»¤æ‰äº† {removed_count} ä¸ªå®½æ³›/ä½è´¨é‡ä¸“é¢˜")
                
            return valid_topics
            
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æè´¨é‡è¯„ä¼°ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return topics

    async def propose_topics_from_titles(self, titles: List[str]) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - `titles`: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        
        è¾“å‡º:
        - æç‚¼å‡ºçš„ä¸“é¢˜åˆ—è¡¨ [{"name": "...", "description": "..."}, ...]
        
        ä½œç”¨:
        - ä»å¤§é‡æ ‡é¢˜ä¸­èšåˆå‡ºæ ¸å¿ƒä¸“é¢˜
        """
        if not titles:
            return []
            
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œèšåˆå‡ºè¿‘æœŸå‘ç”Ÿçš„å…·ä½“ã€å…·æœ‰ä¸€å®šæŒç»­æ€§çš„ä¸“é¢˜äº‹ä»¶ã€‚"
        
        # é™åˆ¶æ•°é‡ä»¥é˜²è¶…é•¿
        limit_n = settings.TOPIC_AGGREGATION_TOP_N
        titles_subset = titles[:limit_n]
        titles_str = "\n".join([f"- {t}" for t in titles_subset])
        
        # åŠ¨æ€è·å–ä¸“é¢˜æ•°é‡èŒƒå›´
        count_range = settings.TOPIC_GENERATION_COUNT or "1-5"
        min_count, max_count = 1, 5
        try:
            parts = count_range.split("-")
            if len(parts) == 2:
                min_count = int(parts[0].strip())
                max_count = int(parts[1].strip())
        except Exception:
            pass

        # è·å–è´¨é‡ç­‰çº§ï¼Œé»˜è®¤ä¸º 3
        quality_level = getattr(settings, "TOPIC_QUALITY_LEVEL", 3)
        logger.info(f"ğŸ” ä¸“é¢˜ç”Ÿæˆè´¨é‡ç­‰çº§: {quality_level}")

        criteria_desc = ""
        if quality_level <= 1:
            criteria_desc = (
                "**ç”Ÿæˆæ ‡å‡†ï¼ˆå®½æ¾ï¼‰**ï¼š\n"
                "1. **å…¨é¢è¦†ç›–**ï¼šè¯†åˆ«æ‰€æœ‰çœŸå®çš„ã€å…·ä½“çš„ã€æœ‰ä¸€å®šä»·å€¼çš„æ–°é—»äº‹ä»¶ï¼Œ**åŒ…æ‹¬åœ°æ–¹æ€§äº‹ä»¶**ã€‚\n"
                "2. **å…·ä½“æ€§**ï¼šå¿…é¡»æŒ‡å‘å…·ä½“çš„äº‹ä»¶ï¼Œè€Œéå®½æ³›çš„æ¦‚å¿µã€‚\n"
            )
        elif quality_level == 2:
            criteria_desc = (
                "**ç”Ÿæˆæ ‡å‡†ï¼ˆè¾ƒå®½æ¾ï¼‰**ï¼š\n"
                "1. **å…¬å…±ä»·å€¼**ï¼šè¯†åˆ«å…·æœ‰ä¸€å®šå…¬å…±ä¿¡æ¯ä»·å€¼çš„æ–°é—»äº‹ä»¶ã€‚\n"
                "2. **æ’é™¤çç¢**ï¼šå¿½ç•¥çº¯ç²¹çš„ä¸ªäººçäº‹æˆ–æ— å…³ç´§è¦çš„å°äº‹æ•…ã€‚\n"
                "3. **å…è®¸åœ°æ–¹æ€§**ï¼šå…è®¸é‡è¦çš„åœ°æ–¹æ€§ç¤¾ä¼šæ–°é—»ã€‚\n"
            )
        elif quality_level == 3:
            criteria_desc = (
                "**ç”Ÿæˆæ ‡å‡†ï¼ˆæ ‡å‡†ï¼‰**ï¼š\n"
                "1. **å¿…é¡»ç”Ÿæˆ**ï¼šæ¶‰åŠ**å›½é™…å†²çªã€å¤–äº¤çº·äº‰ã€æˆ˜äº‰**çš„äº‹ä»¶ï¼ˆæ— è®ºçƒ­åº¦å¦‚ä½•ï¼Œå¦‚â€œæ³°æŸ¬å†²çªâ€ï¼‰ã€‚\n"
                "2. **é‡å¤§ç¤¾ä¼šçƒ­ç‚¹**ï¼šå…·æœ‰å…¨å›½æ€§å½±å“ä¸”è®¨è®ºæ¿€çƒˆçš„ç¤¾ä¼šäº‹ä»¶ã€‚\n"
                "3. **ä¸¥æ ¼æ’é™¤ï¼ˆå³ä½¿æœ‰çƒ­åº¦ä¹Ÿä¸ç”Ÿæˆï¼‰**ï¼š\n"
                "   - **å¸¸è§„ç»æµ/é‡‘èæ³¢åŠ¨**ï¼ˆå¦‚æ±‡ç‡æ¶¨è·Œã€è‚¡å¸‚æ³¢åŠ¨ï¼‰ã€‚\n"
                "   - **åŸºç¡€è®¾æ–½å»ºè®¾/é€šè½¦**ï¼ˆå¦‚é«˜é“å¼€é€šã€å¤§æ¡¥åˆé¾™ï¼‰ã€‚\n"
                "   - **ä¸€èˆ¬æ€§è‡ªç„¶ç¾å®³**ï¼ˆæœªé€ æˆé‡å¤§äººå‘˜ä¼¤äº¡æˆ–æ¬¡ç”Ÿç¾å®³çš„åœ°éœ‡/å¤©æ°”ï¼‰ã€‚\n"
                "   - **æ”¿ç­–å¾æ±‚æ„è§/å¸¸è§„å‘å¸ƒ**ï¼ˆéæ­£å¼è½åœ°æˆ–å¼•å‘å·¨å¤§äº‰è®®çš„æ”¿ç­–ï¼‰ã€‚\n"
            )
        elif quality_level == 4:
            criteria_desc = (
                "**ç”Ÿæˆæ ‡å‡†ï¼ˆä¸¥æ ¼ï¼‰**ï¼š\n"
                "1. **é«˜å½±å“åŠ›**ï¼šå¿…é¡»æ˜¯**å…¨å›½æ€§æˆ–å›½é™…æ€§**çš„é‡å¤§æ–°é—»ã€‚\n"
                "2. **é«˜çƒ­åº¦**ï¼šå¿…é¡»æ˜¯å½“å‰å…¬ä¼—è®¨è®ºçš„ç„¦ç‚¹ã€‚\n"
                "3. **ä¸¥æ ¼æ’é™¤**ï¼šå¿½ç•¥æ‰€æœ‰åœ°æ–¹æ€§æ–°é—»ï¼ˆé™¤éå¼•å‘å…¨å›½å…³æ³¨ï¼‰å’Œä¸€èˆ¬æ€§è¡Œä¸šåŠ¨æ€ã€‚\n"
            )
        else: # >= 5
            criteria_desc = (
                "**ç”Ÿæˆæ ‡å‡†ï¼ˆæä¸¥ï¼‰**ï¼š\n"
                "1. **é¡¶çº§äº‹ä»¶**ï¼šä»…èšåˆ**å†å²çº§ã€æˆ˜ç•¥çº§æˆ–å…¨ç½‘åˆ·å±çº§**çš„è¶…çº§äº‹ä»¶ã€‚\n"
                "2. **å®ç¼ºæ¯‹æ»¥**ï¼šéæ­¤ç±»ç‰¹å¤§äº‹ä»¶ä¸€å¾‹ä¸ç”Ÿæˆã€‚\n"
                "3. **æ·±åº¦å½±å“**ï¼šå…³æ³¨æ¶‰åŠå›½å®¶æ”¿ç­–ã€å›½é™…å…³ç³»å·¨å˜æˆ–è¡Œä¸šæ ¼å±€æ”¹å˜çš„äº‹ä»¶ã€‚\n"
            )

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼ˆå·²æ ‡æ³¨çƒ­åº¦ï¼‰ï¼Œè¯†åˆ«å‡º {min_count} è‡³ {max_count} ä¸ªï¼ˆä¸¥æ ¼é™åˆ¶æ•°é‡ï¼‰å…·ä½“çš„ã€å…·æœ‰ä¸»é¢˜æ€§çš„çƒ­é—¨ä¸“é¢˜äº‹ä»¶ã€‚

{criteria_desc}

**ä¸“é¢˜å‘½åæ ‡å‡†åŒ–æŒ‡å—ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š

1. **æ ‡é¢˜ç»“æ„å…¬å¼**ï¼š
   - **äº‹ä»¶å‹**ï¼š`[æ ¸å¿ƒä¸»ä½“] + [äº‹ä»¶æ€§è´¨] + [èšåˆåç¼€]`
      * **èšåˆåç¼€**ï¼ˆå¿…é¡»åŒ…å«ï¼‰ï¼š**å…¨çºªå½•ã€è¿›ç¨‹ã€å§‹æœ«ã€é£æ³¢ã€äº‰è®®ã€åç»­å½±å“ã€æœ€æ–°è¿›å±•ã€ç¾æƒ…åŠæ•‘æ´**
      * âŒ **é”™è¯¯ç¤ºä¾‹**ï¼ˆç¦æ­¢åƒå•æ¡æ–°é—»æ ‡é¢˜ï¼‰ï¼š
        - æ³°æŸ¬ç­¾ç½²åœç«è”åˆå£°æ˜
      * âœ… **æ­£ç¡®ç¤ºä¾‹**ï¼ˆå¿…é¡»ä½“ç°ä¸“é¢˜æ€§ï¼‰ï¼š
        - æ³°æŸ¬è¾¹å¢ƒå†²çªå§‹æœ«
      
    - **ä¼šè®®/æ´»åŠ¨å‹**ï¼š`[å¹´ä»½] + [æ´»åŠ¨å…¨ç§°/ç®€ç§°] + [æ ¸å¿ƒçœ‹ç‚¹]`
      * ä¾‹ï¼š2024è‹¹æœç§‹å­£å‘å¸ƒä¼šï¼šiPhone16ç³»åˆ—å‘å¸ƒ
    
    - **è´Ÿé¢/äº‰è®®å‹**ï¼š`[ä¸»ä½“] + [äº‰è®®è¡Œä¸º] + [åŠå½±å“]`
     * ä¾‹ï¼šé«˜å¸‚æ—©è‹—é”™è¯¯è¨€è®ºåŠå½±å“ / æ—¥æœ¬æ ¸æ±¡æ°´æ’æµ·å…¨è®°å½•
 
 2. **æ ¸å¿ƒåŸåˆ™**ï¼š
    - **æ‹’ç»åŠ¨ä½œæè¿°**ï¼šä¸è¦ä½¿ç”¨â€œæŸäººåšäº†æŸäº‹â€æˆ–â€œæŸäº‹å‘ç”Ÿâ€è¿™ç§é™ˆè¿°å¥ä½œä¸ºä¸“é¢˜åã€‚
    - **å¿…é¡»ä½“ç°æ—¶é—´è·¨åº¦**ï¼šä¸“é¢˜ååº”æš—ç¤ºè¿™æ˜¯ä¸€ç³»åˆ—äº‹ä»¶çš„é›†åˆï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªç¬é—´åŠ¨ä½œã€‚
    - **å­—æ•°é™åˆ¶**ï¼š8-20 ä¸ªæ±‰å­—ã€‚
    - **å®¢è§‚ä¸­ç«‹**ï¼šé¿å…æƒ…ç»ªåŒ–å½¢å®¹è¯ã€‚

 3. **å…³é”®è¦æ±‚**ï¼š
    - **ä¸¥æ ¼éµå®ˆæ•°é‡é™åˆ¶**ï¼šè¾“å‡ºçš„ä¸“é¢˜æ•°é‡å¿…é¡»åœ¨ {min_count} åˆ° {max_count} ä¹‹é—´ã€‚
    - **ä¼˜å…ˆé«˜çƒ­åº¦**ï¼šä¼˜å…ˆèšåˆçƒ­åº¦é«˜ã€æŠ¥é“é‡å¤§çš„äº‹ä»¶ã€‚
    - **æ‹’ç»å®å¤§å™äº‹**ï¼šä¸è¦ç”Ÿæˆâ€œå›½é™…åœ°ç¼˜æ”¿æ²»â€ã€â€œç§‘æŠ€æ–°é—»æ±‡æ€»â€ç­‰å®½æ³›åç§°ã€‚
    - **æ‹’ç»å•æ¡æ–°é—»æ ‡é¢˜**ï¼šå¦‚æœç”Ÿæˆçš„åç§°çœ‹èµ·æ¥åƒä¸€æ¡æ–°é—»æ ‡é¢˜ï¼Œè¯·ç«‹å³é‡å†™ï¼ŒåŠ ä¸Šâ€œå…¨çºªå½•â€ã€â€œè¿›ç¨‹â€ã€â€œå§‹æœ«â€ç­‰åç¼€ã€‚
    - **å®ä½“é™å®š**ï¼šåç§°ä¸­å¿…é¡»åŒ…å«æ ¸å¿ƒå®ä½“ï¼ˆäººåã€åœ°åã€æœºæ„åï¼‰ã€‚
    - **æ—¶é—´èŒƒå›´**ï¼šä¸“é¢˜äº‹ä»¶å¿…é¡»ä»æœ€è¿‘çš„æ—¶é—´çª—å£å†…è·å–ã€‚
    - **ä¸è¦ç”Ÿæˆ**ï¼šéé‡å¤§ã€éæŒç»­æ€§äº‹ä»¶ä¸“é¢˜ã€‚

æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š
{titles_str}

è¯·ä»…è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"name": "ä¸“é¢˜åç§°", "description": "ä¸“é¢˜æè¿°"}},
  ...
]
ä¸è¦åŒ…å«ä»»ä½• Markdown ä»£ç å—æ ‡è®°æˆ–å…¶ä»–æ–‡å­—ã€‚
"""
        logger.info(f"ğŸ¤– æ­£åœ¨ä» {len(titles_subset)} æ¡æ ‡é¢˜ä¸­æç‚¼ä¸“é¢˜...")
        prefer_backup = self._get_prefer_backup("TOPIC_NAME")
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        
        try:
            if not res:
                return []
            cleaned = res.replace("```json", "").replace("```", "").strip()
            
            # å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯ï¼ˆå¦‚å°¾éƒ¨é€—å·ã€æœªè½¬ä¹‰å¼•å·ç­‰ï¼‰
            # è¿™é‡Œå…ˆç®€å•å°è¯•ç›´æ¥è§£æ
            try:
                data = json.loads(cleaned)
            except json.JSONDecodeError:
                # å‡å¦‚è§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æå–
                import re
                pattern = r'\{\s*"name"\s*:\s*"(.*?)"\s*,\s*"description"\s*:\s*"(.*?)"\s*\}'
                matches = re.findall(pattern, cleaned, re.DOTALL)
                if matches:
                    data = [{"name": m[0], "description": m[1]} for m in matches]
                else:
                    # å†æ¬¡å°è¯•ï¼Œå¯èƒ½æ˜¯å•å¼•å·æˆ–å…¶ä»–æ ¼å¼
                    raise 

            if isinstance(data, list):
                valid_data = []
                for item in data:
                    if isinstance(item, dict) and "name" in item and "description" in item:
                        valid_data.append(item)
                
                # å†æ¬¡å¼ºåˆ¶æˆªæ–­ï¼Œé˜²æ­¢ AI è¿”å›è¿‡å¤š
                if len(valid_data) > max_count:
                    logger.warning(f"âš ï¸ AI è¿”å›ä¸“é¢˜æ•°é‡ ({len(valid_data)}) è¶…è¿‡é™åˆ¶ ({max_count})ï¼Œå·²å¼ºåˆ¶æˆªæ–­")
                    valid_data = valid_data[:max_count]
                
                logger.info(f"âœ… AI æç‚¼å‡º {len(valid_data)} ä¸ªæ½œåœ¨ä¸“é¢˜")
                return valid_data
            return []
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æä¸“é¢˜æç‚¼ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def extract_news_info(self, content: str) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `content`: åŸå§‹é¡µé¢å†…å®¹ï¼ˆHTML/XML/æ–‡æœ¬ï¼‰

        è¾“å‡º:
        - æ–°é—»æ¡ç›®åˆ—è¡¨ï¼ˆtitle/link/summaryï¼‰

        ä½œç”¨:
        - å½“å¸¸è§„ RSS/API è§£æå¤±è´¥æ—¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹ä»å†…å®¹ä¸­æŠ½å–æ–°é—»æ¡ç›®
        """

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–æ–°é—»æ¡ç›®ã€‚\n"
            "è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å« 'items' é”®ï¼Œå¯¹åº”ä¸€ä¸ªåˆ—è¡¨ã€‚åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š\n"
            "- 'title': æ–°é—»æ ‡é¢˜\n"
            "- 'link': æ–°é—»é“¾æ¥\n"
            "- 'summary': æ–°é—»æ‘˜è¦ï¼ˆå¦‚æœå†…å®¹ä¸­åŒ…å«æ‘˜è¦åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™åŸºäºæ ‡é¢˜ç”Ÿæˆç®€çŸ­è¯´æ˜ï¼‰\n"
            "å¦‚æœæ— æ³•æå–ï¼Œè¯·è¿”å› {'items': []}ã€‚"
        )
        user_prompt = f"å†…å®¹ï¼š\n{content[:20000]}"

        async def try_extract(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                return data.get("items", [])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.warning(f"AIæå–ç»“æœè§£æå¤±è´¥: {e}")
                return None

        if not (self._has_main_llm() or self._has_backup_llm()):
            return []

        routes = self._iter_llm_routes(self._get_prefer_backup("SUMMARY"))
        for r_idx, route in enumerate(routes):
            max_attempts = 3 if r_idx == 0 else 1
            for attempt in range(max_attempts):
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    res = await try_extract(client, route["model"])
                if res is not None:
                    return res
                await asyncio.sleep(1)

        return []

    def _normalize_category(self, raw_category: str) -> str:
        """
        è¾“å…¥:
        - `raw_category`: æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†ç±»

        è¾“å‡º:
        - è§„èŒƒåŒ–åçš„åˆ†ç±»åç§°ï¼ˆè½åˆ° `settings.NEWS_CATEGORIES` ä¹‹ä¸€ï¼‰

        ä½œç”¨:
        - å°†æ¨¡å‹å¯èƒ½å‡ºç°çš„è¿‘ä¼¼åˆ†ç±»æ˜ å°„ä¸ºç³»ç»Ÿå†…ç½®åˆ†ç±»ï¼Œå‡å°‘è„æ•°æ®
        """

        if not raw_category:
            return "å…¶ä»–"
        if raw_category in settings.NEWS_CATEGORIES:
            return raw_category
        for cat in settings.NEWS_CATEGORIES:
            if raw_category in cat or cat in raw_category:
                return cat
        return "å…¶ä»–"

    async def analyze_sentiment(self, title: str, content: str = "") -> Dict:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ‘˜è¦æˆ–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æƒ…æ„Ÿåˆ†æç»“æœï¼ˆscore/label/category/region/keywords/entitiesï¼‰

        ä½œç”¨:
        - å¯¹å•æ¡æ–°é—»è¿›è¡Œæ·±åº¦èˆ†æƒ…åˆ†æï¼Œä¸»é€šé“å¤±è´¥æ—¶é™çº§åˆ°å¤‡ç”¨é€šé“
        """

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–°é—»æ ‡é¢˜å’Œå†…å®¹ï¼ˆæ‘˜è¦ï¼‰ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š\n"
            "1. æƒ…æ„Ÿå€¾å‘æ ‡ç­¾ (label): åªèƒ½æ˜¯ 'æ­£é¢'ã€'ä¸­ç«‹' æˆ– 'è´Ÿé¢'ã€‚\n"
            "2. æƒ…æ„Ÿåˆ†æ•° (score): 0åˆ°100ä¹‹é—´çš„æ•´æ•°ã€‚0ä»£è¡¨æåº¦è´Ÿé¢ï¼Œ50ä»£è¡¨ä¸­ç«‹ï¼Œ100ä»£è¡¨æåº¦æ­£é¢ã€‚\n"
            f"3. æ‰€å±é¢†åŸŸ (category): å¿…é¡»ä¸¥æ ¼ä»[{categories_str}]ä¸­é€‰æ‹©æœ€åˆé€‚çš„1ä¸ªé¢†åŸŸï¼Œç¦æ­¢åˆ›é€ æ–°åˆ†ç±»ã€ç¦æ­¢ä½¿ç”¨â€œå…¶ä»–â€ä½œä¸ºæ‰€å±é¢†åŸŸã€‚\n"
            "4. æ¶‰åŠå›½å®¶ (region): åªèƒ½è¾“å‡ºå›½å®¶åç§°ï¼ˆä¸€ä¸ªæˆ–å¤šä¸ªï¼‰ï¼Œç¦æ­¢è¾“å‡ºçœ/å¸‚/åŒº/å¿ç­‰è¡Œæ”¿åŒºåˆ’ï¼Œç¦æ­¢è¾“å‡ºâ€œä¸œäºš/æ¬§æ´²/ä¸­ä¸œâ€ç­‰å¤§åŒºã€‚å…è®¸ç¤ºä¾‹ï¼š'ä¸­å›½'ã€'ç¾å›½'ã€'æ—¥æœ¬'ã€'éŸ©å›½'ã€'ä¿„ç½—æ–¯'ã€'è‹±å›½'ã€'æ³•å›½'ã€'å¾·å›½'ã€'å°åº¦'ã€'åŠ æ‹¿å¤§'ã€'æ¾³å¤§åˆ©äºš'ç­‰ã€‚å¦‚æœæ¶‰åŠå¤šä¸ªå›½å®¶ï¼Œè¯·ç”¨é€—å·åˆ†éš”ï¼ˆå¦‚'ä¸­å›½,ç¾å›½'ï¼‰ã€‚å¦‚æœç¡®å®æ— æ³•åˆ¤æ–­ï¼Œè¯·è¾“å‡º'å…¨çƒ'ã€‚\n"
            "5. å…³é”®è¯ (keywords): æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆå®è¯ï¼‰ï¼Œæ’é™¤'çš„'ã€'äº†'ç­‰åœç”¨è¯ã€‚\n"
            "6. æ¶‰åŠå®ä½“ (entities): æå–æ–°é—»ä¸­æ¶‰åŠçš„äººåã€å…¬å¸åã€ç»„ç»‡æœºæ„åç­‰ã€‚\n"
            "è¿”å›æ ¼å¼å¿…é¡»æ˜¯åˆæ³•çš„ JSON å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š\n"
            "{\n"
            '  "score": 85,\n'
            '  "label": "æ­£é¢",\n'
            '  "category": "ç§‘æŠ€/ç§‘å­¦",\n'
            '  "region": "ä¸­å›½",\n'
            '  "keywords": ["äººå·¥æ™ºèƒ½", "åˆ›æ–°", "å‘å¸ƒ"],\n'
            '  "entities": ["OpenAI", "Sam Altman"]\n'
            "}"
        )
        user_prompt = f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:1000]}"

        async def try_analyze(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                if "score" in data and "label" in data:
                    data["category"] = self._normalize_category(data.get("category", ""))
                    data["region"] = normalize_regions_to_countries(data.get("region"))
                    if not data.get("region") or data.get("region") in ["å…¶ä»–", "æœªçŸ¥"]:
                        data["region"] = "å…¨çƒ"
                    return data
            except AIConfigurationError:
                raise
            except Exception:
                pass
            return None

        routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
        for route in routes:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await try_analyze(client, route["model"])
            if res:
                return res

        return {
            "score": 50,
            "label": "ä¸­ç«‹",
            "category": "å…¶ä»–",
            "region": "å…¶ä»–",
            "keywords": [],
            "entities": [],
        }

    async def batch_analyze_sentiment(self, news_items: List[Dict]) -> Dict[int, Dict]:
        """
        è¾“å…¥:
        - `news_items`: å¾…åˆ†ææ–°é—»åˆ—è¡¨ï¼ˆè‡³å°‘åŒ…å« id/titleï¼‰

        è¾“å‡º:
        - `id -> åˆ†æç»“æœ` çš„æ˜ å°„

        ä½œç”¨:
        - å¯¹å¤šæ¡æ–°é—»è¿›è¡Œæ‰¹é‡å¿«é€Ÿåˆ†æï¼Œç”¨äºæå‡ååä¸é™ä½æˆæœ¬
        """

        if not news_items:
            return {}

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–°é—»æ ‡é¢˜ï¼Œå¿«é€Ÿåˆ¤æ–­å…¶æƒ…æ„Ÿå€¾å‘å’Œæ‰€å±é¢†åŸŸã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. æƒ…æ„Ÿå€¾å‘ (label): 'æ­£é¢'ã€'ä¸­ç«‹' æˆ– 'è´Ÿé¢'ã€‚\n"
            "2. æƒ…æ„Ÿåˆ†æ•° (score): 0-100 (è´Ÿé¢<40, ä¸­ç«‹40-60, æ­£é¢>60)ã€‚\n"
            f"3. æ‰€å±é¢†åŸŸ (category): å¿…é¡»ä¸¥æ ¼ä»[{categories_str}]ä¸­é€‰æ‹©æœ€åˆé€‚çš„1ä¸ªï¼Œç¦æ­¢åˆ›é€ æ–°åˆ†ç±»ã€ç¦æ­¢ä½¿ç”¨â€œå…¶ä»–â€ä½œä¸ºæ‰€å±é¢†åŸŸã€‚\n"
            "4. æ¶‰åŠå›½å®¶ (region): åªèƒ½è¾“å‡ºå›½å®¶åç§°ï¼ˆä¸€ä¸ªæˆ–å¤šä¸ªï¼‰ï¼Œç¦æ­¢è¾“å‡ºçœ/å¸‚/åŒº/å¿ç­‰è¡Œæ”¿åŒºåˆ’ï¼Œç¦æ­¢è¾“å‡ºâ€œä¸œäºš/æ¬§æ´²/ä¸­ä¸œâ€ç­‰å¤§åŒºã€‚å…è®¸ç¤ºä¾‹ï¼š'ä¸­å›½'ã€'ç¾å›½'ã€'æ—¥æœ¬'ã€'éŸ©å›½'ã€'ä¿„ç½—æ–¯'ã€'è‹±å›½'ã€'æ³•å›½'ã€'å¾·å›½'ã€'å°åº¦'ã€'åŠ æ‹¿å¤§'ã€'æ¾³å¤§åˆ©äºš'ç­‰ã€‚å¦‚æœæ¶‰åŠå¤šä¸ªå›½å®¶ï¼Œè¯·ç”¨é€—å·åˆ†éš”ï¼ˆå¦‚'ä¸­å›½,ç¾å›½'ï¼‰ã€‚å¦‚æœç¡®å®æ— æ³•åˆ¤æ–­ï¼Œè¯·è¾“å‡º'å…¨çƒ'ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯åˆæ³•çš„ JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« id, label, score, category, regionã€‚ä¾‹å¦‚ï¼š\n"
            '[{"id": 101, "label": "æ­£é¢", "score": 80, "category": "æ”¿æ²»å†›äº‹", "region": "ä¸­å›½"}, ...]'
        )

        user_prompt = "è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼š\n"
        for item in news_items:
            user_prompt += f"[ID:{item['id']}] {item['title']}\n"

        try:
            routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
            res: Optional[str] = None
            for route in routes:
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    res = await self._call_llm(client, route["model"], user_prompt, system_prompt)
                if res:
                    break

            if not res:
                return {}

            clean_res = res.strip()
            # æ— è®ºæ˜¯å¦åŒ…å« markdown æ ‡è®°ï¼Œéƒ½ä¼˜å…ˆå°è¯•æå– JSON æ•°ç»„
            start = clean_res.find("[")
            end = clean_res.rfind("]")
            if start != -1 and end != -1:
                clean_res = clean_res[start : end + 1]
            else:
                # å…œåº•æ¸…ç†
                clean_res = clean_res.replace("```json", "").replace("```", "").strip()

            results_list = json.loads(clean_res)

            result_map: Dict[int, Dict] = {}
            if isinstance(results_list, list):
                for item in results_list:
                    if "id" in item:
                        if "category" in item:
                            item["category"] = self._normalize_category(item["category"])
                        item["region"] = normalize_regions_to_countries(item.get("region"))
                        result_map[item["id"]] = item
            return result_map

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {}

    def _get_prefer_backup(self, route_key: str) -> bool:
        """æ ¹æ®é…ç½®é”®è·å–æ˜¯å¦ä¼˜å…ˆä½¿ç”¨å¤‡ç”¨ AI"""
        route_value = settings.AI_ROUTE.get(route_key, "main").lower()
        return route_value == "backup"

    async def generate_summary(self, title: str, content: str, max_words: int = 300) -> Optional[str]:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ­£æ–‡
        - `max_words`: æœ€å¤§å­—æ•°é™åˆ¶ (é»˜è®¤ 300)

        è¾“å‡º:
        - æ‘˜è¦æ–‡æœ¬ (å¦‚æœå¤±è´¥è¿”å› None)

        ä½œç”¨:
        - ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ–°é—»æ‘˜è¦ï¼Œæ”¯æŒä¸»å¤‡åˆ‡æ¢
        """
        if not content:
            return None
        system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä¸æ–°é—»æ ‡é¢˜ç›¸å…³çš„ä¿¡æ¯æ€»ç»“ä¸º{max_words}å­—å·¦å³çš„çº¯æ–‡å­—å†…å®¹ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. **å†…å®¹è¯¦å®**ï¼šæ‹’ç»ç®€é™‹æ¦‚æ‹¬ï¼Œå¿…é¡»ä¿ç•™ **å…·ä½“äººåã€åœ°åã€æ•°æ®ã€ç‰©å“åç§°** ç­‰å…³é”®å®ä½“ä¿¡æ¯ï¼Œé¿å…è¿‡åº¦æŠ½è±¡ã€‚\n"
            "2. **å»å™ª**ï¼šå»é™¤å¹¿å‘Šã€é“¾æ¥ç­‰æ— å…³ä¿¡æ¯ã€‚\n"
            "3. **çº¯æ–‡æœ¬**ï¼šä¸è¦ä½¿ç”¨ HTML æ ‡ç­¾ã€‚\n"
            "4. **ç›´æ¥è¾“å‡º**ï¼šç›´æ¥å¼€å§‹è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•â€œå¥½çš„â€ã€â€œæ ¹æ®æ‚¨çš„è¦æ±‚â€ã€â€œæ‘˜è¦å¦‚ä¸‹â€ç­‰å®¢å¥—è¯æˆ–å‰ç¼€ã€‚"
        )
        user_prompt = f"æ ‡é¢˜ï¼š{title}\n\næ­£æ–‡ï¼š{content[:100000]}"

        prefer_backup = self._get_prefer_backup("SUMMARY")
        routes = self._iter_llm_routes(prefer_backup)
        for route in routes:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await self._call_llm(
                    client,
                    route["model"],
                    user_prompt,
                    system_prompt,
                )
            if res:
                return res
        return None

    async def _call_llm_with_routes(
        self,
        user_prompt: str,
        system_prompt: str,
        prefer_backup: Optional[bool] = None,
    ) -> Optional[str]:
        prefer = False if prefer_backup is None else prefer_backup
        routes = self._iter_llm_routes(prefer)
        for i, route in enumerate(routes):
            # ä½¿ç”¨ async with ç¡®ä¿ client èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await self._call_llm(
                    client,
                    route["model"],
                    user_prompt,
                    system_prompt,
                )
            
            if res:
                return res
            
            # å¦‚æœè¿è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜å½“å‰è·¯ç”±å¤±è´¥æˆ–è¿”å›ç©ºå†…å®¹
            if i < len(routes) - 1:
                next_route = routes[i+1]
                logger.warning(f"âš ï¸ è·¯ç”± {route['model']} ({route['type']}) è°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºï¼Œå°è¯•åˆ‡æ¢åˆ° -> {next_route['model']} ({next_route['type']})")
            else:
                logger.error(f"âŒ æ‰€æœ‰å¯ç”¨ AI è·¯ç”±å‡è°ƒç”¨å¤±è´¥")
        return None



    async def verify_topic_match_batch(self, tasks: List[Dict[str, str]]) -> List[Tuple[bool, str]]:
        """
        è¾“å…¥:
        - tasks: ä»»åŠ¡åˆ—è¡¨ï¼ŒåŒ…å« {"topic_name": ..., "topic_summary": ..., "news_title": ..., "news_summary": ...}

        è¾“å‡º:
        - ç»“æœåˆ—è¡¨ï¼ŒåŒ…å« (æ˜¯å¦åŒ¹é…, ç†ç”±)
        """
        if not tasks:
            return []

        # è®°å½•è¯·æ±‚å†…å®¹
        logger.info(f"ğŸ¤– æ‰¹é‡æ ¸éªŒä¸“é¢˜åŒ¹é…: {len(tasks)} ç»„")
        for i, t in enumerate(tasks[:3]):  # è®°å½•å‰ 3 æ¡ç”¨äºé¢„è§ˆ
            logger.info(f"   [{i}] ä¸“é¢˜: {t['topic_name']} <-> æ–°é—»: {t['news_title']}")

        system_prompt = (
            "ä½ æ˜¯äº‹ä»¶ä¸€è‡´æ€§åˆ¤å®šåŠ©æ‰‹ã€‚è¯·æ‰¹é‡åˆ¤æ–­ä»¥ä¸‹æ¯ç»„æ–°é—»æ˜¯å¦å±äºå¯¹åº”ä¸“é¢˜è¿½è¸ªçš„åŒä¸€æ–°é—»äº‹ä»¶ï¼ˆæˆ–å…¶ç›´æ¥åç»­è¿›å±•ï¼‰ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1) è§†ä¸ºåŒä¸€äº‹ä»¶ï¼šæ ¸å¿ƒä¸»ä½“/åœ°ç‚¹/å…³é”®äº‹å®ä¸€è‡´ï¼Œæˆ–æ˜æ˜¾æ˜¯åŒä¸€äº‹ä»¶çš„åç»­è¿›å±•ï¼ˆé€šæŠ¥ã€è°ƒæŸ¥ã€è¿›å±•ã€å›åº”ã€äºŒæ¬¡å½±å“ï¼‰ã€‚\n"
            "2) è§†ä¸ºä¸åŒäº‹ä»¶ï¼šä¸»ä½“æ— å…³ã€ä¸åŒåœ°åŒºä¸åŒä¸»ä½“çš„ç›¸ä¼¼è¯é¢˜ã€ä»…åŒç±»æ³›è¯é¢˜ä½†æ— å…±åŒäº‹å®ã€‚\n"
            "3) å½“ä¿¡æ¯ä¸è¶³æ—¶ï¼Œå€¾å‘äºè¿”å› falseã€‚\n"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'match' (å¸ƒå°”å€¼) å’Œ 'reason' (ç®€çŸ­ç†ç”±)ã€‚\n"
            "ä¾‹å¦‚ï¼š[{\"match\": true, \"reason\": \"æ ¸å¿ƒäº‹å®ä¸€è‡´\"}, {\"match\": false, \"reason\": \"ä¸»ä½“ä¸åŒ\"}]"
        )

        user_prompt = "è¯·åˆ¤æ–­ä»¥ä¸‹å„ç»„åŒ¹é…æƒ…å†µï¼š\n"
        for idx, task in enumerate(tasks):
            user_prompt += (
                f"--- ç¬¬ {idx+1} ç»„ ---\n"
                f"ã€ä¸“é¢˜ã€‘{task['topic_name']}\n"
                f"ã€ä¸“é¢˜æ¦‚è§ˆã€‘{(task['topic_summary'] or '')[:300]}\n"
                f"ã€æ–°é—»æ ‡é¢˜ã€‘{task['news_title']}\n"
                f"ã€æ–°é—»æ‘˜è¦ã€‘{(task['news_summary'] or '')[:300]}\n"
            )

        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return [(False, "AIè°ƒç”¨å¤±è´¥")] * len(tasks)

        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            results = json.loads(clean)
            
            output = []
            if isinstance(results, list):
                for item in results:
                    is_match = bool(item.get("match", False))
                    reason = item.get("reason", "æ— ç†ç”±")
                    output.append((is_match, reason))
                
                # ç¡®ä¿é•¿åº¦åŒ¹é…
                if len(output) < len(tasks):
                    output.extend([(False, "è¿”å›æ•°é‡ä¸è¶³")] * (len(tasks) - len(output)))
                
                return output[:len(tasks)]
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¸“é¢˜æ ¸éªŒè§£æå¤±è´¥: {e}")

        return [(False, "è§£æå¼‚å¸¸")] * len(tasks)




    async def generate_daily_timeline_events(self, date_str: str, news_items: List[Dict[str, Any]], topic_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - date_str: "YYYY-MM-DD"
        - news_items: List of {"id": ..., "title": ..., "summary": ...}
        - topic_name: Optional topic name to focus on

        è¾“å‡º:
        - List of events: [{"content": "...", "source_ids": [id1, id2...]}, ...]
        """
        if not news_items:
            return []

        logger.info(f"ğŸ¤– æ­£åœ¨ä¸º {date_str} åˆæˆæ—¶é—´è½´äº‹ä»¶ (å…± {len(news_items)} æ¡æ–°é—»)...")
        
        focus_instruction = ""
        if topic_name:
            focus_instruction = f"7. **ä¸“é¢˜èšç„¦**ï¼šå½“å‰ä¸“é¢˜ä¸ºâ€œ{topic_name}â€ï¼Œè¯·**ä¸¥æ ¼ä¸“æ³¨äºä¸è¯¥ä¸“é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯**ã€‚æ¯ä¸ªèŠ‚ç‚¹æè¿°å¿…é¡»ä¸ä¸“é¢˜åç§°é«˜åº¦ç›¸å…³ã€‚å¦‚æœæ˜¯æ— å…³çš„å™ªéŸ³æ–°é—»ï¼Œè¯·ç›´æ¥å¿½ç•¥ã€‚å¦‚æœæ‰€æœ‰æ–°é—»éƒ½ä¸è¯¥ä¸“é¢˜æ— å…³ï¼Œè¿”å›ç©ºæ•°ç»„ã€‚\n"

        system_prompt = (
            "ä½ æ˜¯æ—¶é—´è½´äº‹ä»¶åˆæˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æŸä¸€å¤©çš„æ–°é—»åˆ—è¡¨ï¼Œå°†å…¶åˆæˆä¸º 1-2 ä¸ªå…³é”®çš„æ—¶é—´è½´èŠ‚ç‚¹äº‹ä»¶ã€‚\n"
            "ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š\n"
            "1. **åˆå¹¶åŒç±»é¡¹**ï¼šå¿…é¡»å°†æŠ¥é“åŒä¸€äº‹ä»¶çš„ä¸åŒæ–°é—»åˆå¹¶ä¸ºä¸€ä¸ªäº‹ä»¶èŠ‚ç‚¹ã€‚\n"
            "2. **å†…å®¹ç²¾ç‚¼**ï¼šæ¯ä¸ªäº‹ä»¶æè¿°æ§åˆ¶åœ¨ 100 å­—ä»¥å†…ï¼Œæ¦‚æ‹¬æ ¸å¿ƒäº‹å®ã€‚\n"
            "3. **å…³è”æ¥æº**ï¼šå¯¹äºæ¯ä¸ªç”Ÿæˆçš„äº‹ä»¶ï¼Œå¿…é¡»åˆ—å‡ºæ”¯æŒè¯¥äº‹ä»¶çš„æ‰€æœ‰æ–°é—» ID (source_ids)ã€‚\n"
            "4. **æ•°é‡é™åˆ¶**ï¼š**æ¯å¤©æœ€å¤šç”Ÿæˆ 2 ä¸ªäº‹ä»¶èŠ‚ç‚¹**ã€‚å¿…é¡»ä¸¥æ ¼æ‰§è¡Œã€‚å¦‚æœæœ‰å¤šä»¶å¤§äº‹ï¼Œå–æœ€é‡è¦çš„ 2 ä»¶ï¼›å¦‚æœæ˜¯åŒä¸€ä»¶äº‹çš„å¤šä¸ªæ–¹é¢ï¼Œå¿…é¡»åˆå¹¶ä¸º 1 ä»¶ã€‚\n"
            "5. **ä¸¥æ ¼æ—¶é—´è½´åˆå¹¶**ï¼š\n"
            "   - å¯¹äºå‘å¸ƒæ—¶é—´åœ¨ 2 å°æ—¶å†…çš„æ–°é—»ï¼Œå¿…é¡»åˆå¹¶åˆ°åŒä¸€ä¸ªæ—¶é—´è½´èŠ‚ç‚¹ä¸­ï¼Œç¦æ­¢å•ç‹¬åˆ›å»ºæ–°èŠ‚ç‚¹ã€‚\n"
            "   - ç»å¯¹ç¦æ­¢è·¨å¤©åˆå¹¶æ–°é—»äº‹ä»¶ï¼ˆè¾“å…¥å·²é™å®šä¸ºåŒä¸€å¤©ï¼‰ã€‚\n"
            "6. **ç›´æ¥è¾“å‡º**ï¼šä¸è¦åŒ…å«â€œå¥½çš„â€ã€â€œæ ¹æ®æä¾›çš„æ–°é—»â€ç­‰å®¢å¥—è¯ï¼Œç›´æ¥è¿”å›ç»“æœã€‚\n"
            "7. **è¿”å›æ ¼å¼**ï¼šä»…è¿”å› JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š[{\"content\": \"äº‹ä»¶æè¿°...\", \"source_ids\": [1, 3]}, ...]\n"
            f"{focus_instruction}"
        )

        user_prompt = f"æ—¥æœŸï¼š{date_str}\n"
        if topic_name:
            user_prompt += f"ä¸“é¢˜åç§°ï¼š{topic_name}\n"
        user_prompt += "æ–°é—»åˆ—è¡¨ï¼š\n"
        for item in news_items:
            user_prompt += f"[ID: {item['id']}] {item['title']}\næ‘˜è¦: {(item['summary'] or '')[:100]}\n\n"

        prefer_backup = self._get_prefer_backup("TOPIC_TIMELINE")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            return []
            
        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            events = json.loads(clean)
            return events
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£ææ—¶é—´è½´åˆæˆç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def check_topic_duplicate(
        self,
        new_name: str,
        new_desc: str,
        existing_name: str,
        existing_desc: str
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­ä¸¤ä¸ªä¸“é¢˜æ˜¯å¦å®è´¨ä¸Šæ˜¯åŒä¸€ä¸ªäº‹ä»¶ã€‚
        """
        system_prompt = (
            "ä½ æ˜¯ä¸“é¢˜å»é‡åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªä¸“é¢˜æ˜¯å¦æŒ‡çš„æ˜¯åŒä¸€ä¸ªå…·ä½“çš„æ–°é—»äº‹ä»¶ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. ã€è§†ä¸ºç›¸åŒã€‘ï¼š\n"
            "   - æŒ‡å‘åŒä¸€ä¸ªæ ¸å¿ƒçªå‘äº‹ä»¶ï¼ˆå¦‚â€œæŸåœ°åœ°éœ‡â€ä¸â€œæŸåœ°å‘ç”Ÿ6.0çº§åœ°éœ‡â€ï¼‰ã€‚\n"
            "   - ä»…ä»…æ˜¯å‘½åè§’åº¦ä¸åŒï¼ˆå¦‚â€œSpaceXæ˜Ÿèˆ°å‘å°„â€ä¸â€œæ˜Ÿèˆ°ç¬¬äº”æ¬¡è¯•é£â€ï¼‰ã€‚\n"
            "   - ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­é›†æˆ–åˆæœŸé˜¶æ®µï¼Œä¸”æ ¸å¿ƒäº‹å®å®Œå…¨é‡åˆã€‚\n"
            "2. ã€è§†ä¸ºä¸åŒã€‘ï¼š\n"
            "   - ä¸åŒçš„ç‹¬ç«‹äº‹ä»¶ï¼ˆå¦‚â€œä¿„ä¹Œå†²çªâ€ä¸â€œå·´ä»¥å†²çªâ€ï¼‰ã€‚\n"
            "   - åŒä¸€ç±»åˆ«çš„ä¸åŒä¸ªä½“ï¼ˆå¦‚â€œæŸå…¬å¸å‘å¸ƒè´¢æŠ¥â€ä¸â€œå¦ä¸€å…¬å¸å‘å¸ƒè´¢æŠ¥â€ï¼‰ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šä»…è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¾‹å¦‚ï¼š{\"duplicate\": true, \"reason\": \"...\"}"
        )
        user_prompt = (
            f"ã€ä¸“é¢˜Aã€‘åç§°ï¼š{new_name}\næè¿°ï¼š{new_desc[:500]}\n\n"
            f"ã€ä¸“é¢˜Bã€‘åç§°ï¼š{existing_name}\næè¿°ï¼š{existing_desc[:500]}"
        )
        
        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return False, "AIè°ƒç”¨å¤±è´¥"
            
        clean = res.strip()
        try:
            if "```" in clean:
                start = clean.find("{")
                end = clean.rfind("}")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            data = json.loads(clean)
            return bool(data.get("duplicate", False)), data.get("reason", "æ— ç†ç”±")
        except Exception:
            lowered = clean.lower()
            if "true" in lowered and "false" not in lowered:
                return True, "è§£æå¤±è´¥(fallback: true)"
            return False, "è§£æå¤±è´¥"

    async def stream_chat(self, query: str, context: str, model_type: str = "main") -> AsyncIterator[str]:
        """
        è¾“å…¥:
        - `query`: ç”¨æˆ·é—®é¢˜
        - `context`: ç›¸å…³æ–°é—»ä¸Šä¸‹æ–‡
        - `model_type`: ä½¿ç”¨ä¸»/å¤‡é€šé“ï¼ˆmain/backupï¼‰

        è¾“å‡º:
        - SSE å¯è¿­ä»£çš„å¢é‡æ–‡æœ¬ç‰‡æ®µ

        ä½œç”¨:
        - ä»¥æµå¼æ–¹å¼ä¸æ¨¡å‹å¯¹è¯ï¼Œé€‚é…å‰ç«¯å®æ—¶å±•ç¤º
        """
        # å¦‚æœè¯·æ±‚ä½¿ç”¨ main é€šé“ï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥é…ç½®è·¯ç”±æ˜¯å¦æŒ‡å®šäº†ä¼˜å…ˆå¤‡ç”¨
        if model_type == "main":
            if self._get_prefer_backup("CHAT"):
                model_type = "backup"

        api_key = settings.MAIN_AI_API_KEY
        base_url = settings.MAIN_AI_BASE_URL
        model = settings.MAIN_AI_MODEL

        if model_type == "backup":
            api_key = settings.BACKUP_AI_API_KEY
            base_url = settings.BACKUP_AI_BASE_URL
            model = settings.BACKUP_AI_MODEL

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–°é—»ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
            "å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†å›ç­”ï¼Œä½†è¯·æ³¨æ˜â€œæ ¹æ®å·²æœ‰æ–°é—»æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„è¡¥å……...â€ã€‚\n"
            "å›ç­”è¦ç®€æ´ã€å®¢è§‚ã€‚"
        )
        user_prompt = f"ã€æ–°é—»ä¸Šä¸‹æ–‡ã€‘:\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘: {query}"

        try:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=api_key, base_url=base_url) as client:
                extra_body = {}
                if "modelscope" in str(client.base_url):
                    extra_body["enable_thinking"] = False

                logger.info(f"å¼€å§‹æµå¼å¯¹è¯è¯·æ±‚: model={model}, stream=True")
                stream = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    stream=True,
                    temperature=0.7,
                    timeout=60,
                    extra_body=extra_body if extra_body else None,
                )
                logger.info("æµå¼è¯·æ±‚å·²å»ºç«‹ï¼Œå¼€å§‹è¯»å– chunks")
                chunk_count = 0
                async for chunk in stream:
                    logger.debug(f"æ”¶åˆ°åŸå§‹å—: {chunk}")
                    if not chunk.choices:
                        logger.debug(f"å—ä¸­æ— é€‰é¡¹: {chunk}")
                        continue
                    content = chunk.choices[0].delta.content
                    if content:
                        chunk_count += 1
                        # logger.debug(f"ç”Ÿæˆå†…å®¹: {content!r}")
                        yield content
                    else:
                        logger.debug(f"å—å†…å®¹ä¸ºç©º: {chunk}")
                logger.info(f"æµå¼ä¼ è¾“ç»“æŸ, å…±å‘é€ {chunk_count} ä¸ª chunks")
        except Exception as e:
            logger.error(f"èŠå¤©æµé”™è¯¯: {e}", exc_info=True)
            yield f"é”™è¯¯: {str(e)}"

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        è¾“å…¥:
        - `texts`: å¾…å‘é‡åŒ–æ–‡æœ¬åˆ—è¡¨

        è¾“å‡º:
        - å‘é‡åˆ—è¡¨ï¼ˆä¸è¾“å…¥ä¸€ä¸€å¯¹åº”ï¼›å¤±è´¥æ—¶è¿”å›ç©ºå‘é‡ï¼‰

        ä½œç”¨:
        - è°ƒç”¨ embedding æœåŠ¡ç”Ÿæˆå‘é‡ï¼Œç”¨äºè¯­ä¹‰æ£€ç´¢ä¸èšç±»
        """

        if not texts:
            return []
        if not self._has_embedding():
            return [[] for _ in texts]
        cleaned_texts = [t.replace("\n", " ").strip()[:1000] for t in texts]

        url = f"{settings.SILICONFLOW_BASE_URL.rstrip('/')}/embeddings"
        headers = {
            "Authorization": f"Bearer {settings.SILICONFLOW_API_KEY}",
            "Content-Type": "application/json",
        }

        all_embeddings: List[List[float]] = []
        batch_size = 20

        for i in range(0, len(cleaned_texts), batch_size):
            batch = cleaned_texts[i : i + batch_size]
            payload = {
                "model": settings.EMBEDDING_MODEL,
                "input": batch,
                "encoding_format": "float",
            }
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, headers=headers, json=payload, timeout=30) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            batch_res = sorted(data["data"], key=lambda x: x["index"])
                            all_embeddings.extend([x["embedding"] for x in batch_res])
                        elif resp.status == 401:
                             error_text = await resp.text()
                             logger.error(f"âŒ å‘é‡ API è®¤è¯å¤±è´¥ (401): {error_text}")
                             raise AIConfigurationError("Embedding API Key æ— æ•ˆ")
                        else:
                            logger.error(f"âŒ å‘é‡ API é”™è¯¯: {await resp.text()}")
                            all_embeddings.extend([[] for _ in batch])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ å‘é‡ç½‘ç»œé”™è¯¯: {e}")
                all_embeddings.extend([[] for _ in batch])
        return all_embeddings

    async def verify_cluster_batch(self, pairs: List[Dict[str, str]]) -> List[bool]:
        """
        è¾“å…¥:
        - `pairs`: å¾…æ ¸éªŒæ–°é—»å¯¹åˆ—è¡¨ï¼ˆleader/candidate æ ‡é¢˜ï¼‰

        è¾“å‡º:
        - å¸ƒå°”åˆ—è¡¨ï¼ˆä¸è¾“å…¥é¡ºåºä¸€è‡´ï¼Œè¡¨ç¤ºæ˜¯å¦åŒä¸€äº‹ä»¶ï¼‰

        ä½œç”¨:
        - ä½¿ç”¨å¤§æ¨¡å‹å¯¹ç›¸ä¼¼å€™é€‰è¿›è¡Œæ‰¹é‡æ ¸éªŒï¼Œå‡å°‘è¯¯åˆå¹¶
        """

        if not pairs:
            return []
        if not (self._has_main_llm() or self._has_backup_llm()):
            return [False] * len(pairs)

        system_prompt = (
            "è¯·åˆ¤æ–­åˆ—è¡¨ä¸­æ¯å¯¹æ–°é—»æ ‡é¢˜æ˜¯å¦å±äº **åŒä¸€ä¸ªæ–°é—»äº‹ä»¶çš„æŠ¥é“** , æ ‡å‡†é€‚å½“å®½æ¾ï¼Œå°½å¯èƒ½å°†ç›¸å…³å…³é”®è¯æ–°é—»åˆ¤æ–­ä¸ºåŒä¸€æ–°é—»äº‹ä»¶ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. ã€è§†ä¸ºç›¸åŒã€‘ï¼š\n"
            "   - **æ ¸å¿ƒäº‹å®é‡åˆ**ï¼šæè¿°åŒä¸€ä¸ªå…·ä½“çš„çªå‘äº‹ä»¶ã€æ”¿ç­–å‘å¸ƒã€ç§‘æŠ€å‘ç°ç­‰ã€‚æ•°å­—ç•¥æœ‰å·®å¼‚ï¼ˆå¦‚â€œ1178åâ€ä¸â€œé€¾åƒåâ€ï¼‰æˆ–è¡¨è¿°ä¸åŒï¼ˆå¦‚â€œç¦äº§â€ä¸â€œç¦æ­¢ç”Ÿäº§â€ï¼‰è§†ä¸ºç›¸åŒã€‚\n"
            "   - **äº‹ä»¶å»¶ç»­ä¸å…³è”**ï¼šåŒä¸€å¤§äº‹ä»¶èƒŒæ™¯ä¸‹çš„ç›´æ¥åç»­æˆ–ååº”ï¼ˆå¦‚â€œåœ°éœ‡å‘ç”Ÿâ€ä¸â€œåœ°éœ‡åå¼•å‘æµ·å•¸â€ã€â€œæŸäººå¯¹åœ°éœ‡çš„ååº”â€ï¼‰ã€‚\n"
            "   - **æŠ€æœ¯/ä¸“ä¸šè¯é¢˜**ï¼šåŒä¸€æ¼æ´ã€åŒä¸€äº§å“çš„ä¸åŒè§£è¯»ï¼ˆå¦‚â€œReactæ¼æ´â€ï¼‰ã€‚\n"
            "2. ã€è§†ä¸ºä¸åŒã€‘ï¼š\n"
            "   - **ä¸»ä½“å®Œå…¨æ— å…³**ï¼šå¦‚â€œé»„é‡‘â€ä¸â€œç™½é“¶â€ï¼Œâ€œè‹¹æœâ€ä¸â€œé¦™è•‰â€ã€‚\n"
            "   - **æ˜ç¡®çš„ä¸åŒæœŸæ•°**ï¼šå¦‚â€œ1æœˆ1æ—¥æ—¥æŠ¥â€ä¸â€œ1æœˆ2æ—¥æ—¥æŠ¥â€ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šä»…è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«å¯¹åº”é¡ºåºçš„å¸ƒå°”å€¼ï¼Œä¾‹å¦‚ï¼š[true, false, true]"
        )

        user_content = "è¯·åˆ¤æ–­ä»¥ä¸‹æ–°é—»å¯¹ï¼š\n"
        for i, p in enumerate(pairs):
            user_content += f"{i + 1}. [{p['leader']}] vs [{p['candidate']}]\n"

        async def try_verify(client, model):
            try:
                res = await self._call_llm(client, model, user_content, system_prompt)
                if not res:
                    return None

                clean_res = res.strip()
                if clean_res.startswith("```"):
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                else:
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]

                # å°è¯•ä¿®å¤ Python é£æ ¼çš„å¸ƒå°”å€¼
                clean_res = clean_res.replace("True", "true").replace("False", "false")

                try:
                    results = json.loads(clean_res)
                except json.JSONDecodeError:
                    # å°è¯•ä½¿ç”¨æ­£åˆ™æå–å¸ƒå°”å€¼
                    import re
                    bool_matches = re.findall(r'\b(true|false)\b', clean_res, re.IGNORECASE)
                    if len(bool_matches) == len(pairs):
                        results = [b.lower() == 'true' for b in bool_matches]
                    else:
                        raise

                if isinstance(results, list) and len(results) == len(pairs):
                    return [bool(x) for x in results]

                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒè¿”å›æ ¼å¼é”™è¯¯: {results} (é¢„æœŸé•¿åº¦: {len(pairs)})")
                return None
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒå¼‚å¸¸ ({model}): {e}")
                return None

        routes = self._iter_llm_routes(self._get_prefer_backup("CLUSTERING"))
        for route in routes:
            # å¦‚æœæ˜¯ backup é€šé“ï¼Œå°è¯• 3 æ¬¡ï¼›å¦‚æœæ˜¯ main é€šé“ï¼Œå°è¯• 1 æ¬¡
            is_backup = (route["base_url"] == settings.BACKUP_AI_BASE_URL)
            max_attempts = 3 if is_backup else 1
            
            for attempt in range(max_attempts):
                if attempt > 0:
                    await asyncio.sleep(2 if attempt == 1 else 10)
                
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    try:
                        res = await try_verify(client, route["model"])
                    except AIConfigurationError:
                        raise
                
                if res is not None:
                    return res
                if is_backup:
                    logger.warning(f"âš ï¸ å¤‡ç”¨AIæ ¸éªŒå¤±è´¥ (ç¬¬{attempt + 1}æ¬¡)ï¼Œå‡†å¤‡é‡è¯•...")

        logger.error("âŒ æ‰€æœ‰é€šé“æ ¸éªŒå‡å¤±è´¥ï¼Œè·³è¿‡æœ¬æ‰¹æ¬¡")
        return [False] * len(pairs)


    async def generate_topic_overview(self, topic_name: str, news_list: List[Dict[str, str]]) -> str:
        """
        è¾“å…¥:
        - `topic_name`: ä¸“é¢˜åç§°
        - `news_list`: æ–°é—»åˆ—è¡¨ [{"title": "...", "content": "..."}, ...]

        è¾“å‡º:
        - ä¸“é¢˜å¤šç»´åº¦ç»¼è¿°ï¼ˆçº¯æ–‡æœ¬ï¼‰
        
        ä½œç”¨:
        - ä»â€œäº‹ä»¶èƒŒæ™¯â€ã€â€œå‘å±•è¿‡ç¨‹â€ã€â€œå„æ–¹è§‚ç‚¹â€ã€â€œæœªæ¥å±•æœ›â€ç­‰ç»´åº¦å¯¹ä¸“é¢˜è¿›è¡Œæ·±åº¦æ€»ç»“
        """
        if not news_list:
            return ""
            
        logger.info(f"ğŸ¤– ç”Ÿæˆä¸“é¢˜ç»¼è¿°: {topic_name} ({len(news_list)} æ¡æ–°é—»)")
        
        # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…Tokenæº¢å‡º
        # ä¼˜å…ˆå–æœ€æ–°çš„æ–°é—»ï¼Œå’Œæœ€æ—©çš„æ–°é—»ï¼Œä»¥è¦†ç›–å…¨è²Œ
        # å‡è®¾ news_list å·²ç»åŒ…å«äº†ä¸€å®šæ•°é‡çš„ä»£è¡¨æ€§æ–°é—»
        input_text = ""
        for i, item in enumerate(news_list[:30]): # æœ€å¤šå–30æ¡ä½œä¸ºä¸Šä¸‹æ–‡
            t = (item.get("title") or "").replace("\n", " ")
            c = (item.get("summary") or item.get("content") or "")[:200].replace("\n", " ")
            input_text += f"[{i+1}] {t}\n   æ‘˜è¦: {c}\n\n"
            
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ–°é—»è¯„è®ºå‘˜ã€‚è¯·æ ¹æ®æä¾›çš„å¤šæ¡æ–°é—»æŠ¥é“ï¼Œå¯¹è¯¥ä¸“é¢˜äº‹ä»¶è¿›è¡Œå…¨æ–¹ä½çš„æ·±åº¦ç»¼è¿°ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. **ä¸¥æ ¼ä¸€è‡´æ€§**ï¼šç»¼è¿°å†…å®¹å¿…é¡»ä¸ä¸“é¢˜åç§°ä¿æŒä¸¥æ ¼ä¸€è‡´ï¼Œç¦æ­¢åŒ…å«ä¸ä¸“é¢˜æ— å…³çš„å†…å®¹ã€‚\n"
            "2. **ä¿¡æ¯è¯¦å®**ï¼šåœ¨ç»¼è¿°ä¸­å¿…é¡»ä¿ç•™å…³é”®çš„**äº‹å®ç»†èŠ‚**ï¼Œæ‹’ç»ç©ºæ´çš„å®å¤§å™äº‹ã€‚è¯·åŠ¡å¿…åŒ…å«å…·ä½“äººåã€å…³é”®ç‰©å“åç§°ã€å…·ä½“é‡‘é¢/ä¼°å€¼ã€æœºæ„åç§°ã€å…·ä½“çš„æŒ‡æ§æˆ–äº‰è®®ç‚¹ã€å„æ–¹å…·ä½“å›åº”å’Œæ ¸å¿ƒè¯æ®ã€‚\n"
            "3. **ç»“æ„æ¸…æ™°**ï¼šè¯·åŒ…å«ä»¥ä¸‹å‡ ä¸ªç»´åº¦ï¼š\n"
            "   - **ã€äº‹ä»¶èƒŒæ™¯ã€‘**ï¼šè¯¦ç»†é˜è¿°äº‹ä»¶èµ·å› ã€‚\n"
            "   - **ã€å‘å±•è„‰ç»œã€‘**ï¼šæŒ‰é€»è¾‘æ¢³ç†äº‹ä»¶çš„å‡çº§è¿‡ç¨‹ã€‚\n"
            "   - **ã€äº‰è®®ç„¦ç‚¹ã€‘**ï¼šæ ¸å¿ƒçŸ›ç›¾æ˜¯ä»€ä¹ˆã€‚\n"
            "   - **ã€æ ¸å¿ƒå½±å“ã€‘**ï¼šå…·ä½“çš„ç¤¾ä¼š/è¡Œä¸šå½±å“ã€‚\n"
            "   - **ã€æœ€æ–°è¿›å±•ä¸å±•æœ›ã€‘**ï¼šå½“å‰çš„è°ƒæŸ¥çŠ¶æ€åŠå¯èƒ½çš„èµ°å‘ã€‚\n"
            "4. **æ·±åº¦åˆ†æ**ï¼šä¸è¦åªåšè¡¨é¢æ‹¼å‡‘ï¼Œè¦åˆ†æäº‹ä»¶èƒŒåçš„é€»è¾‘å…³è”ã€‚\n"
            "5. **çº¯æ–‡æœ¬æ ¼å¼**ï¼šä¸è¦ä½¿ç”¨ Markdown æ ‡é¢˜ç¬¦å·ï¼ˆå¦‚ #, ##, **ï¼‰ï¼Œä½¿ç”¨ä¸­æ–‡æ–¹æ‹¬å·ã€ã€‘ä½œä¸ºå°æ ‡é¢˜å³å¯ã€‚åˆ†æ®µæ¢è¡Œè¦æ¸…æ™°ã€‚\n"
            "6. å­—æ•°æ§åˆ¶åœ¨ 600-1200 å­—ä¹‹é—´ã€‚\n"
            "7. **ç›´æ¥è¾“å‡º**ï¼šç›´æ¥å¼€å§‹æ­£æ–‡ï¼Œä¸è¦æœ‰â€œå¥½çš„â€ã€â€œç»¼è¿°å¦‚ä¸‹â€ç­‰å¼€åœºç™½ã€‚"
        )
        
        user_prompt = f"ä¸“é¢˜åç§°ï¼š{topic_name}\n\nç›¸å…³æ–°é—»æŠ¥é“ï¼š\n{input_text}"
        
        prefer_backup = self._get_prefer_backup("TOPIC_OVERVIEW")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res.strip()
        return "æš‚æ— æ³•ç”Ÿæˆç»¼è¿°ã€‚"







ai_service = AIService()
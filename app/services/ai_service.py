"""
æœ¬æ–‡ä»¶ç”¨äºå°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼ŒåŒ…æ‹¬å¯¹è¯ã€æ‘˜è¦ã€åˆ†æä¸å‘é‡åŒ–ç­‰èƒ½åŠ›ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `AIService`: AI èƒ½åŠ›å°è£…ï¼ˆä¸»/å¤‡æ¨¡å‹åˆ‡æ¢ã€å¹¶å‘æ§åˆ¶ã€å¤±è´¥é™çº§ï¼‰
- `ai_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import asyncio
import json
import re
import logging
from typing import Any, AsyncIterator, Dict, List, Optional, Tuple

import aiohttp
from openai import AsyncOpenAI, APIStatusError, RateLimitError, APIConnectionError

from app.core.config import get_settings
from app.core.logger import setup_logger
from app.core.exceptions import AIConfigurationError
from app.utils.tools import normalize_regions_to_countries

settings = get_settings()
logger = setup_logger("AIService")


class AIService:
    """
    è¾“å…¥:
    - AI é…ç½®ï¼ˆä¸»/å¤‡æ¨¡å‹ã€å¹¶å‘é™åˆ¶ã€Embedding é…ç½®ï¼‰

    è¾“å‡º:
    - å¤§æ¨¡å‹å¯¹è¯/æ‘˜è¦/åˆ†æç»“æœï¼Œä»¥åŠå‘é‡åŒ–ç»“æœ

    ä½œç”¨:
    - å°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼Œæä¾›ç»Ÿä¸€ã€å¯é™çº§çš„è°ƒç”¨å…¥å£
    """

    def __init__(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå§‹åŒ–ä¸»/å¤‡é€šé“å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        """

        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)

    def reload_config(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - é‡æ–°åŠ è½½å…¨å±€é…ç½®ï¼ˆç”¨äºé…ç½®æ›´æ–°ååˆ·æ–°æœ¬åœ°å¼•ç”¨ï¼‰
        """
        global settings
        from app.core.config import get_settings
        settings = get_settings()
        
        # é‡æ–°åˆå§‹åŒ–ä¿¡å·é‡ï¼ˆå¹¶å‘é…ç½®å¯èƒ½æ”¹å˜ï¼‰
        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)
        logger.info("ğŸ”„ AIService é…ç½®å·²åˆ·æ–°")

    def _has_main_llm(self) -> bool:
        return bool((settings.MAIN_AI_API_KEY or "").strip()) and bool((settings.MAIN_AI_BASE_URL or "").strip()) and bool((settings.MAIN_AI_MODEL or "").strip())

    def _has_backup_llm(self) -> bool:
        return bool((settings.BACKUP_AI_API_KEY or "").strip()) and bool((settings.BACKUP_AI_BASE_URL or "").strip()) and bool((settings.BACKUP_AI_MODEL or "").strip())

    def _has_embedding(self) -> bool:
        return bool((settings.SILICONFLOW_API_KEY or "").strip()) and bool((settings.SILICONFLOW_BASE_URL or "").strip()) and bool((settings.EMBEDDING_MODEL or "").strip())

    def _iter_llm_routes(self, prefer_backup: bool) -> List[Dict[str, str]]:
        routes: List[Dict[str, str]] = []
        if prefer_backup:
            if self._has_backup_llm():
                routes.append(
                    {
                        "base_url": str(settings.BACKUP_AI_BASE_URL),
                        "api_key": str(settings.BACKUP_AI_API_KEY),
                        "model": str(settings.BACKUP_AI_MODEL),
                        "type": "backup",
                    }
                )
            if self._has_main_llm():
                routes.append(
                    {
                        "base_url": str(settings.MAIN_AI_BASE_URL),
                        "api_key": str(settings.MAIN_AI_API_KEY),
                        "model": str(settings.MAIN_AI_MODEL),
                        "type": "main",
                    }
                )
            return routes

        if self._has_main_llm():
            routes.append(
                {
                    "base_url": str(settings.MAIN_AI_BASE_URL),
                    "api_key": str(settings.MAIN_AI_API_KEY),
                    "model": str(settings.MAIN_AI_MODEL),
                    "type": "main",
                }
            )
        if self._has_backup_llm():
            routes.append(
                {
                    "base_url": str(settings.BACKUP_AI_BASE_URL),
                    "api_key": str(settings.BACKUP_AI_API_KEY),
                    "model": str(settings.BACKUP_AI_MODEL),
                    "type": "backup",
                }
            )
        return routes

    async def _call_llm(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: str,
        system: str = "",
        semaphore: asyncio.Semaphore | None = None,
    ) -> Optional[str]:
        """
        è¾“å…¥:
        - `client`: OpenAI å…¼å®¹å®¢æˆ·ç«¯
        - `model`: æ¨¡å‹åç§°
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system`: ç³»ç»Ÿæç¤ºè¯
        - `semaphore`: å¹¶å‘æ§åˆ¶ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹è¿”å›æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - ç»Ÿä¸€å°è£… LLM è°ƒç”¨ã€å¹¶å‘æ§åˆ¶ä¸å¼‚å¸¸å¤„ç†
        """

        try:
            extra_body = {}
            if "modelscope" in str(client.base_url):
                extra_body["enable_thinking"] = False

            if semaphore is None:
                if str(settings.MAIN_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.main_sem
                elif str(settings.BACKUP_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.backup_sem

            # DEBUG Log for prompt
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ”µ [LLM Request] Model: {model}\nSystem: {system}\nPrompt: {prompt[:2000]}...")

            async def do_call():
                return await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=0.3,
                    timeout=120,
                    extra_body=extra_body if extra_body else None,
                )

            # Retry logic
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if semaphore:
                        async with semaphore:
                            response = await do_call()
                    else:
                        response = await do_call()
                    
                    content = response.choices[0].message.content
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"ğŸŸ¢ [LLM Response] Model: {model}\nContent: {content[:2000]}...")
                    
                    if not content:
                         logger.warning(f"âš ï¸ AI è¿”å›å†…å®¹ä¸ºç©º ({model})")

                    return content
                
                except (RateLimitError, APIConnectionError) as e:
                    if attempt == max_retries - 1:
                        raise e
                    wait_time = 2 * (attempt + 1)
                    logger.warning(f"âš ï¸ AI è°ƒç”¨å—é™æˆ–ç½‘ç»œæ³¢åŠ¨ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯• ({attempt + 1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                
                except APIStatusError as e:
                    # 401: Invalid API Key - Fatal error
                    if e.status_code == 401:
                        logger.error(f"âŒ AI è®¤è¯å¤±è´¥ (401) - API Key æ— æ•ˆ ({model}): {e}")
                        raise AIConfigurationError(f"AI API Key æ— æ•ˆ ({model})")

                    # 400 Bad Request usually means content filter or invalid parameters
                    if e.status_code == 400:
                        logger.warning(f"âŒ AI è¯·æ±‚è¢«æ‹’ç» (400) - å¯èƒ½è§¦å‘æ•æ„Ÿè¯è¿‡æ»¤ ({model}): {e}")
                        return None # Fail over to next route
                    
                    # Server error, retry might help
                    if e.status_code >= 500:
                        if attempt == max_retries - 1:
                            raise e
                        wait_time = 2 * (attempt + 1)
                        logger.warning(f"âš ï¸ AI æœåŠ¡ç«¯é”™è¯¯ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯•")
                        await asyncio.sleep(wait_time)
                    else:
                        raise e

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ AI è°ƒç”¨å¼‚å¸¸ ({model}): {e}")
            return None

    async def chat_completion(self, prompt: str, system_prompt: str = "", route_key: str = None) -> str:
        """
        è¾“å…¥:
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system_prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        - `route_key`: é…ç½®è·¯ç”±é”®ï¼ˆå¯é€‰ï¼Œå¦‚ "REPORT", "SUMMARY" ç­‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹å›å¤æ–‡æœ¬ï¼ˆä¿è¯è¿”å›å­—ç¬¦ä¸²ï¼‰

        ä½œç”¨:
        - æ‰§è¡Œä¸€æ¬¡å¯¹è¯è¡¥å…¨ï¼›æ ¹æ® route_key é€‰æ‹©ä¸»/å¤‡é€šé“ç­–ç•¥
        """
        prefer_backup = False
        if route_key:
            prefer_backup = self._get_prefer_backup(route_key)
        
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res

        return "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆè¯·å…ˆåœ¨ç®¡ç†é¡µå®Œå–„ AI é…ç½®ï¼‰"

    async def batch_evaluate_topic_quality(self, topics: List[Dict[str, str]], existing_topics: List[Dict[str, str]] = None) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - topics: List of {"name":..., "description":...}
        - existing_topics: List of {"name":..., "description":...} ç°æœ‰ä¸“é¢˜åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - List of valid topics (filtered)

        ä½œç”¨:
        - æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡ï¼Œè¿‡æ»¤æ‰è¿‡äºå®½æ³›ã€éå…·ä½“äº‹ä»¶ã€æˆ–çº¯ç²¹è¡Œä¸šè¶‹åŠ¿çš„ä¸“é¢˜
        - æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ä¸“é¢˜é‡å¤æˆ–å±äºç°æœ‰ä¸“é¢˜çš„å»¶ä¼¸
        """
        if not topics:
            return []

        logger.info(f"ğŸ¤– æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡: {len(topics)} ä¸ª")
        
        existing_info = ""
        if existing_topics:
            # é™åˆ¶ç°æœ‰ä¸“é¢˜æ•°é‡ä»¥é˜²æ­¢ Prompt è¿‡é•¿ï¼Œå–æœ€è¿‘ 50 ä¸ªå³å¯ï¼ˆå‡è®¾æŒ‰æ—¶é—´å€’åºæˆ–ç›¸å…³æ€§æ’åºï¼‰
            # è¿™é‡Œè°ƒç”¨è€…ä¼ å…¥çš„é€šå¸¸æ˜¯ active ä¸“é¢˜ï¼Œæ•°é‡å¯èƒ½è¾ƒå¤šï¼Œæˆªå–ä¸€ä¸‹æ¯”è¾ƒå®‰å…¨
            limit_existing = existing_topics[:50]
            existing_info = "ã€å·²å­˜åœ¨çš„ä¸“é¢˜åˆ—è¡¨ï¼ˆç”¨äºæŸ¥é‡å’Œåˆ¤æ–­å»¶ä¼¸å…³ç³»ï¼‰ã€‘:\n"
            for t in limit_existing:
                existing_info += f"- {t.get('name')}: {t.get('description')}\n"
            existing_info += "\n"

        system_prompt = (
            "ä½ æ˜¯ä¸“é¢˜è´¨é‡å®¡æ ¸å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹å¾…åˆ›å»ºçš„ä¸“é¢˜æ˜¯å¦ç¬¦åˆæ ‡å‡†ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. ã€é€šè¿‡ã€‘ï¼š\n"
            "   - å…·ä½“çš„æŸä¸ªæ–°é—»äº‹ä»¶ï¼ˆå¦‚â€œSpaceXæ˜Ÿèˆ°ç¬¬äº”æ¬¡è¯•é£â€ã€â€œOpenAIå‘å¸ƒSoraæ¨¡å‹â€ï¼‰ã€‚\n"
            "   - å…·ä½“çš„å†²çªæˆ–ç¾éš¾ï¼ˆå¦‚â€œæŸåœ°å‘ç”Ÿ6.0çº§åœ°éœ‡â€ï¼‰ã€‚\n"
            "   - å…·ä½“çš„æ”¿ç­–å‘å¸ƒï¼ˆå¦‚â€œå¤®è¡Œä¸‹è°ƒå­˜æ¬¾å‡†å¤‡é‡‘ç‡â€ï¼‰ã€‚\n"
            "   - ä¸ç°æœ‰ä¸“é¢˜æ— é‡å¤ï¼Œä¸”ä¸æ˜¯ç°æœ‰ä¸“é¢˜çš„ç®€å•å»¶ä¼¸ã€‚\n"
            "2. ã€æ‹’ç»ã€‘ï¼š\n"
            "   - è¿‡äºå®½æ³›çš„è¡Œä¸šåŠ¨æ€ï¼ˆå¦‚â€œAIè¡Œä¸šæ–°åŠ¨å‘â€ã€â€œæ–°èƒ½æºæ±½è½¦å¸‚åœºåˆ†æâ€ï¼‰ã€‚\n"
            "   - ä»…ä»…æ˜¯å…¬å¸æˆ–äººç‰©çš„é›†åˆï¼ˆå¦‚â€œOpenAIä¸å­—èŠ‚è·³åŠ¨â€ã€â€œé©¬æ–¯å…‹ç›¸å…³æ–°é—»â€ï¼‰ã€‚\n"
            "   - å‘¨æœŸæ€§çš„ä¸€èˆ¬æ±‡æ€»ï¼ˆå¦‚â€œæœ¬å‘¨è´¢ç»è¦é—»â€ï¼‰ã€‚\n"
            "   - ä¸ç°æœ‰ä¸“é¢˜é«˜åº¦é‡å¤ï¼ˆè¯­ä¹‰ç›¸åŒï¼‰ã€‚\n"
            "   - æ˜¯ç°æœ‰ä¸“é¢˜çš„åç»­è¿›å±•æˆ–å»¶ä¼¸ï¼ˆåº”å½’å…¥æ—§ä¸“é¢˜ï¼Œè€Œéåˆ›å»ºæ–°ä¸“é¢˜ï¼‰ã€‚\n"
            "   - æ²¡æœ‰ä»»ä½•å…·ä½“åŠ¨è¯æˆ–äº‹ä»¶çš„çŸ­è¯­ã€‚\n"
            "   - ç”±å¤šä¸ªéå¼ºå…³è”ä¸»ä½“ç»„æˆçš„ä¸¤ä¸ªäº‹ä»¶åˆå¹¶åçš„çŸ­è¯­ï¼ˆå¦‚â€œæ—¥æ–¹å®˜å‘˜æ‹¥æ ¸è¨€è®ºåŠé–å›½ç¥ç¤¾äº‰è®®â€ï¼‰ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'index' (æ•´æ•°), 'valid' (å¸ƒå°”å€¼), 'reason' (ç®€çŸ­ç†ç”±)ã€‚\n"
            "ä¾‹å¦‚ï¼š[{\"index\": 0, \"valid\": true, \"reason\": \"å…·ä½“çªå‘äº‹ä»¶\"}, {\"index\": 1, \"valid\": false, \"reason\": \"è¿‡äºå®½æ³›\"}]"
        )
        
        user_prompt = f"{existing_info}å¾…å®¡æ ¸ä¸“é¢˜åˆ—è¡¨ï¼š\n"
        for i, t in enumerate(topics):
            user_prompt += f"[{i}] åç§°ï¼š{t.get('name')}\n    æè¿°ï¼š{t.get('description')}\n\n"
            
        prefer_backup = self._get_prefer_backup("TOPIC_EVAL")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            logger.warning("âš ï¸ ä¸“é¢˜è´¨é‡è¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤å…¨éƒ¨ä¿ç•™")
            return topics
            
        try:
            clean = res.replace("```json", "").replace("```", "").strip()
            # å°è¯•æå– JSON æ•°ç»„
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1:
                clean = clean[start : end + 1]
                
            results = json.loads(clean)
            
            valid_topics = []
            if isinstance(results, list):
                for item in results:
                    idx = item.get("index")
                    is_valid = item.get("valid")
                    reason = item.get("reason", "æ— ç†ç”±")
                    
                    if isinstance(idx, int) and 0 <= idx < len(topics):
                        topic_name = topics[idx].get("name")
                        if is_valid:
                            logger.info(f"   âœ… [é€šè¿‡] {topic_name}: {reason}")
                            valid_topics.append(topics[idx])
                        else:
                            logger.info(f"   âŒ [æ‹’ç»] {topic_name}: {reason}")
            else:
                # Fallback if structure is wrong
                logger.warning("   âš ï¸ è´¨é‡è¯„ä¼°è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè§£æå¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰")
                return topics

            if len(valid_topics) < len(topics):
                removed_count = len(topics) - len(valid_topics)
                logger.info(f"ğŸ—‘ï¸ è¿‡æ»¤æ‰äº† {removed_count} ä¸ªå®½æ³›/ä½è´¨é‡ä¸“é¢˜")
                
            return valid_topics
            
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æè´¨é‡è¯„ä¼°ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return topics

    async def propose_topics_from_titles(self, titles: List[str]) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - `titles`: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        
        è¾“å‡º:
        - æç‚¼å‡ºçš„ä¸“é¢˜åˆ—è¡¨ [{"name": "...", "description": "..."}, ...]
        
        ä½œç”¨:
        - ä»å¤§é‡æ ‡é¢˜ä¸­èšåˆå‡ºæ ¸å¿ƒä¸“é¢˜
        """
        if not titles:
            return []
            
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œèšåˆå‡ºè¿‘æœŸå‘ç”Ÿçš„å…·ä½“ã€ç»†é¢—ç²’åº¦çš„ä¸“é¢˜äº‹ä»¶ã€‚"
        
        # é™åˆ¶æ•°é‡ä»¥é˜²è¶…é•¿
        limit_n = settings.TOPIC_AGGREGATION_TOP_N
        titles_subset = titles[:limit_n]
        titles_str = "\n".join([f"- {t}" for t in titles_subset])
        
        # åŠ¨æ€è·å–ä¸“é¢˜æ•°é‡èŒƒå›´
        count_range = settings.TOPIC_GENERATION_COUNT or "1-5"
        min_count, max_count = 1, 5
        try:
            parts = count_range.split("-")
            if len(parts) == 2:
                min_count = int(parts[0].strip())
                max_count = int(parts[1].strip())
        except Exception:
            pass
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼Œè¯†åˆ«å‡º {min_count} è‡³ {max_count} ä¸ªï¼ˆä¸¥æ ¼é™åˆ¶æ•°é‡ï¼‰å…·ä½“çš„ã€ç»†é¢—ç²’åº¦çš„çƒ­é—¨ä¸“é¢˜äº‹ä»¶ã€‚
**å…³é”®è¦æ±‚**ï¼š
1. **ä¸¥æ ¼éµå®ˆæ•°é‡é™åˆ¶**ï¼šè¾“å‡ºçš„ä¸“é¢˜æ•°é‡å¿…é¡»åœ¨ {min_count} åˆ° {max_count} ä¹‹é—´ï¼Œç»å¯¹ä¸èƒ½è¶…è¿‡ {max_count} ä¸ªã€‚
2. **æ‹’ç»å®å¤§å™äº‹**ï¼šä¸è¦ç”Ÿæˆç±»ä¼¼â€œèµ„æœ¬å¸‚åœºä¸ç›‘ç®¡â€ã€â€œèˆªå¤©ä¸å›½é˜²è¿›å±•â€ã€â€œå›½é™…åœ°ç¼˜æ”¿æ²»â€è¿™æ ·å®½æ³›çš„è¡Œä¸šæˆ–é¢†åŸŸåç§°ã€‚
3. **å…·ä½“äº‹ä»¶å¯¼å‘**ï¼šä¸“é¢˜å¿…é¡»æŒ‡å‘å…·ä½“çš„æŸä¸ªæ–°é—»äº‹ä»¶ï¼ˆå•ä¸ªå…·ä½“æ–°é—»äº‹ä»¶ï¼‰ã€‚ä¾‹å¦‚ï¼š
   - âŒ é”™è¯¯ç¤ºä¾‹ï¼šâ€œèµ„æœ¬å¸‚åœºåŠ¨æ€â€ã€â€œæ—¥æ–¹å®˜å‘˜æ‹¥æ ¸è¨€è®ºåŠé–å›½ç¥ç¤¾äº‰è®®â€
   - âœ… æ­£ç¡®ç¤ºä¾‹ï¼šâ€œè¯ç›‘ä¼šå‘å¸ƒå¸‚å€¼ç®¡ç†æ–°è§„â€ã€â€œSpaceXæ˜Ÿèˆ°ç¬¬äº”æ¬¡è¯•é£â€ã€â€œä¹Œå…‹å…°ä¸œéƒ¨æˆ˜äº‹å‡çº§â€ã€‚
4. **ç¼©å°èŒƒå›´**ï¼šåç§°ä¸­å°½é‡åŒ…å«å…·ä½“çš„å®ä½“ï¼ˆäººåã€åœ°åã€æœºæ„åï¼‰æˆ–å®šè¯­ï¼Œä»¥é™å®šèŒƒå›´ã€‚
5. **å¿½ç•¥çç¢**ï¼šå¿½ç•¥è¿‡äºå­¤ç«‹æˆ–ä½ä»·å€¼çš„æ–°é—»ã€‚

å¯¹äºæ¯ä¸ªä¸“é¢˜ï¼Œæä¾›ï¼š
1. "name": å…·ä½“çš„ä¸“é¢˜åç§°ï¼ˆä¸­æ–‡ï¼Œä¸è¶…è¿‡20å­—ï¼Œå¿…é¡»å…·ä½“ã€ç»†åŒ–ï¼‰ã€‚
2. "description": è¯¥ä¸“é¢˜äº‹ä»¶çš„ç®€è¦æè¿°ï¼ˆä¸­æ–‡ï¼Œ50-100å­—ï¼Œçº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸å«Markdownæˆ–HTMLï¼‰ã€‚
   - æ³¨æ„ï¼šåœ¨ description å­—ç¬¦ä¸²å†…éƒ¨ï¼Œè¯·å‹¿ä½¿ç”¨è‹±æ–‡åŒå¼•å· "ï¼Œå¦‚éœ€å¼•ç”¨è¯·ä½¿ç”¨ä¸­æ–‡å¼•å· â€œ â€ æˆ–å•å¼•å· 'ï¼Œä»¥å…ç ´å JSON æ ¼å¼ã€‚

æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š
{titles_str}

è¯·ä»…è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"name": "ä¸“é¢˜åç§°", "description": "ä¸“é¢˜æè¿°"}},
  ...
]
ä¸è¦åŒ…å«ä»»ä½• Markdown ä»£ç å—æ ‡è®°æˆ–å…¶ä»–æ–‡å­—ã€‚
"""
        logger.info(f"ğŸ¤– æ­£åœ¨ä» {len(titles_subset)} æ¡æ ‡é¢˜ä¸­æç‚¼ä¸“é¢˜...")
        prefer_backup = self._get_prefer_backup("TOPIC_NAME")
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        
        try:
            if not res:
                return []
            cleaned = res.replace("```json", "").replace("```", "").strip()
            
            # å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯ï¼ˆå¦‚å°¾éƒ¨é€—å·ã€æœªè½¬ä¹‰å¼•å·ç­‰ï¼‰
            # è¿™é‡Œå…ˆç®€å•å°è¯•ç›´æ¥è§£æ
            try:
                data = json.loads(cleaned)
            except json.JSONDecodeError:
                # å‡å¦‚è§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æå–
                import re
                pattern = r'\{\s*"name"\s*:\s*"(.*?)"\s*,\s*"description"\s*:\s*"(.*?)"\s*\}'
                matches = re.findall(pattern, cleaned, re.DOTALL)
                if matches:
                    data = [{"name": m[0], "description": m[1]} for m in matches]
                else:
                    # å†æ¬¡å°è¯•ï¼Œå¯èƒ½æ˜¯å•å¼•å·æˆ–å…¶ä»–æ ¼å¼
                    raise 

            if isinstance(data, list):
                valid_data = []
                for item in data:
                    if isinstance(item, dict) and "name" in item and "description" in item:
                        valid_data.append(item)
                
                # å†æ¬¡å¼ºåˆ¶æˆªæ–­ï¼Œé˜²æ­¢ AI è¿”å›è¿‡å¤š
                if len(valid_data) > max_count:
                    logger.warning(f"âš ï¸ AI è¿”å›ä¸“é¢˜æ•°é‡ ({len(valid_data)}) è¶…è¿‡é™åˆ¶ ({max_count})ï¼Œå·²å¼ºåˆ¶æˆªæ–­")
                    valid_data = valid_data[:max_count]
                
                logger.info(f"âœ… AI æç‚¼å‡º {len(valid_data)} ä¸ªæ½œåœ¨ä¸“é¢˜")
                return valid_data
            return []
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æä¸“é¢˜æç‚¼ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def extract_news_info(self, content: str) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `content`: åŸå§‹é¡µé¢å†…å®¹ï¼ˆHTML/XML/æ–‡æœ¬ï¼‰

        è¾“å‡º:
        - æ–°é—»æ¡ç›®åˆ—è¡¨ï¼ˆtitle/link/summaryï¼‰

        ä½œç”¨:
        - å½“å¸¸è§„ RSS/API è§£æå¤±è´¥æ—¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹ä»å†…å®¹ä¸­æŠ½å–æ–°é—»æ¡ç›®
        """

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–æ–°é—»æ¡ç›®ã€‚\n"
            "è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å« 'items' é”®ï¼Œå¯¹åº”ä¸€ä¸ªåˆ—è¡¨ã€‚åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š\n"
            "- 'title': æ–°é—»æ ‡é¢˜\n"
            "- 'link': æ–°é—»é“¾æ¥\n"
            "- 'summary': æ–°é—»æ‘˜è¦ï¼ˆå¦‚æœå†…å®¹ä¸­åŒ…å«æ‘˜è¦åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™åŸºäºæ ‡é¢˜ç”Ÿæˆç®€çŸ­è¯´æ˜ï¼‰\n"
            "å¦‚æœæ— æ³•æå–ï¼Œè¯·è¿”å› {'items': []}ã€‚"
        )
        user_prompt = f"å†…å®¹ï¼š\n{content[:20000]}"

        async def try_extract(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                return data.get("items", [])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.warning(f"AIæå–ç»“æœè§£æå¤±è´¥: {e}")
                return None

        if not (self._has_main_llm() or self._has_backup_llm()):
            return []

        routes = self._iter_llm_routes(self._get_prefer_backup("SUMMARY"))
        for r_idx, route in enumerate(routes):
            max_attempts = 3 if r_idx == 0 else 1
            for attempt in range(max_attempts):
                client = AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"])
                res = await try_extract(client, route["model"])
                if res is not None:
                    return res
                await asyncio.sleep(1)

        return []

    def _normalize_category(self, raw_category: str) -> str:
        """
        è¾“å…¥:
        - `raw_category`: æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†ç±»

        è¾“å‡º:
        - è§„èŒƒåŒ–åçš„åˆ†ç±»åç§°ï¼ˆè½åˆ° `settings.NEWS_CATEGORIES` ä¹‹ä¸€ï¼‰

        ä½œç”¨:
        - å°†æ¨¡å‹å¯èƒ½å‡ºç°çš„è¿‘ä¼¼åˆ†ç±»æ˜ å°„ä¸ºç³»ç»Ÿå†…ç½®åˆ†ç±»ï¼Œå‡å°‘è„æ•°æ®
        """

        if not raw_category:
            return "å…¶ä»–"
        if raw_category in settings.NEWS_CATEGORIES:
            return raw_category
        for cat in settings.NEWS_CATEGORIES:
            if raw_category in cat or cat in raw_category:
                return cat
        return "å…¶ä»–"

    async def analyze_sentiment(self, title: str, content: str = "") -> Dict:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ‘˜è¦æˆ–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æƒ…æ„Ÿåˆ†æç»“æœï¼ˆscore/label/category/region/keywords/entitiesï¼‰

        ä½œç”¨:
        - å¯¹å•æ¡æ–°é—»è¿›è¡Œæ·±åº¦èˆ†æƒ…åˆ†æï¼Œä¸»é€šé“å¤±è´¥æ—¶é™çº§åˆ°å¤‡ç”¨é€šé“
        """

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–°é—»æ ‡é¢˜å’Œå†…å®¹ï¼ˆæ‘˜è¦ï¼‰ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š\n"
            "1. æƒ…æ„Ÿå€¾å‘æ ‡ç­¾ (label): åªèƒ½æ˜¯ 'æ­£é¢'ã€'ä¸­ç«‹' æˆ– 'è´Ÿé¢'ã€‚\n"
            "2. æƒ…æ„Ÿåˆ†æ•° (score): 0åˆ°100ä¹‹é—´çš„æ•´æ•°ã€‚0ä»£è¡¨æåº¦è´Ÿé¢ï¼Œ50ä»£è¡¨ä¸­ç«‹ï¼Œ100ä»£è¡¨æåº¦æ­£é¢ã€‚\n"
            f"3. æ‰€å±é¢†åŸŸ (category): å¿…é¡»ä¸¥æ ¼ä»[{categories_str}]ä¸­é€‰æ‹©æœ€åˆé€‚çš„1ä¸ªé¢†åŸŸï¼Œç¦æ­¢åˆ›é€ æ–°åˆ†ç±»ã€ç¦æ­¢ä½¿ç”¨â€œå…¶ä»–â€ä½œä¸ºæ‰€å±é¢†åŸŸã€‚\n"
            "4. æ¶‰åŠå›½å®¶ (region): åªèƒ½è¾“å‡ºå›½å®¶åç§°ï¼ˆä¸€ä¸ªæˆ–å¤šä¸ªï¼‰ï¼Œç¦æ­¢è¾“å‡ºçœ/å¸‚/åŒº/å¿ç­‰è¡Œæ”¿åŒºåˆ’ï¼Œç¦æ­¢è¾“å‡ºâ€œä¸œäºš/æ¬§æ´²/ä¸­ä¸œâ€ç­‰å¤§åŒºã€‚å…è®¸ç¤ºä¾‹ï¼š'ä¸­å›½'ã€'ç¾å›½'ã€'æ—¥æœ¬'ã€'éŸ©å›½'ã€'ä¿„ç½—æ–¯'ã€'è‹±å›½'ã€'æ³•å›½'ã€'å¾·å›½'ã€'å°åº¦'ã€'åŠ æ‹¿å¤§'ã€'æ¾³å¤§åˆ©äºš'ç­‰ã€‚å¦‚æœæ¶‰åŠå¤šä¸ªå›½å®¶ï¼Œè¯·ç”¨é€—å·åˆ†éš”ï¼ˆå¦‚'ä¸­å›½,ç¾å›½'ï¼‰ã€‚å¦‚æœç¡®å®æ— æ³•åˆ¤æ–­ï¼Œè¯·è¾“å‡º'å…¨çƒ'ã€‚\n"
            "5. å…³é”®è¯ (keywords): æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆå®è¯ï¼‰ï¼Œæ’é™¤'çš„'ã€'äº†'ç­‰åœç”¨è¯ã€‚\n"
            "6. æ¶‰åŠå®ä½“ (entities): æå–æ–°é—»ä¸­æ¶‰åŠçš„äººåã€å…¬å¸åã€ç»„ç»‡æœºæ„åç­‰ã€‚\n"
            "è¿”å›æ ¼å¼å¿…é¡»æ˜¯åˆæ³•çš„ JSON å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š\n"
            "{\n"
            '  "score": 85,\n'
            '  "label": "æ­£é¢",\n'
            '  "category": "ç§‘æŠ€/ç§‘å­¦",\n'
            '  "region": "ä¸­å›½",\n'
            '  "keywords": ["äººå·¥æ™ºèƒ½", "åˆ›æ–°", "å‘å¸ƒ"],\n'
            '  "entities": ["OpenAI", "Sam Altman"]\n'
            "}"
        )
        user_prompt = f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:1000]}"

        async def try_analyze(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                if "score" in data and "label" in data:
                    data["category"] = self._normalize_category(data.get("category", ""))
                    data["region"] = normalize_regions_to_countries(data.get("region"))
                    if not data.get("region") or data.get("region") in ["å…¶ä»–", "æœªçŸ¥"]:
                        data["region"] = "å…¨çƒ"
                    return data
            except AIConfigurationError:
                raise
            except Exception:
                pass
            return None

        routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
        for route in routes:
            client = AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"])
            res = await try_analyze(client, route["model"])
            if res:
                return res

        return {
            "score": 50,
            "label": "ä¸­ç«‹",
            "category": "å…¶ä»–",
            "region": "å…¶ä»–",
            "keywords": [],
            "entities": [],
        }

    async def batch_analyze_sentiment(self, news_items: List[Dict]) -> Dict[int, Dict]:
        """
        è¾“å…¥:
        - `news_items`: å¾…åˆ†ææ–°é—»åˆ—è¡¨ï¼ˆè‡³å°‘åŒ…å« id/titleï¼‰

        è¾“å‡º:
        - `id -> åˆ†æç»“æœ` çš„æ˜ å°„

        ä½œç”¨:
        - å¯¹å¤šæ¡æ–°é—»è¿›è¡Œæ‰¹é‡å¿«é€Ÿåˆ†æï¼Œç”¨äºæå‡ååä¸é™ä½æˆæœ¬
        """

        if not news_items:
            return {}

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–°é—»æ ‡é¢˜ï¼Œå¿«é€Ÿåˆ¤æ–­å…¶æƒ…æ„Ÿå€¾å‘å’Œæ‰€å±é¢†åŸŸã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. æƒ…æ„Ÿå€¾å‘ (label): 'æ­£é¢'ã€'ä¸­ç«‹' æˆ– 'è´Ÿé¢'ã€‚\n"
            "2. æƒ…æ„Ÿåˆ†æ•° (score): 0-100 (è´Ÿé¢<40, ä¸­ç«‹40-60, æ­£é¢>60)ã€‚\n"
            f"3. æ‰€å±é¢†åŸŸ (category): å¿…é¡»ä¸¥æ ¼ä»[{categories_str}]ä¸­é€‰æ‹©æœ€åˆé€‚çš„1ä¸ªï¼Œç¦æ­¢åˆ›é€ æ–°åˆ†ç±»ã€ç¦æ­¢ä½¿ç”¨â€œå…¶ä»–â€ä½œä¸ºæ‰€å±é¢†åŸŸã€‚\n"
            "4. æ¶‰åŠå›½å®¶ (region): åªèƒ½è¾“å‡ºå›½å®¶åç§°ï¼ˆä¸€ä¸ªæˆ–å¤šä¸ªï¼‰ï¼Œç¦æ­¢è¾“å‡ºçœ/å¸‚/åŒº/å¿ç­‰è¡Œæ”¿åŒºåˆ’ï¼Œç¦æ­¢è¾“å‡ºâ€œä¸œäºš/æ¬§æ´²/ä¸­ä¸œâ€ç­‰å¤§åŒºã€‚å…è®¸ç¤ºä¾‹ï¼š'ä¸­å›½'ã€'ç¾å›½'ã€'æ—¥æœ¬'ã€'éŸ©å›½'ã€'ä¿„ç½—æ–¯'ã€'è‹±å›½'ã€'æ³•å›½'ã€'å¾·å›½'ã€'å°åº¦'ã€'åŠ æ‹¿å¤§'ã€'æ¾³å¤§åˆ©äºš'ç­‰ã€‚å¦‚æœæ¶‰åŠå¤šä¸ªå›½å®¶ï¼Œè¯·ç”¨é€—å·åˆ†éš”ï¼ˆå¦‚'ä¸­å›½,ç¾å›½'ï¼‰ã€‚å¦‚æœç¡®å®æ— æ³•åˆ¤æ–­ï¼Œè¯·è¾“å‡º'å…¨çƒ'ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯åˆæ³•çš„ JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« id, label, score, category, regionã€‚ä¾‹å¦‚ï¼š\n"
            '[{"id": 101, "label": "æ­£é¢", "score": 80, "category": "æ”¿æ²»å†›äº‹", "region": "ä¸­å›½"}, ...]'
        )

        user_prompt = "è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼š\n"
        for item in news_items:
            user_prompt += f"[ID:{item['id']}] {item['title']}\n"

        try:
            routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
            res: Optional[str] = None
            for route in routes:
                client = AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"])
                res = await self._call_llm(client, route["model"], user_prompt, system_prompt)
                if res:
                    break

            if not res:
                return {}

            clean_res = res.strip()
            # æ— è®ºæ˜¯å¦åŒ…å« markdown æ ‡è®°ï¼Œéƒ½ä¼˜å…ˆå°è¯•æå– JSON æ•°ç»„
            start = clean_res.find("[")
            end = clean_res.rfind("]")
            if start != -1 and end != -1:
                clean_res = clean_res[start : end + 1]
            else:
                # å…œåº•æ¸…ç†
                clean_res = clean_res.replace("```json", "").replace("```", "").strip()

            results_list = json.loads(clean_res)

            result_map: Dict[int, Dict] = {}
            if isinstance(results_list, list):
                for item in results_list:
                    if "id" in item:
                        if "category" in item:
                            item["category"] = self._normalize_category(item["category"])
                        item["region"] = normalize_regions_to_countries(item.get("region"))
                        result_map[item["id"]] = item
            return result_map

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {}

    def _get_prefer_backup(self, route_key: str) -> bool:
        """æ ¹æ®é…ç½®é”®è·å–æ˜¯å¦ä¼˜å…ˆä½¿ç”¨å¤‡ç”¨ AI"""
        route_value = settings.AI_ROUTE.get(route_key, "main").lower()
        return route_value == "backup"

    async def generate_summary(self, title: str, content: str, max_words: int = 300) -> Optional[str]:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ­£æ–‡
        - `max_words`: æœ€å¤§å­—æ•°é™åˆ¶ (é»˜è®¤ 300)

        è¾“å‡º:
        - æ‘˜è¦æ–‡æœ¬ (å¦‚æœå¤±è´¥è¿”å› None)

        ä½œç”¨:
        - ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ–°é—»æ‘˜è¦ï¼Œæ”¯æŒä¸»å¤‡åˆ‡æ¢
        """
        if not content:
            return None
        system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä¸æ–°é—»æ ‡é¢˜ç›¸å…³çš„ä¿¡æ¯æ€»ç»“ä¸º{max_words}å­—å·¦å³çš„çº¯æ–‡å­—å†…å®¹ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. **å†…å®¹è¯¦å®**ï¼šæ‹’ç»ç®€é™‹æ¦‚æ‹¬ï¼Œå¿…é¡»ä¿ç•™ **å…·ä½“äººåã€åœ°åã€æ•°æ®ã€ç‰©å“åç§°** ç­‰å…³é”®å®ä½“ä¿¡æ¯ï¼Œé¿å…è¿‡åº¦æŠ½è±¡ã€‚\n"
            "2. **å»å™ª**ï¼šå»é™¤å¹¿å‘Šã€é“¾æ¥ç­‰æ— å…³ä¿¡æ¯ã€‚\n"
            "3. **çº¯æ–‡æœ¬**ï¼šä¸è¦ä½¿ç”¨ HTML æ ‡ç­¾ã€‚\n"
            "4. **ç›´æ¥è¾“å‡º**ï¼šç›´æ¥å¼€å§‹è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•â€œå¥½çš„â€ã€â€œæ ¹æ®æ‚¨çš„è¦æ±‚â€ã€â€œæ‘˜è¦å¦‚ä¸‹â€ç­‰å®¢å¥—è¯æˆ–å‰ç¼€ã€‚"
        )
        user_prompt = f"æ ‡é¢˜ï¼š{title}\n\næ­£æ–‡ï¼š{content[:100000]}"

        prefer_backup = self._get_prefer_backup("SUMMARY")
        routes = self._iter_llm_routes(prefer_backup)
        for route in routes:
            res = await self._call_llm(
                AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]),
                route["model"],
                user_prompt,
                system_prompt,
            )
            if res:
                return res
        return None

    async def _call_llm_with_routes(
        self,
        user_prompt: str,
        system_prompt: str,
        prefer_backup: Optional[bool] = None,
    ) -> Optional[str]:
        prefer = False if prefer_backup is None else prefer_backup
        routes = self._iter_llm_routes(prefer)
        for i, route in enumerate(routes):
            res = await self._call_llm(
                AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]),
                route["model"],
                user_prompt,
                system_prompt,
            )
            if res:
                return res
            
            # If we are here, the current route failed or returned empty content
            if i < len(routes) - 1:
                next_route = routes[i+1]
                logger.warning(f"âš ï¸ è·¯ç”± {route['model']} ({route['type']}) è°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºï¼Œå°è¯•åˆ‡æ¢åˆ° -> {next_route['model']} ({next_route['type']})")
            else:
                logger.error(f"âŒ æ‰€æœ‰å¯ç”¨ AI è·¯ç”±å‡è°ƒç”¨å¤±è´¥")
        return None



    async def verify_topic_match_batch(self, tasks: List[Dict[str, str]]) -> List[Tuple[bool, str]]:
        """
        è¾“å…¥:
        - tasks: List of {"topic_name": ..., "topic_summary": ..., "news_title": ..., "news_summary": ...}

        è¾“å‡º:
        - List of (is_match, reason)
        """
        if not tasks:
            return []

        # Log request content
        logger.info(f"ğŸ¤– æ‰¹é‡æ ¸éªŒä¸“é¢˜åŒ¹é…: {len(tasks)} ç»„")
        for i, t in enumerate(tasks[:3]):  # Log first 3 for preview
            logger.info(f"   [{i}] Topic: {t['topic_name']} <-> News: {t['news_title']}")

        system_prompt = (
            "ä½ æ˜¯äº‹ä»¶ä¸€è‡´æ€§åˆ¤å®šåŠ©æ‰‹ã€‚è¯·æ‰¹é‡åˆ¤æ–­ä»¥ä¸‹æ¯ç»„æ–°é—»æ˜¯å¦å±äºå¯¹åº”ä¸“é¢˜è¿½è¸ªçš„åŒä¸€æ–°é—»äº‹ä»¶ï¼ˆæˆ–å…¶ç›´æ¥åç»­è¿›å±•ï¼‰ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1) è§†ä¸ºåŒä¸€äº‹ä»¶ï¼šæ ¸å¿ƒä¸»ä½“/åœ°ç‚¹/å…³é”®äº‹å®ä¸€è‡´ï¼Œæˆ–æ˜æ˜¾æ˜¯åŒä¸€äº‹ä»¶çš„åç»­è¿›å±•ï¼ˆé€šæŠ¥ã€è°ƒæŸ¥ã€è¿›å±•ã€å›åº”ã€äºŒæ¬¡å½±å“ï¼‰ã€‚\n"
            "2) è§†ä¸ºä¸åŒäº‹ä»¶ï¼šä¸»ä½“æ— å…³ã€ä¸åŒåœ°åŒºä¸åŒä¸»ä½“çš„ç›¸ä¼¼è¯é¢˜ã€ä»…åŒç±»æ³›è¯é¢˜ä½†æ— å…±åŒäº‹å®ã€‚\n"
            "3) å½“ä¿¡æ¯ä¸è¶³æ—¶ï¼Œå€¾å‘äºè¿”å› falseã€‚\n"
            "è¿”å›æ ¼å¼ï¼šå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'match' (å¸ƒå°”å€¼) å’Œ 'reason' (ç®€çŸ­ç†ç”±)ã€‚\n"
            "ä¾‹å¦‚ï¼š[{\"match\": true, \"reason\": \"æ ¸å¿ƒäº‹å®ä¸€è‡´\"}, {\"match\": false, \"reason\": \"ä¸»ä½“ä¸åŒ\"}]"
        )

        user_prompt = "è¯·åˆ¤æ–­ä»¥ä¸‹å„ç»„åŒ¹é…æƒ…å†µï¼š\n"
        for idx, task in enumerate(tasks):
            user_prompt += (
                f"--- ç¬¬ {idx+1} ç»„ ---\n"
                f"ã€ä¸“é¢˜ã€‘{task['topic_name']}\n"
                f"ã€ä¸“é¢˜æ¦‚è§ˆã€‘{(task['topic_summary'] or '')[:300]}\n"
                f"ã€æ–°é—»æ ‡é¢˜ã€‘{task['news_title']}\n"
                f"ã€æ–°é—»æ‘˜è¦ã€‘{(task['news_summary'] or '')[:300]}\n"
            )

        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return [(False, "AIè°ƒç”¨å¤±è´¥")] * len(tasks)

        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            results = json.loads(clean)
            
            output = []
            if isinstance(results, list):
                for item in results:
                    is_match = bool(item.get("match", False))
                    reason = item.get("reason", "æ— ç†ç”±")
                    output.append((is_match, reason))
                
                # Ensure length matches
                if len(output) < len(tasks):
                    output.extend([(False, "è¿”å›æ•°é‡ä¸è¶³")] * (len(tasks) - len(output)))
                
                return output[:len(tasks)]
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¸“é¢˜æ ¸éªŒè§£æå¤±è´¥: {e}")

        return [(False, "è§£æå¼‚å¸¸")] * len(tasks)




    async def generate_daily_timeline_events(self, date_str: str, news_items: List[Dict[str, Any]], topic_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - date_str: "YYYY-MM-DD"
        - news_items: List of {"id": ..., "title": ..., "summary": ...}
        - topic_name: Optional topic name to focus on

        è¾“å‡º:
        - List of events: [{"content": "...", "source_ids": [id1, id2...]}, ...]
        """
        if not news_items:
            return []

        logger.info(f"ğŸ¤– æ­£åœ¨ä¸º {date_str} åˆæˆæ—¶é—´è½´äº‹ä»¶ (å…± {len(news_items)} æ¡æ–°é—»)...")
        
        focus_instruction = ""
        if topic_name:
            focus_instruction = f"7. **ä¸“é¢˜èšç„¦**ï¼šå½“å‰ä¸“é¢˜ä¸ºâ€œ{topic_name}â€ï¼Œè¯·ä¸¥æ ¼ä¸“æ³¨äºä¸è¯¥ä¸“é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚å¦‚æœæ˜¯æ— å…³çš„å™ªéŸ³æ–°é—»ï¼Œè¯·ç›´æ¥å¿½ç•¥ã€‚å¦‚æœæ‰€æœ‰æ–°é—»éƒ½ä¸è¯¥ä¸“é¢˜æ— å…³ï¼Œè¿”å›ç©ºæ•°ç»„ã€‚\n"

        system_prompt = (
            "ä½ æ˜¯æ—¶é—´è½´äº‹ä»¶åˆæˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æŸä¸€å¤©çš„æ–°é—»åˆ—è¡¨ï¼Œå°†å…¶åˆæˆä¸º 1-2 ä¸ªå…³é”®çš„æ—¶é—´è½´èŠ‚ç‚¹äº‹ä»¶ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. **åˆå¹¶åŒç±»é¡¹**ï¼šå°†æŠ¥é“åŒä¸€äº‹ä»¶çš„ä¸åŒæ–°é—»åˆå¹¶ä¸ºä¸€ä¸ªäº‹ä»¶èŠ‚ç‚¹ã€‚\n"
            "2. **å†…å®¹ç²¾ç‚¼**ï¼šæ¯ä¸ªäº‹ä»¶æè¿°æ§åˆ¶åœ¨ 100 å­—ä»¥å†…ï¼Œæ¦‚æ‹¬æ ¸å¿ƒäº‹å®ã€‚\n"
            "3. **å…³è”æ¥æº**ï¼šå¯¹äºæ¯ä¸ªç”Ÿæˆçš„äº‹ä»¶ï¼Œå¿…é¡»åˆ—å‡ºæ”¯æŒè¯¥äº‹ä»¶çš„æ‰€æœ‰æ–°é—» ID (source_ids)ã€‚\n"
            "4. **æ•°é‡é™åˆ¶**ï¼šæ¯å¤©æœ€å¤šç”Ÿæˆ 2 ä¸ªäº‹ä»¶ã€‚å¦‚æœæœ‰å¤šä»¶å¤§äº‹ï¼Œå–æœ€é‡è¦çš„ 2 ä»¶ï¼›å¦‚æœæ˜¯åŒä¸€ä»¶äº‹çš„å¤šä¸ªæ–¹é¢ï¼Œå°½é‡åˆå¹¶ä¸º 1 ä»¶ã€‚\n"
            "5. **ç›´æ¥è¾“å‡º**ï¼šä¸è¦åŒ…å«â€œå¥½çš„â€ã€â€œæ ¹æ®æä¾›çš„æ–°é—»â€ç­‰å®¢å¥—è¯ï¼Œç›´æ¥è¿”å›ç»“æœã€‚\n"
            "6. **è¿”å›æ ¼å¼**ï¼šä»…è¿”å› JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š[{\"content\": \"äº‹ä»¶æè¿°...\", \"source_ids\": [1, 3]}, ...]\n"
            f"{focus_instruction}"
        )

        user_prompt = f"æ—¥æœŸï¼š{date_str}\n"
        if topic_name:
            user_prompt += f"ä¸“é¢˜åç§°ï¼š{topic_name}\n"
        user_prompt += "æ–°é—»åˆ—è¡¨ï¼š\n"
        for item in news_items:
            user_prompt += f"[ID: {item['id']}] {item['title']}\næ‘˜è¦: {(item['summary'] or '')[:100]}\n\n"

        prefer_backup = self._get_prefer_backup("TOPIC_TIMELINE")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            return []
            
        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            events = json.loads(clean)
            return events
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£ææ—¶é—´è½´åˆæˆç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def check_topic_duplicate(
        self,
        new_name: str,
        new_desc: str,
        existing_name: str,
        existing_desc: str
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­ä¸¤ä¸ªä¸“é¢˜æ˜¯å¦å®è´¨ä¸Šæ˜¯åŒä¸€ä¸ªäº‹ä»¶ã€‚
        """
        system_prompt = (
            "ä½ æ˜¯ä¸“é¢˜å»é‡åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªä¸“é¢˜æ˜¯å¦æŒ‡çš„æ˜¯åŒä¸€ä¸ªå…·ä½“çš„æ–°é—»äº‹ä»¶ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. ã€è§†ä¸ºç›¸åŒã€‘ï¼š\n"
            "   - æŒ‡å‘åŒä¸€ä¸ªæ ¸å¿ƒçªå‘äº‹ä»¶ï¼ˆå¦‚â€œæŸåœ°åœ°éœ‡â€ä¸â€œæŸåœ°å‘ç”Ÿ6.0çº§åœ°éœ‡â€ï¼‰ã€‚\n"
            "   - ä»…ä»…æ˜¯å‘½åè§’åº¦ä¸åŒï¼ˆå¦‚â€œSpaceXæ˜Ÿèˆ°å‘å°„â€ä¸â€œæ˜Ÿèˆ°ç¬¬äº”æ¬¡è¯•é£â€ï¼‰ã€‚\n"
            "   - ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­é›†æˆ–åˆæœŸé˜¶æ®µï¼Œä¸”æ ¸å¿ƒäº‹å®å®Œå…¨é‡åˆã€‚\n"
            "2. ã€è§†ä¸ºä¸åŒã€‘ï¼š\n"
            "   - ä¸åŒçš„ç‹¬ç«‹äº‹ä»¶ï¼ˆå¦‚â€œä¿„ä¹Œå†²çªâ€ä¸â€œå·´ä»¥å†²çªâ€ï¼‰ã€‚\n"
            "   - åŒä¸€ç±»åˆ«çš„ä¸åŒä¸ªä½“ï¼ˆå¦‚â€œæŸå…¬å¸å‘å¸ƒè´¢æŠ¥â€ä¸â€œå¦ä¸€å…¬å¸å‘å¸ƒè´¢æŠ¥â€ï¼‰ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šä»…è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¾‹å¦‚ï¼š{\"duplicate\": true, \"reason\": \"...\"}"
        )
        user_prompt = (
            f"ã€ä¸“é¢˜Aã€‘åç§°ï¼š{new_name}\næè¿°ï¼š{new_desc[:500]}\n\n"
            f"ã€ä¸“é¢˜Bã€‘åç§°ï¼š{existing_name}\næè¿°ï¼š{existing_desc[:500]}"
        )
        
        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return False, "AIè°ƒç”¨å¤±è´¥"
            
        clean = res.strip()
        try:
            if "```" in clean:
                start = clean.find("{")
                end = clean.rfind("}")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            data = json.loads(clean)
            return bool(data.get("duplicate", False)), data.get("reason", "æ— ç†ç”±")
        except Exception:
            lowered = clean.lower()
            if "true" in lowered and "false" not in lowered:
                return True, "è§£æå¤±è´¥(fallback: true)"
            return False, "è§£æå¤±è´¥"

    async def stream_chat(self, query: str, context: str, model_type: str = "main") -> AsyncIterator[str]:
        """
        è¾“å…¥:
        - `query`: ç”¨æˆ·é—®é¢˜
        - `context`: ç›¸å…³æ–°é—»ä¸Šä¸‹æ–‡
        - `model_type`: ä½¿ç”¨ä¸»/å¤‡é€šé“ï¼ˆmain/backupï¼‰

        è¾“å‡º:
        - SSE å¯è¿­ä»£çš„å¢é‡æ–‡æœ¬ç‰‡æ®µ

        ä½œç”¨:
        - ä»¥æµå¼æ–¹å¼ä¸æ¨¡å‹å¯¹è¯ï¼Œé€‚é…å‰ç«¯å®æ—¶å±•ç¤º
        """
        # å¦‚æœè¯·æ±‚ä½¿ç”¨ main é€šé“ï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥é…ç½®è·¯ç”±æ˜¯å¦æŒ‡å®šäº†ä¼˜å…ˆå¤‡ç”¨
        if model_type == "main":
            if self._get_prefer_backup("CHAT"):
                model_type = "backup"

        if model_type == "backup":
            client = AsyncOpenAI(api_key=settings.BACKUP_AI_API_KEY, base_url=settings.BACKUP_AI_BASE_URL)
            model = settings.BACKUP_AI_MODEL
        else:
            client = AsyncOpenAI(api_key=settings.MAIN_AI_API_KEY, base_url=settings.MAIN_AI_BASE_URL)
            model = settings.MAIN_AI_MODEL

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–°é—»ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
            "å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†å›ç­”ï¼Œä½†è¯·æ³¨æ˜â€œæ ¹æ®å·²æœ‰æ–°é—»æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„è¡¥å……...â€ã€‚\n"
            "å›ç­”è¦ç®€æ´ã€å®¢è§‚ã€‚"
        )
        user_prompt = f"ã€æ–°é—»ä¸Šä¸‹æ–‡ã€‘:\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘: {query}"

        try:
            extra_body = {}
            if "modelscope" in str(client.base_url):
                extra_body["enable_thinking"] = False

            logger.info(f"å¼€å§‹æµå¼å¯¹è¯è¯·æ±‚: model={model}, stream=True")
            stream = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                stream=True,
                temperature=0.7,
                timeout=60,
                extra_body=extra_body if extra_body else None,
            )
            logger.info("æµå¼è¯·æ±‚å·²å»ºç«‹ï¼Œå¼€å§‹è¯»å– chunks")
            chunk_count = 0
            async for chunk in stream:
                logger.debug(f"Raw chunk received: {chunk}")
                if not chunk.choices:
                    logger.debug(f"Chunk without choices: {chunk}")
                    continue
                content = chunk.choices[0].delta.content
                if content:
                    chunk_count += 1
                    # logger.debug(f"Yielding content: {content!r}")
                    yield content
                else:
                    logger.debug(f"Chunk with empty content: {chunk}")
            logger.info(f"æµå¼ä¼ è¾“ç»“æŸ, å…±å‘é€ {chunk_count} ä¸ª chunks")
        except Exception as e:
            logger.error(f"èŠå¤©æµé”™è¯¯: {e}", exc_info=True)
            yield f"é”™è¯¯: {str(e)}"

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        è¾“å…¥:
        - `texts`: å¾…å‘é‡åŒ–æ–‡æœ¬åˆ—è¡¨

        è¾“å‡º:
        - å‘é‡åˆ—è¡¨ï¼ˆä¸è¾“å…¥ä¸€ä¸€å¯¹åº”ï¼›å¤±è´¥æ—¶è¿”å›ç©ºå‘é‡ï¼‰

        ä½œç”¨:
        - è°ƒç”¨ embedding æœåŠ¡ç”Ÿæˆå‘é‡ï¼Œç”¨äºè¯­ä¹‰æ£€ç´¢ä¸èšç±»
        """

        if not texts:
            return []
        if not self._has_embedding():
            return [[] for _ in texts]
        cleaned_texts = [t.replace("\n", " ").strip()[:1000] for t in texts]

        url = f"{settings.SILICONFLOW_BASE_URL.rstrip('/')}/embeddings"
        headers = {
            "Authorization": f"Bearer {settings.SILICONFLOW_API_KEY}",
            "Content-Type": "application/json",
        }

        all_embeddings: List[List[float]] = []
        batch_size = 20

        for i in range(0, len(cleaned_texts), batch_size):
            batch = cleaned_texts[i : i + batch_size]
            payload = {
                "model": settings.EMBEDDING_MODEL,
                "input": batch,
                "encoding_format": "float",
            }
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, headers=headers, json=payload, timeout=30) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            batch_res = sorted(data["data"], key=lambda x: x["index"])
                            all_embeddings.extend([x["embedding"] for x in batch_res])
                        elif resp.status == 401:
                             error_text = await resp.text()
                             logger.error(f"âŒ å‘é‡ API è®¤è¯å¤±è´¥ (401): {error_text}")
                             raise AIConfigurationError("Embedding API Key æ— æ•ˆ")
                        else:
                            logger.error(f"âŒ å‘é‡ API é”™è¯¯: {await resp.text()}")
                            all_embeddings.extend([[] for _ in batch])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ å‘é‡ç½‘ç»œé”™è¯¯: {e}")
                all_embeddings.extend([[] for _ in batch])
        return all_embeddings

    async def verify_cluster_batch(self, pairs: List[Dict[str, str]]) -> List[bool]:
        """
        è¾“å…¥:
        - `pairs`: å¾…æ ¸éªŒæ–°é—»å¯¹åˆ—è¡¨ï¼ˆleader/candidate æ ‡é¢˜ï¼‰

        è¾“å‡º:
        - å¸ƒå°”åˆ—è¡¨ï¼ˆä¸è¾“å…¥é¡ºåºä¸€è‡´ï¼Œè¡¨ç¤ºæ˜¯å¦åŒä¸€äº‹ä»¶ï¼‰

        ä½œç”¨:
        - ä½¿ç”¨å¤§æ¨¡å‹å¯¹ç›¸ä¼¼å€™é€‰è¿›è¡Œæ‰¹é‡æ ¸éªŒï¼Œå‡å°‘è¯¯åˆå¹¶
        """

        if not pairs:
            return []
        if not (self._has_main_llm() or self._has_backup_llm()):
            return [False] * len(pairs)

        system_prompt = (
            "è¯·åˆ¤æ–­åˆ—è¡¨ä¸­æ¯å¯¹æ–°é—»æ ‡é¢˜æ˜¯å¦å±äº **åŒä¸€ä¸ªæ–°é—»äº‹ä»¶çš„æŠ¥é“** , æ ‡å‡†é€‚å½“å®½æ¾ï¼Œå°½å¯èƒ½å°†ç›¸å…³å…³é”®è¯æ–°é—»åˆ¤æ–­ä¸ºåŒä¸€æ–°é—»äº‹ä»¶ã€‚\n"
            "åˆ¤å®šæ ‡å‡†ï¼š\n"
            "1. ã€è§†ä¸ºç›¸åŒã€‘ï¼š\n"
            "   - **æ ¸å¿ƒäº‹å®é‡åˆ**ï¼šæè¿°åŒä¸€ä¸ªå…·ä½“çš„çªå‘äº‹ä»¶ã€æ”¿ç­–å‘å¸ƒã€ç§‘æŠ€å‘ç°ç­‰ã€‚æ•°å­—ç•¥æœ‰å·®å¼‚ï¼ˆå¦‚â€œ1178åâ€ä¸â€œé€¾åƒåâ€ï¼‰æˆ–è¡¨è¿°ä¸åŒï¼ˆå¦‚â€œç¦äº§â€ä¸â€œç¦æ­¢ç”Ÿäº§â€ï¼‰è§†ä¸ºç›¸åŒã€‚\n"
            "   - **äº‹ä»¶å»¶ç»­ä¸å…³è”**ï¼šåŒä¸€å¤§äº‹ä»¶èƒŒæ™¯ä¸‹çš„ç›´æ¥åç»­æˆ–ååº”ï¼ˆå¦‚â€œåœ°éœ‡å‘ç”Ÿâ€ä¸â€œåœ°éœ‡åå¼•å‘æµ·å•¸â€ã€â€œæŸäººå¯¹åœ°éœ‡çš„ååº”â€ï¼‰ã€‚\n"
            "   - **æŠ€æœ¯/ä¸“ä¸šè¯é¢˜**ï¼šåŒä¸€æ¼æ´ã€åŒä¸€äº§å“çš„ä¸åŒè§£è¯»ï¼ˆå¦‚â€œReactæ¼æ´â€ï¼‰ã€‚\n"
            "2. ã€è§†ä¸ºä¸åŒã€‘ï¼š\n"
            "   - **ä¸»ä½“å®Œå…¨æ— å…³**ï¼šå¦‚â€œé»„é‡‘â€ä¸â€œç™½é“¶â€ï¼Œâ€œè‹¹æœâ€ä¸â€œé¦™è•‰â€ã€‚\n"
            "   - **æ˜ç¡®çš„ä¸åŒæœŸæ•°**ï¼šå¦‚â€œ1æœˆ1æ—¥æ—¥æŠ¥â€ä¸â€œ1æœˆ2æ—¥æ—¥æŠ¥â€ã€‚\n"
            "è¿”å›æ ¼å¼ï¼šä»…è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«å¯¹åº”é¡ºåºçš„å¸ƒå°”å€¼ï¼Œä¾‹å¦‚ï¼š[true, false, true]"
        )

        user_content = "è¯·åˆ¤æ–­ä»¥ä¸‹æ–°é—»å¯¹ï¼š\n"
        for i, p in enumerate(pairs):
            user_content += f"{i + 1}. [{p['leader']}] vs [{p['candidate']}]\n"

        async def try_verify(client, model):
            try:
                res = await self._call_llm(client, model, user_content, system_prompt)
                if not res:
                    return None

                clean_res = res.strip()
                if clean_res.startswith("```"):
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                else:
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]

                # å°è¯•ä¿®å¤ Python é£æ ¼çš„å¸ƒå°”å€¼
                clean_res = clean_res.replace("True", "true").replace("False", "false")

                try:
                    results = json.loads(clean_res)
                except json.JSONDecodeError:
                    # å°è¯•ä½¿ç”¨æ­£åˆ™æå–å¸ƒå°”å€¼
                    import re
                    bool_matches = re.findall(r'\b(true|false)\b', clean_res, re.IGNORECASE)
                    if len(bool_matches) == len(pairs):
                        results = [b.lower() == 'true' for b in bool_matches]
                    else:
                        raise

                if isinstance(results, list) and len(results) == len(pairs):
                    return [bool(x) for x in results]

                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒè¿”å›æ ¼å¼é”™è¯¯: {results} (é¢„æœŸé•¿åº¦: {len(pairs)})")
                return None
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒå¼‚å¸¸ ({model}): {e}")
                return None

        routes = self._iter_llm_routes(self._get_prefer_backup("CLUSTERING"))
        for route in routes:
            # å¦‚æœæ˜¯ backup é€šé“ï¼Œå°è¯• 3 æ¬¡ï¼›å¦‚æœæ˜¯ main é€šé“ï¼Œå°è¯• 1 æ¬¡
            is_backup = (route["base_url"] == settings.BACKUP_AI_BASE_URL)
            max_attempts = 3 if is_backup else 1
            
            for attempt in range(max_attempts):
                if attempt > 0:
                    await asyncio.sleep(2 if attempt == 1 else 10)
                
                client = AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"])
                try:
                    res = await try_verify(client, route["model"])
                except AIConfigurationError:
                    raise
                
                if res is not None:
                    return res
                if is_backup:
                    logger.warning(f"âš ï¸ å¤‡ç”¨AIæ ¸éªŒå¤±è´¥ (ç¬¬{attempt + 1}æ¬¡)ï¼Œå‡†å¤‡é‡è¯•...")

        logger.error("âŒ æ‰€æœ‰é€šé“æ ¸éªŒå‡å¤±è´¥ï¼Œè·³è¿‡æœ¬æ‰¹æ¬¡")
        return [False] * len(pairs)


    async def generate_topic_overview(self, topic_name: str, news_list: List[Dict[str, str]]) -> str:
        """
        è¾“å…¥:
        - `topic_name`: ä¸“é¢˜åç§°
        - `news_list`: æ–°é—»åˆ—è¡¨ [{"title": "...", "content": "..."}, ...]

        è¾“å‡º:
        - ä¸“é¢˜å¤šç»´åº¦ç»¼è¿°ï¼ˆçº¯æ–‡æœ¬ï¼‰
        
        ä½œç”¨:
        - ä»â€œäº‹ä»¶èƒŒæ™¯â€ã€â€œå‘å±•è¿‡ç¨‹â€ã€â€œå„æ–¹è§‚ç‚¹â€ã€â€œæœªæ¥å±•æœ›â€ç­‰ç»´åº¦å¯¹ä¸“é¢˜è¿›è¡Œæ·±åº¦æ€»ç»“
        """
        if not news_list:
            return ""
            
        logger.info(f"ğŸ¤– ç”Ÿæˆä¸“é¢˜ç»¼è¿°: {topic_name} ({len(news_list)} æ¡æ–°é—»)")
        
        # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…Tokenæº¢å‡º
        # ä¼˜å…ˆå–æœ€æ–°çš„æ–°é—»ï¼Œå’Œæœ€æ—©çš„æ–°é—»ï¼Œä»¥è¦†ç›–å…¨è²Œ
        # å‡è®¾ news_list å·²ç»åŒ…å«äº†ä¸€å®šæ•°é‡çš„ä»£è¡¨æ€§æ–°é—»
        input_text = ""
        for i, item in enumerate(news_list[:30]): # æœ€å¤šå–30æ¡ä½œä¸ºä¸Šä¸‹æ–‡
            t = (item.get("title") or "").replace("\n", " ")
            c = (item.get("summary") or item.get("content") or "")[:200].replace("\n", " ")
            input_text += f"[{i+1}] {t}\n   æ‘˜è¦: {c}\n\n"
            
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ–°é—»è¯„è®ºå‘˜ã€‚è¯·æ ¹æ®æä¾›çš„å¤šæ¡æ–°é—»æŠ¥é“ï¼Œå¯¹è¯¥ä¸“é¢˜äº‹ä»¶è¿›è¡Œå…¨æ–¹ä½çš„æ·±åº¦ç»¼è¿°ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. **ä¿¡æ¯è¯¦å®**ï¼šåœ¨ç»¼è¿°ä¸­å¿…é¡»ä¿ç•™å…³é”®çš„**äº‹å®ç»†èŠ‚**ï¼Œæ‹’ç»ç©ºæ´çš„å®å¤§å™äº‹ã€‚è¯·åŠ¡å¿…åŒ…å«ï¼š\n"
            "   - **å…·ä½“äººå**ï¼ˆå¦‚å¾æ¹–å¹³ï¼‰ã€**å…³é”®ç‰©å“åç§°**ï¼ˆå¦‚ã€Šæ±Ÿå—æ˜¥ã€‹ç”»å·ï¼‰ã€**å…·ä½“é‡‘é¢/ä¼°å€¼**ï¼ˆå¦‚8800ä¸‡ï¼‰ã€**æœºæ„åç§°**ç­‰ã€‚\n"
            "   - **å…·ä½“çš„æŒ‡æ§æˆ–äº‰è®®ç‚¹**ï¼ˆå¦‚â€œä¼ªé€ é‰´å®šâ€ã€â€œæ’•æ¯å°æ¡â€ã€â€œå€’å–æ–‡ç‰©â€ç­‰å…·ä½“è¡Œä¸ºï¼Œè€Œéä»…è¯´â€œè¿è§„â€ï¼‰ã€‚\n"
            "   - **å„æ–¹å…·ä½“å›åº”**å’Œ**æ ¸å¿ƒè¯æ®**ã€‚\n"
            "2. **ç»“æ„æ¸…æ™°**ï¼šè¯·åŒ…å«ä»¥ä¸‹å‡ ä¸ªç»´åº¦ï¼š\n"
            "   - **ã€äº‹ä»¶èƒŒæ™¯ã€‘**ï¼šè¯¦ç»†é˜è¿°äº‹ä»¶èµ·å› ï¼ŒåŒ…æ‹¬å†å²æ¸Šæºï¼ˆå¦‚æèµ èƒŒæ™¯ã€æ–‡ç‰©çš„æ¥æºï¼‰ã€‚\n"
            "   - **ã€å‘å±•è„‰ç»œã€‘**ï¼šæŒ‰é€»è¾‘æ¢³ç†äº‹ä»¶çš„å‡çº§è¿‡ç¨‹ï¼Œä¸è¦æµæ°´è´¦ï¼Œè¦ä½“ç°å› æœå…³ç³»ã€‚\n"
            "   - **ã€äº‰è®®ç„¦ç‚¹ã€‘**ï¼šæ ¸å¿ƒçŸ›ç›¾æ˜¯ä»€ä¹ˆï¼ˆå¦‚çœŸä¼ªé‰´å®šæƒã€ç®¡ç†æµç¨‹æ¼æ´ã€åˆ©ç›Šè¾“é€é“¾æ¡ï¼‰ã€‚\n"
            "   - **ã€æ ¸å¿ƒå½±å“ã€‘**ï¼šå…·ä½“çš„ç¤¾ä¼š/è¡Œä¸šå½±å“ï¼ˆå¦‚å¯¹å…¬ç›Šæèµ ä¿¡ä»»çš„æ‰“å‡»ã€å¯¹æ–‡åšç³»ç»Ÿåˆ¶åº¦çš„åæ€ï¼‰ã€‚\n"
            "   - **ã€æœ€æ–°è¿›å±•ä¸å±•æœ›ã€‘**ï¼šå½“å‰çš„è°ƒæŸ¥çŠ¶æ€åŠå¯èƒ½çš„èµ°å‘ã€‚\n"
            "3. **æ·±åº¦åˆ†æ**ï¼šä¸è¦åªåšè¡¨é¢æ‹¼å‡‘ï¼Œè¦åˆ†æäº‹ä»¶èƒŒåçš„é€»è¾‘å…³è”ã€‚\n"
            "4. **çº¯æ–‡æœ¬æ ¼å¼**ï¼šä¸è¦ä½¿ç”¨ Markdown æ ‡é¢˜ç¬¦å·ï¼ˆå¦‚ #, ##, **ï¼‰ï¼Œä½¿ç”¨ä¸­æ–‡æ–¹æ‹¬å·ã€ã€‘ä½œä¸ºå°æ ‡é¢˜å³å¯ã€‚åˆ†æ®µæ¢è¡Œè¦æ¸…æ™°ã€‚\n"
            "5. å­—æ•°æ§åˆ¶åœ¨ 600-1200 å­—ä¹‹é—´ã€‚\n"
            "6. **ç›´æ¥è¾“å‡º**ï¼šç›´æ¥å¼€å§‹æ­£æ–‡ï¼Œä¸è¦æœ‰â€œå¥½çš„â€ã€â€œç»¼è¿°å¦‚ä¸‹â€ç­‰å¼€åœºç™½ï¼Œä¹Ÿä¸è¦åŒ…å«â€œæ ¹æ®æ‚¨æä¾›çš„...â€ç­‰å¥—è¯ã€‚"
        )
        
        user_prompt = f"ä¸“é¢˜åç§°ï¼š{topic_name}\n\nç›¸å…³æ–°é—»æŠ¥é“ï¼š\n{input_text}"
        
        prefer_backup = self._get_prefer_backup("TOPIC_OVERVIEW")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res.strip()
        return "æš‚æ— æ³•ç”Ÿæˆç»¼è¿°ã€‚"







ai_service = AIService()

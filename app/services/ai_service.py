"""
æœ¬æ–‡ä»¶ç”¨äºå°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼ŒåŒ…æ‹¬å¯¹è¯ã€æ‘˜è¦ã€åˆ†æä¸å‘é‡åŒ–ç­‰èƒ½åŠ›ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `AIService`: AI èƒ½åŠ›å°è£…ï¼ˆä¸»/å¤‡æ¨¡å‹åˆ‡æ¢ã€å¹¶å‘æ§åˆ¶ã€å¤±è´¥é™çº§ï¼‰
- `ai_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import asyncio
import json
import re
import logging
from typing import Any, AsyncIterator, Dict, List, Optional, Tuple

import aiohttp
from openai import AsyncOpenAI, APIStatusError, RateLimitError, APIConnectionError

from app.core.config import get_settings
from app.core.logger import setup_logger
from app.core.exceptions import AIConfigurationError
from app.core.prompts import prompt_manager
from app.utils.tools import normalize_regions_to_countries

settings = get_settings()
logger = setup_logger("AIService")


class AIService:
    """
    è¾“å…¥:
    - AI é…ç½®ï¼ˆä¸»/å¤‡æ¨¡å‹ã€å¹¶å‘é™åˆ¶ã€Embedding é…ç½®ï¼‰

    è¾“å‡º:
    - å¤§æ¨¡å‹å¯¹è¯/æ‘˜è¦/åˆ†æç»“æœï¼Œä»¥åŠå‘é‡åŒ–ç»“æœ

    ä½œç”¨:
    - å°è£…ä¸å¤–éƒ¨ AI æœåŠ¡çš„äº¤äº’ï¼Œæä¾›ç»Ÿä¸€ã€å¯é™çº§çš„è°ƒç”¨å…¥å£
    """

    def __init__(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå§‹åŒ–ä¸»/å¤‡é€šé“å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        """

        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)

    def reload_config(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - é‡æ–°åŠ è½½å…¨å±€é…ç½®ï¼ˆç”¨äºé…ç½®æ›´æ–°ååˆ·æ–°æœ¬åœ°å¼•ç”¨ï¼‰
        """
        global settings
        from app.core.config import get_settings
        settings = get_settings()
        
        # é‡æ–°åˆå§‹åŒ–ä¿¡å·é‡ï¼ˆå¹¶å‘é…ç½®å¯èƒ½æ”¹å˜ï¼‰
        self.main_sem = asyncio.Semaphore(settings.MAIN_AI_CONCURRENCY)
        self.backup_sem = asyncio.Semaphore(settings.BACKUP_AI_CONCURRENCY)
        logger.info("ğŸ”„ AIService é…ç½®å·²åˆ·æ–°")

    def _has_main_llm(self) -> bool:
        return bool((settings.MAIN_AI_API_KEY or "").strip()) and bool((settings.MAIN_AI_BASE_URL or "").strip()) and bool((settings.MAIN_AI_MODEL or "").strip())

    def _has_backup_llm(self) -> bool:
        return bool((settings.BACKUP_AI_API_KEY or "").strip()) and bool((settings.BACKUP_AI_BASE_URL or "").strip()) and bool((settings.BACKUP_AI_MODEL or "").strip())

    def _has_embedding(self) -> bool:
        return bool((settings.SILICONFLOW_API_KEY or "").strip()) and bool((settings.SILICONFLOW_BASE_URL or "").strip()) and bool((settings.EMBEDDING_MODEL or "").strip())

    def _iter_llm_routes(self, prefer_backup: bool) -> List[Dict[str, str]]:
        routes: List[Dict[str, str]] = []
        if prefer_backup:
            if self._has_backup_llm():
                routes.append(
                    {
                        "base_url": str(settings.BACKUP_AI_BASE_URL),
                        "api_key": str(settings.BACKUP_AI_API_KEY),
                        "model": str(settings.BACKUP_AI_MODEL),
                        "type": "backup",
                    }
                )
            if self._has_main_llm():
                routes.append(
                    {
                        "base_url": str(settings.MAIN_AI_BASE_URL),
                        "api_key": str(settings.MAIN_AI_API_KEY),
                        "model": str(settings.MAIN_AI_MODEL),
                        "type": "main",
                    }
                )
            return routes

        if self._has_main_llm():
            routes.append(
                {
                    "base_url": str(settings.MAIN_AI_BASE_URL),
                    "api_key": str(settings.MAIN_AI_API_KEY),
                    "model": str(settings.MAIN_AI_MODEL),
                    "type": "main",
                }
            )
        if self._has_backup_llm():
            routes.append(
                {
                    "base_url": str(settings.BACKUP_AI_BASE_URL),
                    "api_key": str(settings.BACKUP_AI_API_KEY),
                    "model": str(settings.BACKUP_AI_MODEL),
                    "type": "backup",
                }
            )
        return routes

    async def _call_llm(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: str,
        system: str = "",
        semaphore: asyncio.Semaphore | None = None,
    ) -> Optional[str]:
        """
        è¾“å…¥:
        - `client`: OpenAI å…¼å®¹å®¢æˆ·ç«¯
        - `model`: æ¨¡å‹åç§°
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system`: ç³»ç»Ÿæç¤ºè¯
        - `semaphore`: å¹¶å‘æ§åˆ¶ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹è¿”å›æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - ç»Ÿä¸€å°è£… LLM è°ƒç”¨ã€å¹¶å‘æ§åˆ¶ä¸å¼‚å¸¸å¤„ç†
        """

        try:
            extra_body = {}
            if "modelscope" in str(client.base_url):
                extra_body["enable_thinking"] = False

            if semaphore is None:
                if str(settings.MAIN_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.main_sem
                elif str(settings.BACKUP_AI_BASE_URL) in str(client.base_url):
                    semaphore = self.backup_sem

            # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æç¤ºè¯
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ”µ [LLM è¯·æ±‚] æ¨¡å‹: {model}\nç³»ç»Ÿæç¤ºè¯: {system}\nç”¨æˆ·æç¤ºè¯: {prompt[:2000]}...")

            async def do_call():
                return await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=0.6,
                    timeout=120,
                    extra_body=extra_body if extra_body else None,
                )

            # é‡è¯•é€»è¾‘
            max_retries = 4
            for attempt in range(max_retries):
                try:
                    if semaphore:
                        async with semaphore:
                            response = await do_call()
                    else:
                        response = await do_call()
                    
                    content = response.choices[0].message.content
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"ğŸŸ¢ [LLM å“åº”] æ¨¡å‹: {model}\nå†…å®¹: {content[:2000]}...")
                    
                    if not content:
                         logger.warning(f"âš ï¸ AI è¿”å›å†…å®¹ä¸ºç©º ({model})")
                         if attempt < max_retries - 1:
                             logger.info(f"   ğŸ”„ ç©ºå†…å®¹é‡è¯• ({attempt + 1}/{max_retries})...")
                             await asyncio.sleep(1)
                             continue
                         return None

                    return content
                
                except (RateLimitError, APIConnectionError) as e:
                    if attempt == max_retries - 1:
                        raise e
                    wait_time = 2 * (attempt + 1)
                    logger.warning(f"âš ï¸ AI è°ƒç”¨å—é™æˆ–ç½‘ç»œæ³¢åŠ¨ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯• ({attempt + 1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                
                except APIStatusError as e:
                    # 401: API Key æ— æ•ˆ - è‡´å‘½é”™è¯¯
                    if e.status_code == 401:
                        logger.error(f"âŒ AI è®¤è¯å¤±è´¥ (401) - API Key æ— æ•ˆ ({model}): {e}")
                        raise AIConfigurationError(f"AI API Key æ— æ•ˆ ({model})")

                    # 400 Bad Request é€šå¸¸æ„å‘³ç€å†…å®¹è¿‡æ»¤æˆ–å‚æ•°æ— æ•ˆ
                    if e.status_code == 400:
                        logger.warning(f"âŒ AI è¯·æ±‚è¢«æ‹’ç» (400) - å¯èƒ½è§¦å‘æ•æ„Ÿè¯è¿‡æ»¤ ({model}): {e}")
                        # è§¦å‘å¤–éƒ¨çš„åˆ‡æ¢é€»è¾‘ï¼Œå¦‚æœæ˜¯è·¯ç”±æ¨¡å¼ï¼Œä¼šæ•è· None ç„¶ååˆ‡æ¢
                        return None 
                    
                    # æœåŠ¡ç«¯é”™è¯¯ï¼Œé‡è¯•å¯èƒ½æœ‰æ•ˆ
                    if e.status_code >= 500:
                        if attempt == max_retries - 1:
                            raise e
                        wait_time = 2 * (attempt + 1)
                        logger.warning(f"âš ï¸ AI æœåŠ¡ç«¯é”™è¯¯ ({model}): {e}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯•")
                        await asyncio.sleep(wait_time)
                    else:
                        raise e

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ AI è°ƒç”¨å¼‚å¸¸ ({model}): {e}")
            return None

    async def chat_completion(self, prompt: str, system_prompt: str = "", route_key: str = None) -> str:
        """
        è¾“å…¥:
        - `prompt`: ç”¨æˆ·æç¤ºè¯
        - `system_prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        - `route_key`: é…ç½®è·¯ç”±é”®ï¼ˆå¯é€‰ï¼Œå¦‚ "REPORT", "SUMMARY" ç­‰ï¼‰

        è¾“å‡º:
        - æ¨¡å‹å›å¤æ–‡æœ¬ï¼ˆä¿è¯è¿”å›å­—ç¬¦ä¸²ï¼‰

        ä½œç”¨:
        - æ‰§è¡Œä¸€æ¬¡å¯¹è¯è¡¥å…¨ï¼›æ ¹æ® route_key é€‰æ‹©ä¸»/å¤‡é€šé“ç­–ç•¥
        """
        prefer_backup = False
        if route_key:
            prefer_backup = self._get_prefer_backup(route_key)
        
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res

        return "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆè¯·å…ˆåœ¨ç®¡ç†é¡µå®Œå–„ AI é…ç½®ï¼‰"

    async def stream_completion(self, prompt: str, system_prompt: str = "", route_key: Optional[str] = None) -> AsyncIterator[str]:
        prefer_backup = False
        if route_key:
            prefer_backup = self._get_prefer_backup(route_key)

        routes = self._iter_llm_routes(prefer_backup)
        if not routes:
            yield "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆè¯·å…ˆåœ¨ç®¡ç†é¡µå®Œå–„ AI é…ç½®ï¼‰"
            return
            # raise AIConfigurationError("AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆè¯·å…ˆåœ¨ç®¡ç†é¡µå®Œå–„ AI é…ç½®ï¼‰")
        last_error: Optional[Exception] = None

        for idx, route in enumerate(routes):
            try:
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    extra_body = {}
                    if "modelscope" in str(client.base_url):
                        extra_body["enable_thinking"] = False

                    stream = await client.chat.completions.create(
                        model=route["model"],
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt},
                        ],
                        stream=True,
                        temperature=0.6,
                        timeout=120,
                        extra_body=extra_body if extra_body else None,
                    )

                    async for chunk in stream:
                        if not chunk.choices:
                            continue
                        content = chunk.choices[0].delta.content
                        if content:
                            yield content
                    return
            except Exception as e:
                last_error = e
                logger.error(f"æµå¼è¡¥å…¨è·¯ç”±å¤±è´¥: {e}")
                if idx < len(routes) - 1:
                    next_route = routes[idx + 1]
                    logger.warning(
                        f"âš ï¸ æµå¼è·¯ç”± {route['model']} ({route['type']}) å¤±è´¥ï¼Œåˆ‡æ¢åˆ° -> {next_route['model']} ({next_route['type']})"
                    )

        if last_error:
            logger.error(f"æ‰€æœ‰æµå¼è·¯ç”±å‡å¤±è´¥: {last_error}")

    async def batch_evaluate_topic_quality(self, topics: List[Dict[str, str]], existing_topics: List[Dict[str, str]] = None) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - topics: ä¸“é¢˜åˆ—è¡¨ [{"name":..., "description":...}]
        - existing_topics: ç°æœ‰ä¸“é¢˜åˆ—è¡¨ [{"name":..., "description":...}] ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - ç»è¿‡ç­›é€‰çš„æœ‰æ•ˆä¸“é¢˜åˆ—è¡¨

        ä½œç”¨:
        - æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡ï¼Œè¿‡æ»¤æ‰è¿‡äºå®½æ³›ã€éå…·ä½“äº‹ä»¶ã€æˆ–çº¯ç²¹è¡Œä¸šè¶‹åŠ¿çš„ä¸“é¢˜
        - æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ä¸“é¢˜é‡å¤æˆ–å±äºç°æœ‰ä¸“é¢˜çš„å»¶ä¼¸
        """
        if not topics:
            return []

        logger.info(f"ğŸ¤– æ‰¹é‡è¯„ä¼°ä¸“é¢˜è´¨é‡: {len(topics)} ä¸ª")
        
        existing_info = ""
        if existing_topics:
            # é™åˆ¶ç°æœ‰ä¸“é¢˜æ•°é‡ä»¥é˜²æ­¢ Prompt è¿‡é•¿ï¼Œå–æœ€è¿‘ 50 ä¸ªå³å¯ï¼ˆå‡è®¾æŒ‰æ—¶é—´å€’åºæˆ–ç›¸å…³æ€§æ’åºï¼‰
            # è¿™é‡Œè°ƒç”¨è€…ä¼ å…¥çš„é€šå¸¸æ˜¯ active ä¸“é¢˜ï¼Œæ•°é‡å¯èƒ½è¾ƒå¤šï¼Œæˆªå–ä¸€ä¸‹æ¯”è¾ƒå®‰å…¨
            limit_existing = existing_topics[:50]
            existing_info = "ã€å·²å­˜åœ¨çš„ä¸“é¢˜åˆ—è¡¨ï¼ˆç”¨äºæŸ¥é‡å’Œåˆ¤æ–­å»¶ä¼¸å…³ç³»ï¼‰ã€‘:\n"
            for t in limit_existing:
                existing_info += f"- {t.get('name')}: {t.get('description')}\n"
            existing_info += "\n"

        # è·å–è´¨é‡ç­‰çº§ï¼Œé»˜è®¤ä¸º 3
        quality_level = getattr(settings, "TOPIC_QUALITY_LEVEL", 3)
        logger.info(f"ğŸ” ä¸“é¢˜å®¡æ ¸è´¨é‡ç­‰çº§: {quality_level}")

        # é»˜è®¤ä½¿ç”¨ç­‰çº§ 3
        criteria_text = prompt_manager.get_user_prompt(f"topic_quality_criteria_{quality_level}")
        if not criteria_text:
             criteria_text = prompt_manager.get_user_prompt("topic_quality_criteria_3")

        # æ„å»ºç¦æ­¢é¡¹
        forbidden_common = prompt_manager.get_user_prompt("topic_quality_forbidden_common")
        forbidden_strict = prompt_manager.get_user_prompt("topic_quality_forbidden_strict")
        forbidden_loose = prompt_manager.get_user_prompt("topic_quality_forbidden_loose")

        if quality_level >= 3:
            forbidden_text = forbidden_strict + "\n" + forbidden_common
        else:
            forbidden_text = forbidden_loose + "\n" + forbidden_common

        system_prompt = prompt_manager.get_system_prompt(
            "topic_quality_eval_base",
            quality_level=quality_level,
            criteria_text=criteria_text,
            forbidden_text=forbidden_text
        )
        
        topics_text = ""
        for i, t in enumerate(topics):
            topics_text += f"[{i}] åç§°ï¼š{t.get('name')}\n    æè¿°ï¼š{t.get('description')}\n\n"

        user_prompt = prompt_manager.get_user_prompt(
            "topic_quality_eval_base",
            existing_info=existing_info,
            topics_text=topics_text
        )
            
        prefer_backup = self._get_prefer_backup("TOPIC_EVAL")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            logger.warning("âš ï¸ ä¸“é¢˜è´¨é‡è¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤å…¨éƒ¨ä¿ç•™")
            return topics
            
        try:
            clean = res.replace("```json", "").replace("```", "").strip()
            # å°è¯•æå– JSON æ•°ç»„
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1:
                clean = clean[start : end + 1]
                
            results = json.loads(clean)
            
            valid_topics = []
            if isinstance(results, list):
                for item in results:
                    idx = item.get("index")
                    is_valid = item.get("valid")
                    reason = item.get("reason", "æ— ç†ç”±")
                    
                    if isinstance(idx, int) and 0 <= idx < len(topics):
                        topic_name = topics[idx].get("name")
                        if is_valid:
                            logger.info(f"   âœ… [é€šè¿‡] {topic_name}: {reason}")
                            valid_topics.append(topics[idx])
                        else:
                            logger.info(f"   âŒ [æ‹’ç»] {topic_name}: {reason}")
            else:
                # å¦‚æœç»“æ„é”™è¯¯åˆ™é™çº§å¤„ç†
                logger.warning("   âš ï¸ è´¨é‡è¯„ä¼°è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè§£æå¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰")
                return topics

            if len(valid_topics) < len(topics):
                removed_count = len(topics) - len(valid_topics)
                logger.info(f"ğŸ—‘ï¸ è¿‡æ»¤æ‰äº† {removed_count} ä¸ªå®½æ³›/ä½è´¨é‡ä¸“é¢˜")
                
            return valid_topics
            
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æè´¨é‡è¯„ä¼°ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return topics

    async def propose_topics_from_titles(self, titles: List[str]) -> List[Dict[str, str]]:
        """
        è¾“å…¥:
        - `titles`: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        
        è¾“å‡º:
        - æç‚¼å‡ºçš„ä¸“é¢˜åˆ—è¡¨ [{"name": "...", "description": "..."}, ...]
        
        ä½œç”¨:
        - ä»å¤§é‡æ ‡é¢˜ä¸­èšåˆå‡ºæ ¸å¿ƒä¸“é¢˜
        """
        if not titles:
            return []
            
        system_prompt = prompt_manager.get_system_prompt("topic_propose")
        
        # é™åˆ¶æ•°é‡ä»¥é˜²è¶…é•¿
        limit_n = settings.TOPIC_AGGREGATION_TOP_N
        titles_subset = titles[:limit_n]
        titles_str = "\n".join([f"- {t}" for t in titles_subset])
        
        # åŠ¨æ€è·å–ä¸“é¢˜æ•°é‡èŒƒå›´
        count_range = settings.TOPIC_GENERATION_COUNT or "1-5"
        min_count, max_count = 1, 5
        try:
            parts = count_range.split("-")
            if len(parts) == 2:
                min_count = int(parts[0].strip())
                max_count = int(parts[1].strip())
        except Exception:
            pass

        # è·å–è´¨é‡ç­‰çº§ï¼Œé»˜è®¤ä¸º 3
        quality_level = getattr(settings, "TOPIC_QUALITY_LEVEL", 3)
        logger.info(f"ğŸ” ä¸“é¢˜ç”Ÿæˆè´¨é‡ç­‰çº§: {quality_level}")

        criteria_desc = prompt_manager.get_user_prompt(f"topic_propose_criteria_{quality_level}")
        if not criteria_desc:
            criteria_desc = prompt_manager.get_user_prompt("topic_propose_criteria_3")

        prompt = prompt_manager.get_user_prompt(
            "topic_propose_user",
            min_count=min_count,
            max_count=max_count,
            criteria_desc=criteria_desc,
            titles_str=titles_str
        )
        logger.info(f"ğŸ¤– æ­£åœ¨ä» {len(titles_subset)} æ¡æ ‡é¢˜ä¸­æç‚¼ä¸“é¢˜...")
        prefer_backup = self._get_prefer_backup("TOPIC_NAME")
        res = await self._call_llm_with_routes(prompt, system_prompt, prefer_backup=prefer_backup)
        
        try:
            if not res:
                return []
            cleaned = res.replace("```json", "").replace("```", "").strip()
            
            # å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯ï¼ˆå¦‚å°¾éƒ¨é€—å·ã€æœªè½¬ä¹‰å¼•å·ç­‰ï¼‰
            # è¿™é‡Œå…ˆç®€å•å°è¯•ç›´æ¥è§£æ
            try:
                data = json.loads(cleaned)
            except json.JSONDecodeError:
                # å‡å¦‚è§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æå–
                import re
                pattern = r'\{\s*"name"\s*:\s*"(.*?)"\s*,\s*"description"\s*:\s*"(.*?)"\s*\}'
                matches = re.findall(pattern, cleaned, re.DOTALL)
                if matches:
                    data = [{"name": m[0], "description": m[1]} for m in matches]
                else:
                    # å†æ¬¡å°è¯•ï¼Œå¯èƒ½æ˜¯å•å¼•å·æˆ–å…¶ä»–æ ¼å¼
                    raise 

            if isinstance(data, list):
                valid_data = []
                for item in data:
                    if isinstance(item, dict) and "name" in item and "description" in item:
                        valid_data.append(item)
                
                # å†æ¬¡å¼ºåˆ¶æˆªæ–­ï¼Œé˜²æ­¢ AI è¿”å›è¿‡å¤š
                if len(valid_data) > max_count:
                    logger.warning(f"âš ï¸ AI è¿”å›ä¸“é¢˜æ•°é‡ ({len(valid_data)}) è¶…è¿‡é™åˆ¶ ({max_count})ï¼Œå·²å¼ºåˆ¶æˆªæ–­")
                    valid_data = valid_data[:max_count]
                
                logger.info(f"âœ… AI æç‚¼å‡º {len(valid_data)} ä¸ªæ½œåœ¨ä¸“é¢˜")
                return valid_data
            return []
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£æä¸“é¢˜æç‚¼ç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def extract_news_info(self, content: str) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `content`: åŸå§‹é¡µé¢å†…å®¹ï¼ˆHTML/XML/æ–‡æœ¬ï¼‰

        è¾“å‡º:
        - æ–°é—»æ¡ç›®åˆ—è¡¨ï¼ˆtitle/link/summaryï¼‰

        ä½œç”¨:
        - å½“å¸¸è§„ RSS/API è§£æå¤±è´¥æ—¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹ä»å†…å®¹ä¸­æŠ½å–æ–°é—»æ¡ç›®
        """

        system_prompt = prompt_manager.get_system_prompt("news_extract_info")
        user_prompt = prompt_manager.get_user_prompt("news_extract_info", content=content[:20000])

        async def try_extract(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                return data.get("items", [])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.warning(f"AIæå–ç»“æœè§£æå¤±è´¥: {e}")
                return None

        if not (self._has_main_llm() or self._has_backup_llm()):
            return []

        routes = self._iter_llm_routes(self._get_prefer_backup("SUMMARY"))
        for r_idx, route in enumerate(routes):
            max_attempts = 3 if r_idx == 0 else 1
            for attempt in range(max_attempts):
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    res = await try_extract(client, route["model"])
                if res is not None:
                    return res
                await asyncio.sleep(1)

        return []

    def _normalize_category(self, raw_category: str) -> str:
        """
        è¾“å…¥:
        - `raw_category`: æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†ç±»

        è¾“å‡º:
        - è§„èŒƒåŒ–åçš„åˆ†ç±»åç§°ï¼ˆè½åˆ° `settings.NEWS_CATEGORIES` ä¹‹ä¸€ï¼‰

        ä½œç”¨:
        - å°†æ¨¡å‹å¯èƒ½å‡ºç°çš„è¿‘ä¼¼åˆ†ç±»æ˜ å°„ä¸ºç³»ç»Ÿå†…ç½®åˆ†ç±»ï¼Œå‡å°‘è„æ•°æ®
        """

        if not raw_category:
            return "å…¶ä»–"
        if raw_category in settings.NEWS_CATEGORIES:
            return raw_category
        for cat in settings.NEWS_CATEGORIES:
            if raw_category in cat or cat in raw_category:
                return cat
        return "å…¶ä»–"

    async def analyze_sentiment(self, title: str, content: str = "") -> Dict:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ‘˜è¦æˆ–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æƒ…æ„Ÿåˆ†æç»“æœï¼ˆscore/label/category/region/keywords/entitiesï¼‰

        ä½œç”¨:
        - å¯¹å•æ¡æ–°é—»è¿›è¡Œæ·±åº¦èˆ†æƒ…åˆ†æï¼Œä¸»é€šé“å¤±è´¥æ—¶é™çº§åˆ°å¤‡ç”¨é€šé“
        """

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = prompt_manager.get_system_prompt("sentiment_analysis_single", categories_str=categories_str)
        user_prompt = prompt_manager.get_user_prompt("sentiment_analysis_single", title=title, content=content[:1000])

        async def try_analyze(client, model):
            res = await self._call_llm(client, model, user_prompt, system_prompt)
            if not res:
                return None
            try:
                clean_res = res.strip()
                if "```" in clean_res:
                    start = clean_res.find("{")
                    end = clean_res.rfind("}")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                data = json.loads(clean_res)
                if "score" in data and "label" in data:
                    data["category"] = self._normalize_category(data.get("category", ""))
                    data["region"] = normalize_regions_to_countries(data.get("region"))
                    if not data.get("region") or data.get("region") in ["å…¶ä»–", "æœªçŸ¥"]:
                        data["region"] = "å…¨çƒ"
                    
                    # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µï¼Œé˜²æ­¢ KeyError
                    if "keywords" not in data:
                        data["keywords"] = []
                    if "entities" not in data:
                        data["entities"] = []
                        
                    return data
            except AIConfigurationError:
                raise
            except Exception:
                pass
            return None

        routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
        for route in routes:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await try_analyze(client, route["model"])
            if res:
                return res

        return {
            "score": 50,
            "label": "ä¸­ç«‹",
            "category": "å…¶ä»–",
            "region": "å…¶ä»–",
            "keywords": [],
            "entities": [],
        }

    async def batch_analyze_sentiment(self, news_items: List[Dict]) -> Dict[int, Dict]:
        """
        è¾“å…¥:
        - `news_items`: å¾…åˆ†ææ–°é—»åˆ—è¡¨ï¼ˆè‡³å°‘åŒ…å« id/titleï¼‰

        è¾“å‡º:
        - `id -> åˆ†æç»“æœ` çš„æ˜ å°„

        ä½œç”¨:
        - å¯¹å¤šæ¡æ–°é—»è¿›è¡Œæ‰¹é‡å¿«é€Ÿåˆ†æï¼Œç”¨äºæå‡ååä¸é™ä½æˆæœ¬
        """

        if not news_items:
            return {}

        categories_str = "ã€".join(settings.NEWS_CATEGORIES)
        system_prompt = prompt_manager.get_system_prompt("sentiment_analysis_batch", categories_str=categories_str)

        items_text = ""
        for item in news_items:
            items_text += f"[ID:{item['id']}] {item['title']}\n"
        user_prompt = prompt_manager.get_user_prompt("sentiment_analysis_batch", items_text=items_text)

        try:
            routes = self._iter_llm_routes(self._get_prefer_backup("SENTIMENT"))
            res: Optional[str] = None
            for route in routes:
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    res = await self._call_llm(client, route["model"], user_prompt, system_prompt)
                if res:
                    break

            if not res:
                return {}

            clean_res = res.strip()
            # æ— è®ºæ˜¯å¦åŒ…å« markdown æ ‡è®°ï¼Œéƒ½ä¼˜å…ˆå°è¯•æå– JSON æ•°ç»„
            start = clean_res.find("[")
            end = clean_res.rfind("]")
            if start != -1 and end != -1:
                clean_res = clean_res[start : end + 1]
            else:
                # å…œåº•æ¸…ç†
                clean_res = clean_res.replace("```json", "").replace("```", "").strip()

            results_list = json.loads(clean_res)

            result_map: Dict[int, Dict] = {}
            if isinstance(results_list, list):
                for item in results_list:
                    if "id" in item:
                        if "category" in item:
                            item["category"] = self._normalize_category(item["category"])
                        item["region"] = normalize_regions_to_countries(item.get("region"))
                        result_map[item["id"]] = item
            return result_map

        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {}

    def _get_prefer_backup(self, route_key: str) -> bool:
        """æ ¹æ®é…ç½®é”®è·å–æ˜¯å¦ä¼˜å…ˆä½¿ç”¨å¤‡ç”¨ AI"""
        route_value = settings.AI_ROUTE.get(route_key, "main").lower()
        return route_value == "backup"

    async def generate_summary(self, title: str, content: str, max_words: Optional[int] = None) -> Optional[str]:
        """
        è¾“å…¥:
        - `title`: æ–°é—»æ ‡é¢˜
        - `content`: æ–°é—»æ­£æ–‡
        - `max_words`: æœ€å¤§å­—æ•°é™åˆ¶ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®)

        è¾“å‡º:
        - æ‘˜è¦æ–‡æœ¬ (å¦‚æœå¤±è´¥è¿”å› None)

        ä½œç”¨:
        - ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ–°é—»æ‘˜è¦ï¼Œæ”¯æŒä¸»å¤‡åˆ‡æ¢
        """
        if not content:
            return None
            
        # ä½¿ç”¨é…ç½®çš„é»˜è®¤é•¿åº¦
        if max_words is None:
            max_words = getattr(settings, "SUMMARY_OUTPUT_LENGTH", 300)
            
        # æˆªå–è¾“å…¥å†…å®¹
        input_limit = getattr(settings, "SUMMARY_INPUT_MAX_LENGTH", 5000)
        if input_limit > 0 and len(content) > input_limit:
            content = content[:input_limit] + "\n...(å·²æˆªæ–­)"

        system_prompt = prompt_manager.get_system_prompt("summary_generation", max_words=max_words)
        user_prompt = prompt_manager.get_user_prompt("summary_generation", title=title, content=content)

        prefer_backup = self._get_prefer_backup("SUMMARY")
        routes = self._iter_llm_routes(prefer_backup)
        for route in routes:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await self._call_llm(
                    client,
                    route["model"],
                    user_prompt,
                    system_prompt,
                )
            if res:
                return res
        return None

    async def _call_llm_with_routes(
        self,
        user_prompt: str,
        system_prompt: str,
        prefer_backup: Optional[bool] = None,
    ) -> Optional[str]:
        prefer = False if prefer_backup is None else prefer_backup
        routes = self._iter_llm_routes(prefer)
        for i, route in enumerate(routes):
            # ä½¿ç”¨ async with ç¡®ä¿ client èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                res = await self._call_llm(
                    client,
                    route["model"],
                    user_prompt,
                    system_prompt,
                )
            
            if res:
                return res
            
            # å¦‚æœè¿è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜å½“å‰è·¯ç”±å¤±è´¥æˆ–è¿”å›ç©ºå†…å®¹
            if i < len(routes) - 1:
                next_route = routes[i+1]
                logger.warning(f"âš ï¸ è·¯ç”± {route['model']} ({route['type']}) è°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºï¼Œå°è¯•åˆ‡æ¢åˆ° -> {next_route['model']} ({next_route['type']})")
            else:
                logger.error(f"âŒ æ‰€æœ‰å¯ç”¨ AI è·¯ç”±å‡è°ƒç”¨å¤±è´¥")
        return None



    async def verify_topic_match_batch(self, tasks: List[Dict[str, str]]) -> List[Tuple[bool, str]]:
        """
        è¾“å…¥:
        - tasks: ä»»åŠ¡åˆ—è¡¨ï¼ŒåŒ…å« {"topic_name": ..., "topic_summary": ..., "news_title": ..., "news_summary": ...}

        è¾“å‡º:
        - ç»“æœåˆ—è¡¨ï¼ŒåŒ…å« (æ˜¯å¦åŒ¹é…, ç†ç”±)
        """
        if not tasks:
            return []

        # è®°å½•è¯·æ±‚å†…å®¹
        logger.info(f"ğŸ¤– æ‰¹é‡æ ¸éªŒä¸“é¢˜åŒ¹é…: {len(tasks)} ç»„")
        for i, t in enumerate(tasks[:3]):  # è®°å½•å‰ 3 æ¡ç”¨äºé¢„è§ˆ
            logger.info(f"   [{i}] ä¸“é¢˜: {t['topic_name']} <-> æ–°é—»: {t['news_title']}")

        system_prompt = prompt_manager.get_system_prompt("topic_match_batch")

        tasks_text = ""
        for idx, task in enumerate(tasks):
            tasks_text += (
                f"--- ç¬¬ {idx+1} ç»„ ---\n"
                f"ã€ä¸“é¢˜ã€‘{task['topic_name']}\n"
                f"ã€ä¸“é¢˜æ¦‚è§ˆã€‘{(task['topic_summary'] or '')[:300]}\n"
                f"ã€æ–°é—»æ ‡é¢˜ã€‘{task['news_title']}\n"
                f"ã€æ–°é—»æ‘˜è¦ã€‘{(task['news_summary'] or '')[:300]}\n"
            )
            
        user_prompt = prompt_manager.get_user_prompt("topic_match_batch", tasks_text=tasks_text)

        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return [(False, "AIè°ƒç”¨å¤±è´¥")] * len(tasks)

        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            results = json.loads(clean)
            
            output = []
            if isinstance(results, list):
                for item in results:
                    is_match = bool(item.get("match", False))
                    reason = item.get("reason", "æ— ç†ç”±")
                    output.append((is_match, reason))
                
                # ç¡®ä¿é•¿åº¦åŒ¹é…
                if len(output) < len(tasks):
                    output.extend([(False, "è¿”å›æ•°é‡ä¸è¶³")] * (len(tasks) - len(output)))
                
                return output[:len(tasks)]
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¸“é¢˜æ ¸éªŒè§£æå¤±è´¥: {e}")

        return [(False, "è§£æå¼‚å¸¸")] * len(tasks)




    async def generate_daily_timeline_events(self, date_str: str, news_items: List[Dict[str, Any]], topic_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - date_str: "YYYY-MM-DD"
        - news_items: List of {"id": ..., "title": ..., "summary": ...}
        - topic_name: Optional topic name to focus on

        è¾“å‡º:
        - List of events: [{"content": "...", "source_ids": [id1, id2...]}, ...]
        """
        if not news_items:
            return []

        logger.info(f"ğŸ¤– æ­£åœ¨ä¸º {date_str} åˆæˆæ—¶é—´è½´äº‹ä»¶ (å…± {len(news_items)} æ¡æ–°é—»)...")
        
        focus_instruction = ""
        if topic_name:
            focus_instruction = prompt_manager.get_user_prompt("topic_timeline_focus", topic_name=topic_name)

        system_prompt = prompt_manager.get_system_prompt("topic_timeline_generation", focus_instruction=focus_instruction)

        topic_focus = ""
        if topic_name:
            topic_focus = f"ä¸“é¢˜åç§°ï¼š{topic_name}\n"
        
        news_list = ""
        for item in news_items:
            news_list += f"[ID: {item['id']}] {item['title']}\næ‘˜è¦: {(item['summary'] or '')[:100]}\n\n"
            
        user_prompt = prompt_manager.get_user_prompt("topic_timeline_generation", date_str=date_str, topic_focus=topic_focus, news_list=news_list)

        prefer_backup = self._get_prefer_backup("TOPIC_TIMELINE")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        
        if not res:
            return []
            
        try:
            clean = res.strip()
            if "```" in clean:
                start = clean.find("[")
                end = clean.rfind("]")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            events = json.loads(clean)
            return events
        except AIConfigurationError:
            raise
        except Exception as e:
            logger.error(f"âŒ è§£ææ—¶é—´è½´åˆæˆç»“æœå¤±è´¥: {e}\nRaw: {res}")
            return []

    async def check_topic_duplicate(
        self,
        new_name: str,
        new_desc: str,
        existing_name: str,
        existing_desc: str
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­ä¸¤ä¸ªä¸“é¢˜æ˜¯å¦å®è´¨ä¸Šæ˜¯åŒä¸€ä¸ªäº‹ä»¶ã€‚
        """
        system_prompt = prompt_manager.get_system_prompt("topic_duplicate_check")
        user_prompt = prompt_manager.get_user_prompt(
            "topic_duplicate_check",
            new_name=new_name,
            new_desc=new_desc[:500],
            existing_name=existing_name,
            existing_desc=existing_desc[:500]
        )
        
        prefer_backup = self._get_prefer_backup("TOPIC_MATCH")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if not res:
            return False, "AIè°ƒç”¨å¤±è´¥"
            
        clean = res.strip()
        try:
            if "```" in clean:
                start = clean.find("{")
                end = clean.rfind("}")
                if start != -1 and end != -1:
                    clean = clean[start : end + 1]
            data = json.loads(clean)
            return bool(data.get("duplicate", False)), data.get("reason", "æ— ç†ç”±")
        except Exception:
            lowered = clean.lower()
            if "true" in lowered and "false" not in lowered:
                return True, "è§£æå¤±è´¥(fallback: true)"
            return False, "è§£æå¤±è´¥"

    async def stream_chat(self, query: str, context: str, model_type: str = "main") -> AsyncIterator[str]:
        """
        è¾“å…¥:
        - `query`: ç”¨æˆ·é—®é¢˜
        - `context`: ç›¸å…³æ–°é—»ä¸Šä¸‹æ–‡
        - `model_type`: ä½¿ç”¨ä¸»/å¤‡é€šé“ï¼ˆmain/backupï¼‰

        è¾“å‡º:
        - SSE å¯è¿­ä»£çš„å¢é‡æ–‡æœ¬ç‰‡æ®µ

        ä½œç”¨:
        - ä»¥æµå¼æ–¹å¼ä¸æ¨¡å‹å¯¹è¯ï¼Œé€‚é…å‰ç«¯å®æ—¶å±•ç¤º
        """
        # å¦‚æœè¯·æ±‚ä½¿ç”¨ main é€šé“ï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥é…ç½®è·¯ç”±æ˜¯å¦æŒ‡å®šäº†ä¼˜å…ˆå¤‡ç”¨
        if model_type == "main":
            if self._get_prefer_backup("CHAT"):
                model_type = "backup"

        api_key = settings.MAIN_AI_API_KEY
        base_url = settings.MAIN_AI_BASE_URL
        model = settings.MAIN_AI_MODEL

        if model_type == "backup":
            api_key = settings.BACKUP_AI_API_KEY
            base_url = settings.BACKUP_AI_BASE_URL
            model = settings.BACKUP_AI_MODEL

        system_prompt = prompt_manager.get_system_prompt("chat_stream")
        user_prompt = prompt_manager.get_user_prompt("chat_stream", context=context, query=query)

        try:
            # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
            async with AsyncOpenAI(api_key=api_key, base_url=base_url) as client:
                extra_body = {}
                if "modelscope" in str(client.base_url):
                    extra_body["enable_thinking"] = False

                logger.info(f"å¼€å§‹æµå¼å¯¹è¯è¯·æ±‚: model={model}, stream=True")
                stream = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    stream=True,
                    temperature=0.7,
                    timeout=60,
                    extra_body=extra_body if extra_body else None,
                )
                logger.info("æµå¼è¯·æ±‚å·²å»ºç«‹ï¼Œå¼€å§‹è¯»å– chunks")
                chunk_count = 0
                async for chunk in stream:
                    logger.debug(f"æ”¶åˆ°åŸå§‹å—: {chunk}")
                    if not chunk.choices:
                        logger.debug(f"å—ä¸­æ— é€‰é¡¹: {chunk}")
                        continue
                    content = chunk.choices[0].delta.content
                    if content:
                        chunk_count += 1
                        # logger.debug(f"ç”Ÿæˆå†…å®¹: {content!r}")
                        yield content
                    else:
                        logger.debug(f"å—å†…å®¹ä¸ºç©º: {chunk}")
                logger.info(f"æµå¼ä¼ è¾“ç»“æŸ, å…±å‘é€ {chunk_count} ä¸ª chunks")
        except Exception as e:
            logger.error(f"èŠå¤©æµé”™è¯¯: {e}", exc_info=True)
            yield f"é”™è¯¯: {str(e)}"

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        è¾“å…¥:
        - `texts`: å¾…å‘é‡åŒ–æ–‡æœ¬åˆ—è¡¨

        è¾“å‡º:
        - å‘é‡åˆ—è¡¨ï¼ˆä¸è¾“å…¥ä¸€ä¸€å¯¹åº”ï¼›å¤±è´¥æ—¶è¿”å›ç©ºå‘é‡ï¼‰

        ä½œç”¨:
        - è°ƒç”¨ embedding æœåŠ¡ç”Ÿæˆå‘é‡ï¼Œç”¨äºè¯­ä¹‰æ£€ç´¢ä¸èšç±»
        """

        if not texts:
            return []
        if not self._has_embedding():
            return [[] for _ in texts]
        cleaned_texts = [t.replace("\n", " ").strip()[:1000] for t in texts]

        url = f"{settings.SILICONFLOW_BASE_URL.rstrip('/')}/embeddings"
        headers = {
            "Authorization": f"Bearer {settings.SILICONFLOW_API_KEY}",
            "Content-Type": "application/json",
        }

        all_embeddings: List[List[float]] = []
        batch_size = 20

        for i in range(0, len(cleaned_texts), batch_size):
            batch = cleaned_texts[i : i + batch_size]
            payload = {
                "model": settings.EMBEDDING_MODEL,
                "input": batch,
                "encoding_format": "float",
            }
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, headers=headers, json=payload, timeout=30) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            batch_res = sorted(data["data"], key=lambda x: x["index"])
                            all_embeddings.extend([x["embedding"] for x in batch_res])
                        elif resp.status == 401:
                             error_text = await resp.text()
                             logger.error(f"âŒ å‘é‡ API è®¤è¯å¤±è´¥ (401): {error_text}")
                             raise AIConfigurationError("Embedding API Key æ— æ•ˆ")
                        else:
                            logger.error(f"âŒ å‘é‡ API é”™è¯¯: {await resp.text()}")
                            all_embeddings.extend([[] for _ in batch])
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ å‘é‡ç½‘ç»œé”™è¯¯: {e}")
                all_embeddings.extend([[] for _ in batch])
        return all_embeddings

    async def verify_cluster_batch(self, pairs: List[Dict[str, str]]) -> List[bool]:
        """
        è¾“å…¥:
        - `pairs`: å¾…æ ¸éªŒæ–°é—»å¯¹åˆ—è¡¨ï¼ˆleader/candidate æ ‡é¢˜ï¼‰

        è¾“å‡º:
        - å¸ƒå°”åˆ—è¡¨ï¼ˆä¸è¾“å…¥é¡ºåºä¸€è‡´ï¼Œè¡¨ç¤ºæ˜¯å¦åŒä¸€äº‹ä»¶ï¼‰

        ä½œç”¨:
        - ä½¿ç”¨å¤§æ¨¡å‹å¯¹ç›¸ä¼¼å€™é€‰è¿›è¡Œæ‰¹é‡æ ¸éªŒï¼Œå‡å°‘è¯¯åˆå¹¶
        """

        if not pairs:
            return []
        if not (self._has_main_llm() or self._has_backup_llm()):
            return [False] * len(pairs)

        system_prompt = prompt_manager.get_system_prompt("cluster_verification_batch")

        pairs_text = ""
        for i, p in enumerate(pairs):
            pairs_text += f"{i + 1}. [{p['leader']}] vs [{p['candidate']}]\n"
            
        user_content = prompt_manager.get_user_prompt("cluster_verification_batch", pairs_text=pairs_text)

        async def try_verify(client, model):
            try:
                res = await self._call_llm(client, model, user_content, system_prompt)
                if not res:
                    return None

                clean_res = res.strip()
                if clean_res.startswith("```"):
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]
                else:
                    start = clean_res.find("[")
                    end = clean_res.rfind("]")
                    if start != -1 and end != -1:
                        clean_res = clean_res[start : end + 1]

                # å°è¯•ä¿®å¤ Python é£æ ¼çš„å¸ƒå°”å€¼
                clean_res = clean_res.replace("True", "true").replace("False", "false")

                try:
                    results = json.loads(clean_res)
                except json.JSONDecodeError:
                    # å°è¯•ä½¿ç”¨æ­£åˆ™æå–å¸ƒå°”å€¼
                    import re
                    bool_matches = re.findall(r'\b(true|false)\b', clean_res, re.IGNORECASE)
                    if len(bool_matches) == len(pairs):
                        results = [b.lower() == 'true' for b in bool_matches]
                    else:
                        raise

                if isinstance(results, list) and len(results) == len(pairs):
                    return [bool(x) for x in results]

                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒè¿”å›æ ¼å¼é”™è¯¯: {results} (é¢„æœŸé•¿åº¦: {len(pairs)})")
                return None
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡æ ¸éªŒå¼‚å¸¸ ({model}): {e}")
                return None

        routes = self._iter_llm_routes(self._get_prefer_backup("CLUSTERING"))
        # å°† routes è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿ç´¢å¼•
        route_list = list(routes)
        
        for i, route in enumerate(route_list):
            # å¦‚æœæ˜¯ backup é€šé“ï¼Œå°è¯• 3 æ¬¡ï¼›å¦‚æœæ˜¯ main é€šé“ï¼Œå°è¯• 1 æ¬¡
            is_backup = (route["base_url"] == settings.BACKUP_AI_BASE_URL)
            max_attempts = 3 if is_backup else 1
            
            for attempt in range(max_attempts):
                if attempt > 0:
                    await asyncio.sleep(2 if attempt == 1 else 10)
                
                # ä½¿ç”¨ async with ç¡®ä¿èµ„æºé‡Šæ”¾
                async with AsyncOpenAI(api_key=route["api_key"], base_url=route["base_url"]) as client:
                    try:
                        res = await try_verify(client, route["model"])
                    except AIConfigurationError:
                        raise
                
                if res is not None:
                    return res
                
                # å¦‚æœè¿è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜å°è¯•å¤±è´¥ï¼ˆè¿”å›äº† Noneï¼‰
                if is_backup and attempt < max_attempts - 1:
                    logger.warning(f"âš ï¸ å¤‡ç”¨AIæ ¸éªŒå¤±è´¥ (ç¬¬{attempt + 1}æ¬¡)ï¼Œå‡†å¤‡é‡è¯•...")

            # å½“å‰è·¯ç”±çš„æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
            if i < len(route_list) - 1:
                next_route = route_list[i+1]
                logger.warning(f"âš ï¸ è·¯ç”± {route['model']} (CLUSTERING) è°ƒç”¨å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢åˆ° -> {next_route['model']}")

        logger.error("âŒ æ‰€æœ‰é€šé“æ ¸éªŒå‡å¤±è´¥ï¼Œè·³è¿‡æœ¬æ‰¹æ¬¡")
        return [False] * len(pairs)


    async def generate_topic_initial_summary(self, topic_name: str, news_list: List[Dict[str, str]]) -> str:
        """
        è¾“å…¥:
        - `topic_name`: ä¸“é¢˜åç§°
        - `news_list`: æ–°é—»åˆ—è¡¨ [{"title": "...", "content": "..."}, ...]

        è¾“å‡º:
        - ä¸“é¢˜åˆå§‹äº‹å®æ‘˜è¦ï¼ˆçº¯æ–‡æœ¬ï¼‰

        ä½œç”¨:
        - ç”ŸæˆåŸºäºäº‹å®çš„å¿«é€Ÿæ‘˜è¦ï¼Œç”¨äºä¸“é¢˜å®¡æ ¸ä¸å»é‡
        """
        if not news_list:
            return ""

        logger.info(f"ğŸ¤– ç”Ÿæˆä¸“é¢˜åˆå§‹æ‘˜è¦: {topic_name} ({len(news_list)} æ¡æ–°é—»)")
        
        # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…Tokenæº¢å‡º
        input_text = ""
        # åˆå§‹æ‘˜è¦ä¸éœ€è¦å¤ªå¤šæ–°é—»ï¼Œå– Top 10-15 å³å¯
        for i, item in enumerate(news_list[:15]): 
            t = (item.get("title") or "").replace("\n", " ")
            c = (item.get("summary") or item.get("content") or "")[:200].replace("\n", " ")
            input_text += f"[{i+1}] {t}\n   å†…å®¹: {c}\n\n"
        
        system_prompt = prompt_manager.get_system_prompt("topic_initial_summary")
        user_prompt = prompt_manager.get_user_prompt("topic_initial_summary", topic_name=topic_name, input_text=input_text)
        
        # ä½¿ç”¨å•ç‹¬é…ç½®çš„è·¯ç”±ï¼Œé»˜è®¤ main
        prefer_backup = self._get_prefer_backup("TOPIC_INITIAL_SUMMARY")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res.strip()
        return ""

    async def generate_topic_overview(self, topic_name: str, news_list: List[Dict[str, str]]) -> str:
        """
        è¾“å…¥:
        - `topic_name`: ä¸“é¢˜åç§°
        - `news_list`: æ–°é—»åˆ—è¡¨ [{"title": "...", "content": "..."}, ...]

        è¾“å‡º:
        - ä¸“é¢˜å¤šç»´åº¦ç»¼è¿°ï¼ˆçº¯æ–‡æœ¬ï¼‰
        
        ä½œç”¨:
        - ä»â€œäº‹ä»¶èƒŒæ™¯â€ã€â€œå‘å±•è¿‡ç¨‹â€ã€â€œå„æ–¹è§‚ç‚¹â€ã€â€œæœªæ¥å±•æœ›â€ç­‰ç»´åº¦å¯¹ä¸“é¢˜è¿›è¡Œæ·±åº¦æ€»ç»“
        """
        if not news_list:
            return ""
            
        logger.info(f"ğŸ¤– ç”Ÿæˆä¸“é¢˜ç»¼è¿°: {topic_name} ({len(news_list)} æ¡æ–°é—»)")
        
        # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…Tokenæº¢å‡º
        # ä¼˜å…ˆå–æœ€æ–°çš„æ–°é—»ï¼Œå’Œæœ€æ—©çš„æ–°é—»ï¼Œä»¥è¦†ç›–å…¨è²Œ
        # å‡è®¾ news_list å·²ç»åŒ…å«äº†ä¸€å®šæ•°é‡çš„ä»£è¡¨æ€§æ–°é—»
        input_text = ""
        for i, item in enumerate(news_list[:30]): # æœ€å¤šå–30æ¡ä½œä¸ºä¸Šä¸‹æ–‡
            t = (item.get("title") or "").replace("\n", " ")
            c = (item.get("summary") or item.get("content") or "")[:200].replace("\n", " ")
            input_text += f"[{i+1}] {t}\n   æ‘˜è¦: {c}\n\n"
            
        system_prompt = prompt_manager.get_system_prompt("topic_overview")
        
        user_prompt = prompt_manager.get_user_prompt("topic_overview", topic_name=topic_name, input_text=input_text)
        
        prefer_backup = self._get_prefer_backup("TOPIC_OVERVIEW")
        res = await self._call_llm_with_routes(user_prompt, system_prompt, prefer_backup=prefer_backup)
        if res:
            return res.strip()
        return ""


ai_service = AIService() 

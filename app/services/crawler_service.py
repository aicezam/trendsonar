"""
æœ¬æ–‡ä»¶ç”¨äºå®ç°å…¨ç½‘æŠ“å–ä¸è§£æé€»è¾‘ï¼ŒåŒ…æ‹¬ RSS/HTML è§£æã€å†…å®¹æŠ“å–ä¸å…¥åº“ç­‰æµç¨‹ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `CrawlerService`: æŠ“å–æœåŠ¡å®ç°
- `crawler_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import asyncio
import email.utils
import json
import logging
import contextlib
import io
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import aiohttp
from bs4 import BeautifulSoup
from sqlalchemy import select
from sqlalchemy.dialects.postgresql import insert

from app.core.config import BASE_DIR, get_settings
from app.core.database import AsyncSessionLocal
from app.core.logger import setup_logger
from app.models.news import News
from app.services.ai_service import ai_service
from app.utils.tools import clean_html_tags

settings = get_settings()
logger = setup_logger("CrawlerService")


class CrawlerService:
    """
    è¾“å…¥:
    - `news_sources.json` ä¸­çš„æ–°é—»æºé…ç½®

    è¾“å‡º:
    - æŠ“å–åˆ°çš„æ–°é—»å…ƒä¿¡æ¯åˆ—è¡¨ï¼Œå¹¶å…¥åº“ä¸º `News` è®°å½•

    ä½œç”¨:
    - è´Ÿè´£ä» RSS/API/ç½‘é¡µç­‰æ¥æºæŠ“å–æ–°é—»ï¼Œå¹¶å°†ç»“æœå†™å…¥æ•°æ®åº“
    """

    def __init__(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå§‹åŒ–æ–°é—»æºé…ç½®æ–‡ä»¶å
        """

        self.sources_file = "news_sources.json"

    def _get_sources_path_candidates(self) -> List[Path]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å¯èƒ½çš„æ–°é—»æºé…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

        ä½œç”¨:
        - å…¼å®¹ `data/` ä¸é¡¹ç›®æ ¹ç›®å½•ä¸¤ç§æ”¾ç½®æ–¹å¼
        """

        return [
            BASE_DIR / "data" / self.sources_file,
            BASE_DIR / self.sources_file,
        ]

    def load_sources(self) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ–°é—»æºé…ç½®åˆ—è¡¨ï¼ˆæ¯é¡¹åŒ…å« name/weight/addressï¼‰

        ä½œç”¨:
        - ä»é…ç½®æ–‡ä»¶è¯»å–æ–°é—»æºï¼Œä¸ºæŠ“å–æµç¨‹æä¾›è¾“å…¥
        """

        try:
            path = None
            for candidate in self._get_sources_path_candidates():
                if candidate.exists():
                    path = candidate
                    break

            if path is None:
                logger.warning("æœªæ‰¾åˆ°æ–°é—»æºæ–‡ä»¶")
                return []

            with path.open("r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ–°é—»æºå¤±è´¥: {e}")
            return []

    def _clean_summary(self, summary: Optional[str]) -> Optional[str]:
        """
        è¾“å…¥:
        - `summary`: åŸå§‹æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å« HTMLï¼‰

        è¾“å‡º:
        - æ¸…æ´—åçš„æ‘˜è¦

        ä½œç”¨:
        - ç§»é™¤æ— æ³•è®¿é—®çš„å†…éƒ¨å›¾ç‰‡é“¾æ¥
        """
        if not summary:
            return summary
            
        # ä¼˜å…ˆä½¿ç”¨ clean_html_tags è¿›è¡Œå½»åº•æ¸…æ´—
        return clean_html_tags(summary)


    def _process_meta(
        self,
        source_name: str,
        weight: float,
        title: Optional[str],
        url: Optional[str],
        pub_date: datetime,
        summary: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `source_name`: æ¥æºåç§°
        - `weight`: æ¥æºæƒé‡
        - `title`: æ ‡é¢˜
        - `url`: é“¾æ¥
        - `pub_date`: å‘å¸ƒæ—¶é—´
        - `summary`: æ‘˜è¦ï¼ˆå¯é€‰ï¼‰

        è¾“å‡º:
        - æ¸…æ´—åçš„æ–°é—»å…ƒä¿¡æ¯å­—å…¸ï¼›ä¸åˆè§„æ—¶è¿”å› None

        ä½œç”¨:
        - ç»Ÿä¸€åšç©ºå€¼ã€åŸŸåé»‘åå•è¿‡æ»¤ä¸å­—æ®µæ ‡å‡†åŒ–
        """

        if not url or not title:
            return None
        for domain in settings.IGNORED_DOMAINS:
            if domain in url:
                return None
        
        # æ¸…æ´—æ‘˜è¦ä¸­çš„åé“¾
        cleaned_summary = self._clean_summary(summary)

        return {
            "title": title.strip(),
            "url": url.strip(),
            "source": source_name,
            "publish_date": pub_date,
            "heat": weight,
            "summary": cleaned_summary,
        }

    async def fetch_and_parse(
        self, session: aiohttp.ClientSession, source: Dict[str, Any], prefix: str = ""
    ) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `session`: å¤ç”¨çš„ HTTP ä¼šè¯
        - `source`: å•ä¸ªæ–°é—»æºé…ç½®
        - `prefix`: æ—¥å¿—å‰ç¼€ï¼ˆç”¨äºå¹¶å‘è¿›åº¦å±•ç¤ºï¼‰

        è¾“å‡º:
        - å½“å‰æ–°é—»æºæŠ“å–åˆ°çš„æ–°é—»å…ƒä¿¡æ¯åˆ—è¡¨

        ä½œç”¨:
        - å…¼å®¹ JSON APIã€RSS/XMLï¼Œå¹¶åœ¨å¤±è´¥æ—¶å°è¯•ç”¨ AI ä»é¡µé¢å†…å®¹ä¸­æŠ½å–æ¡ç›®
        """

        url = source.get("address")
        name = source.get("name")
        weight = source.get("weight", 1.0)

        log_prefix = f"   {prefix}" if prefix else ""
        logger.debug(f"{log_prefix} æ­£åœ¨æŠ“å–: {name} ({url})")

        try:
            async with session.get(url, timeout=20) as resp:
                if resp.status != 200:
                    logger.error(f"æŠ“å–å¤±è´¥ {name}: HTTP {resp.status}")
                    return []

                content_type = resp.headers.get("Content-Type", "").lower()
                try:
                    text = await resp.text()
                except UnicodeDecodeError:
                    raw = await resp.read()
                    text = raw.decode("utf-8", errors="ignore")

                items = []

                if "json" in content_type:
                    try:
                        data = json.loads(text)
                        raw_items = []
                        if isinstance(data, list):
                            raw_items = data
                        elif isinstance(data, dict):
                            raw_items = data.get("data", []) or data.get("items", []) or data.get("stories", [])

                        for item in raw_items:
                            title = item.get("title")
                            link = item.get("url") or item.get("link") or item.get("share_url")
                            summary = item.get("summary") or item.get("description") or item.get("digest")
                            pub_date = datetime.now()

                            p = self._process_meta(name, weight, title, link, pub_date, summary)
                            if p:
                                items.append(p)

                        if items:
                            pass
                    except Exception as e:
                        logger.warning(f"JSONè§£æå¤±è´¥ {name}: {e}")

                if not items:
                    try:
                        features = "xml"
                        try:
                            import lxml  # noqa: F401

                            features = "lxml-xml"
                        except ImportError:
                            pass

                        soup = BeautifulSoup(text, features)
                        rss_items = soup.find_all("item") or soup.find_all("entry")
                        if rss_items:
                            for item in rss_items:
                                title_elem = item.find("title")
                                title = title_elem.text if title_elem else ""

                                link_elem = item.find("link")
                                link = link_elem.text if link_elem else ""
                                if not link and link_elem and link_elem.get("href"):
                                    link = link_elem.get("href")

                                desc_elem = item.find("content:encoded") or item.find("content")
                                if not desc_elem:
                                    desc_elem = item.find("description") or item.find("summary")

                                summary = desc_elem.text if desc_elem else ""

                                pub_date = datetime.now()
                                date_elem = item.find("pubDate") or item.find("dc:date") or item.find("updated")
                                if date_elem and date_elem.text:
                                    try:
                                        parsed_tuple = email.utils.parsedate_tz(date_elem.text)
                                        if parsed_tuple:
                                            ts = email.utils.mktime_tz(parsed_tuple)
                                            pub_date = datetime.fromtimestamp(ts)
                                        else:
                                            dt_str = date_elem.text.replace("Z", "+00:00")
                                            pub_date = datetime.fromisoformat(dt_str)
                                    except Exception:
                                        pass

                                p = self._process_meta(name, weight, title, link, pub_date, summary)
                                if p:
                                    items.append(p)
                            if items:
                                return items
                    except Exception as e:
                        logger.warning(f"RSS/XMLè§£æå¤±è´¥ {name}: {e}")

                if not items:
                    logger.info(f"å¸¸è§„è§£æå¤±è´¥ï¼Œå°è¯• AI æå–: {name}")
                    extracted_items = await ai_service.extract_news_info(text)
                    for item in extracted_items:
                        p = self._process_meta(
                            name, weight, item.get("title"), item.get("link"), datetime.now(), item.get("summary")
                        )
                        if p:
                            items.append(p)

                return items

        except Exception as e:
            logger.error(f"æŠ“å–å¼‚å¸¸ {name}: {e}")
            return []

    async def fetch_all_sources(self) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å…¨éƒ¨æ–°é—»æºæŠ“å–åˆ°çš„æ–°é—»å…ƒä¿¡æ¯åˆ—è¡¨

        ä½œç”¨:
        - å¹¶å‘æŠ“å–å¤šä¸ªæ–°é—»æºå¹¶æ±‡æ€»ç»“æœ
        """

        logger.info("ğŸ•·ï¸ å¼€å§‹å…¨ç½‘æŠ“å– (åŸºäºé…ç½®æº)...")
        sources = self.load_sources()
        if not sources:
            logger.warning("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–°é—»æºï¼Œè¯·æ£€æŸ¥ news_sources.json")
            return []

        all_news = []

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        }

        connector = aiohttp.TCPConnector(limit=50)
        async with aiohttp.ClientSession(connector=connector, headers=headers) as session:
            tasks = []
            total_sources = len(sources)

            async def fetch_wrapper(idx, src):
                return await self.fetch_and_parse(session, src, prefix=f"({idx}/{total_sources})")

            for i, src in enumerate(sources, 1):
                tasks.append(fetch_wrapper(i, src))

            results = await asyncio.gather(*tasks)
            for res in results:
                all_news.extend(res)

        # æ˜¾å¼ GC
        import gc
        gc.collect()
        
        return all_news

    async def save_raw_news(self, news_list: List[Dict[str, Any]]) -> None:
        """
        è¾“å…¥:
        - `news_list`: æŠ“å–åˆ°çš„æ–°é—»å…ƒä¿¡æ¯åˆ—è¡¨

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - æŒ‰å…³é”®è¯è¿‡æ»¤/å»é‡åï¼Œå°†æ–°é—»å†™å…¥æ•°æ®åº“ï¼Œå¹¶ä¿è¯ URL å”¯ä¸€
        """

        if not news_list:
            return

        follow_keywords = settings.FOLLOW_KEYWORDS
        final_list = news_list

        if follow_keywords:
            keywords = [k.strip() for k in follow_keywords.split(",") if k.strip()]
            if keywords:
                logger.info(f"ğŸ” å¼€å§‹å…³é”®è¯è¿‡æ»¤ (å…³é”®è¯: {keywords})")

                titles = [item["title"] for item in news_list]
                try:
                    title_embeddings = await ai_service.get_embeddings(titles)
                    keyword_embeddings = await ai_service.get_embeddings(keywords)

                    if title_embeddings and keyword_embeddings:
                        filtered_list = []
                        import numpy as np

                        kw_vecs = np.array(keyword_embeddings)
                        kw_norms = np.linalg.norm(kw_vecs, axis=1, keepdims=True)
                        kw_vecs_norm = kw_vecs / (kw_norms + 1e-9)

                        for i, item in enumerate(news_list):
                            if not title_embeddings[i]:
                                continue

                            t_vec = np.array(title_embeddings[i])
                            t_norm = np.linalg.norm(t_vec)
                            if t_norm == 0:
                                continue
                            t_vec_norm = t_vec / t_norm

                            sims = np.dot(t_vec_norm, kw_vecs_norm.T)
                            max_sim = float(np.max(sims))

                            if max_sim >= settings.FOLLOW_KEYWORDS_THRESHOLD:
                                filtered_list.append(item)

                        logger.info(f"âœ… è¿‡æ»¤å®Œæˆ: {len(news_list)} -> {len(filtered_list)}")
                        final_list = filtered_list
                    else:
                        logger.warning("âš ï¸ å‘é‡è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡å…³é”®è¯è¿‡æ»¤")
                except Exception as e:
                    logger.error(f"âŒ å…³é”®è¯è¿‡æ»¤å¼‚å¸¸: {e}")

        urls = [item["url"] for item in final_list]
        existing_urls = set()
        async with AsyncSessionLocal() as db:
            if urls:
                result = await db.execute(select(News.url).where(News.url.in_(urls)))
                existing_urls = set(r for r in result.scalars())

        final_list = [item for item in final_list if item["url"] not in existing_urls]

        titles = [item["title"] for item in final_list]
        existing_pairs = set()
        if titles:
            async with AsyncSessionLocal() as db:
                result = await db.execute(select(News.title, News.source).where(News.title.in_(titles)))
                for row in result:
                    existing_pairs.add((row.title, row.source))

        original_count = len(final_list)
        final_list = [item for item in final_list if (item["title"], item["source"]) not in existing_pairs]
        skipped_by_title = original_count - len(final_list)
        if skipped_by_title > 0:
            logger.debug(f"   ğŸš« å› æ ‡é¢˜å’Œæ¥æºé‡å¤è·³è¿‡ {skipped_by_title} æ¡")

        async with AsyncSessionLocal() as db:
            count = 0
            for item in final_list:
                stmt = insert(News).values(
                    url=item["url"],
                    title=item["title"],
                    source=item["source"],
                    publish_date=item["publish_date"],
                    heat_score=item["heat"],
                    summary=item.get("summary", ""),
                    sources=[{"name": item["source"], "url": item["url"]}],
                    sentiment_score=item.get("sentiment_score", 50.0),
                    sentiment_label=item.get("sentiment_label", "ä¸­ç«‹"),
                    keywords=item.get("keywords", []),
                    entities=item.get("entities", []),
                )

                stmt = stmt.on_conflict_do_nothing(index_elements=["url"])
                res = await db.execute(stmt)
                if res.rowcount > 0:
                    count += 1
            await db.commit()
            logger.info(f"ğŸ“¥ å…¥åº“æ–°å¢ {count} æ¡")

    async def _refresh_weibo_cookie(self) -> Optional[str]:
        """
        è‡ªåŠ¨åˆ·æ–°å¾®åšè®¿å®¢ Cookie
        """
        logger.info("ğŸ”„ æ­£åœ¨å°è¯•è‡ªåŠ¨åˆ·æ–°å¾®åš Cookie...")
        try:
            from playwright.async_api import async_playwright
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                page = await context.new_page()
                
                # è®¿é—®å¾®åšæœç´¢é¡µé¢ï¼Œè§¦å‘è®¿å®¢è®¤è¯
                try:
                    await page.goto("https://s.weibo.com/weibo?q=Python", timeout=30000)
                    await page.wait_for_load_state("networkidle")
                except Exception as e:
                    logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶æˆ–å‡ºé”™ï¼Œå°è¯•ç›´æ¥è·å–Cookie: {e}")

                cookies = await context.cookies()
                await browser.close()
                
                # æå–å¹¶æ‹¼æ¥ Cookie
                cookie_list = [f"{c['name']}={c['value']}" for c in cookies]
                cookie_str = "; ".join(cookie_list)
                
                if "SUB=" in cookie_str:
                    logger.info("âœ… å¾®åš Cookie åˆ·æ–°æˆåŠŸ")
                    # æ›´æ–°å†…å­˜ä¸­çš„é…ç½®
                    settings.WEIBO_COOKIE = cookie_str

                    # å°è¯•æŒä¹…åŒ–åˆ° config.yaml
                    try:
                        from app.utils.config_io import load_yaml_dict, dump_yaml_text, save_yaml_text
                        from app.core.config import CONFIG_PATH
                        
                        config_data = load_yaml_dict(CONFIG_PATH)
                        config_data["WEIBO_COOKIE"] = cookie_str
                        save_yaml_text(CONFIG_PATH, dump_yaml_text(config_data))
                        logger.info("ğŸ’¾ å¾®åš Cookie å·²ä¿å­˜åˆ° config.yaml")
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜ Cookie åˆ°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

                    return cookie_str
                else:
                    logger.warning("âš ï¸ å¾®åš Cookie åˆ·æ–°å¤±è´¥: æœªæ‰¾åˆ° SUB å­—æ®µ")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ å¾®åš Cookie åˆ·æ–°å¼‚å¸¸: {e}")
            return None

    async def crawl_weibo_simple(self, session: aiohttp.ClientSession, url: str, retry: bool = True) -> Optional[str]:
        """
        è¾“å…¥:
        - `session`: HTTP ä¼šè¯
        - `url`: å¾®åšè¯¦æƒ…é¡µé“¾æ¥
        - `retry`: æ˜¯å¦åœ¨å¤±è´¥æ—¶å°è¯•åˆ·æ–° Cookie å¹¶é‡è¯•

        è¾“å‡º:
        - æŠ“å–åˆ°çš„æ­£æ–‡æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - ä»¥è½»é‡æ–¹å¼æŠ“å–å¾®åšå†…å®¹ï¼Œé¿å…é‡å‹æ¸²æŸ“å¸¦æ¥çš„å¼€é”€
        """

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cookie": settings.WEIBO_COOKIE,
        }

        logger.debug(f"   ğŸ” [å¾®åšæŠ“å–] æ­£åœ¨æŠ“å–: {url}")
        try:
            async with session.get(url, headers=headers, timeout=30) as resp:
                if resp.status != 200:
                    logger.error(f"   âŒ [å¾®åšæŠ“å–] HTTP {resp.status}")
                    return None

                content_bytes = await resp.read()
                try:
                    html = content_bytes.decode("utf-8")
                except UnicodeDecodeError:
                    try:
                        html = content_bytes.decode("gb18030")
                    except UnicodeDecodeError:
                        html = content_bytes.decode("utf-8", errors="replace")

                soup = BeautifulSoup(html, "html.parser")

                cards = soup.select("div.card-wrap")
                content_list = []

                for card in cards:
                    txt_p = card.select_one("p.txt")
                    if txt_p:
                        text = txt_p.get_text(separator=" ", strip=True)
                        content_list.append(text)

                if not content_list:
                    logger.warning("   âš ï¸ [å¾®åšæŠ“å–] æœªæ‰¾åˆ°å¾®åšå¡ç‰‡ï¼Œå°è¯•æå–å…¨æ–‡")
                    body_text = soup.body.get_text(separator="\n", strip=True) if soup.body else ""
                    if "Sina Visitor System" in body_text or "è®¿é—®å—é™" in body_text:
                        logger.error("   âŒ [å¾®åšæŠ“å–] è§¦å‘åçˆ¬éªŒè¯")
                        
                        if retry:
                            new_cookie = await self._refresh_weibo_cookie()
                            if new_cookie:
                                return await self.crawl_weibo_simple(session, url, retry=False)

                        return None
                    return body_text[:5000]

                logger.debug(f"   âœ… [å¾®åšæŠ“å–] æŠ“å–åˆ° {len(content_list)} æ¡å¾®åš")
                return "\n\n".join(content_list)

        except Exception as e:
            logger.error(f"   âŒ [å¾®åšæŠ“å–] å¼‚å¸¸: {e}")
            return None

    @contextlib.asynccontextmanager
    async def make_crawler(self):
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - AsyncWebCrawler å®ä¾‹ä¸Šä¸‹æ–‡

        ä½œç”¨:
        - åˆ›å»ºå¹¶ç®¡ç†çˆ¬è™«å®ä¾‹çš„ç”Ÿå‘½å‘¨æœŸï¼Œæ”¯æŒæ‰¹é‡å¤ç”¨
        """
        try:
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig

            log_level = getattr(logging, (settings.LOG_LEVEL or "").upper(), logging.INFO)
            is_verbose = log_level == logging.DEBUG
            
            browser_conf = BrowserConfig(headless=True, verbose=is_verbose)
            
            async with AsyncWebCrawler(config=browser_conf) as crawler:
                yield crawler

        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            yield None

    async def crawl_content_with_instance(self, target_url: str, crawler) -> Optional[str]:
        """
        è¾“å…¥:
        - `target_url`: ç›®æ ‡é¡µé¢ URL
        - `crawler`: å¤ç”¨çš„çˆ¬è™«å®ä¾‹

        è¾“å‡º:
        - é¡µé¢æ­£æ–‡/Markdown æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - ä½¿ç”¨å¤ç”¨çš„çˆ¬è™«å®ä¾‹æŠ“å–å†…å®¹ï¼Œå‡å°‘æµè§ˆå™¨å¯åŠ¨å¼€é”€
        """
        if not crawler:
            return await self.crawl_content(target_url)

        logger.debug(f"æŠ“å–æ–°é—» (å¤ç”¨å®ä¾‹): {target_url}")

        if "weibo.com" in target_url or "weibo.cn" in target_url:
            try:
                async with aiohttp.ClientSession() as session:
                    return await self.crawl_weibo_simple(session, target_url)
            except Exception as e:
                logger.error(f"âŒ å¾®åšæŠ“å–å¤±è´¥: {e}")
                return None

        try:
            from crawl4ai import CacheMode, CrawlerRunConfig

            log_level = getattr(logging, (settings.LOG_LEVEL or "").upper(), logging.INFO)
            is_verbose = log_level == logging.DEBUG

            run_conf = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                verbose=is_verbose
            )

            result = await crawler.arun(url=target_url, config=run_conf)

            if result and result.markdown:
                if hasattr(result.markdown, "raw_markdown"):
                    return clean_html_tags(result.markdown.raw_markdown)
                return clean_html_tags(str(result.markdown))

        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return None

    async def crawl_content(self, target_url: str) -> Optional[str]:
        """
        è¾“å…¥:
        - `target_url`: ç›®æ ‡é¡µé¢ URL

        è¾“å‡º:
        - é¡µé¢æ­£æ–‡/Markdown æ–‡æœ¬ï¼›å¤±è´¥è¿”å› None

        ä½œç”¨:
        - æŠ“å–æ–°é—»æ­£æ–‡ï¼Œç”¨äºæ‘˜è¦ç”Ÿæˆä¸æ·±åº¦åˆ†æ
        """

        logger.debug(f"æŠ“å–æ–°é—»: {target_url}")

        if "weibo.com" in target_url or "weibo.cn" in target_url:
            try:
                async with aiohttp.ClientSession() as session:
                    return await self.crawl_weibo_simple(session, target_url)
            except Exception as e:
                logger.error(f"âŒ å¾®åšæŠ“å–å¤±è´¥: {e}")
                return None

        try:
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig

            log_level = getattr(logging, (settings.LOG_LEVEL or "").upper(), logging.INFO)
            is_verbose = log_level == logging.DEBUG
            
            browser_conf = BrowserConfig(headless=True, verbose=is_verbose)
            run_conf = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                verbose=is_verbose
            )

            async with AsyncWebCrawler(config=browser_conf) as crawler:
                result = await crawler.arun(url=target_url, config=run_conf)

            if result and result.markdown:
                if hasattr(result.markdown, "raw_markdown"):
                    return clean_html_tags(result.markdown.raw_markdown)
                return clean_html_tags(str(result.markdown))

        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return None


crawler_service = CrawlerService()
# app/services/topic_service.py

from __future__ import annotations

import asyncio
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple

from collections import defaultdict
import numpy as np
from sqlalchemy import desc, select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import get_settings
from app.core.database import AsyncSessionLocal, check_db_connection
from app.core.logger import setup_logger
from app.core.exceptions import AIConfigurationError
from app.models.news import News
from app.models.topic import Topic, TopicTimelineItem
from app.services.ai_service import AIService
from app.services.crawler_service import crawler_service
from app.utils.tools import clean_html_tags

settings = get_settings()
logger = setup_logger("TopicService")


class TopicService:
    def __init__(self, ai: AIService) -> None:
        self.ai = ai

    @staticmethod
    def _cosine_similarity(a: List[float], b: List[float]) -> float:
        if not a or not b:
            return 0.0
        va = np.array(a, dtype=np.float32)
        vb = np.array(b, dtype=np.float32)
        na = float(np.linalg.norm(va))
        nb = float(np.linalg.norm(vb))
        if na <= 0 or nb <= 0:
            return 0.0
        return float(np.dot(va, vb) / (na * nb))

    async def refresh_topics(self) -> None:
        """
        ä¸“é¢˜è¿½è¸ªé€»è¾‘ï¼š
        1. æ‰¾å‡ºæœªå½’ç±»çš„æ–°é—»ï¼ˆNå¤©å†…ï¼‰ã€‚
        2. èšåˆæ ‡é¢˜è®© AI æç‚¼ä¸“é¢˜ã€‚
        3. å¯¹æç‚¼çš„ä¸“é¢˜è¿›è¡Œå‘é‡åŒ¹é…+AIæ ¸éªŒã€‚
        4. åªæœ‰æ–°é—»æ•° > 3 çš„ä¸“é¢˜æ‰åˆ›å»ºã€‚
        5. è¡¥å…¨è¯¦æƒ…ã€‚
        """
        if not (settings.DATABASE_URL or "").strip():
            return

        async with AsyncSessionLocal() as db:
            # 1. è·å–å·²å½’ç±»çš„æ–°é—»IDé›†åˆ
            used_stmt = select(TopicTimelineItem.news_id).where(TopicTimelineItem.news_id.isnot(None))
            used_ids_res = await db.execute(used_stmt)
            used_ids = set(used_ids_res.scalars().all())
            
            # 2. è·å–å€™é€‰æ–°é—»æ± ï¼ˆNå¤©å†…ï¼Œæœªå½’ç±»ï¼‰
            days = settings.TOPIC_LOOKBACK_DAYS
            start_date = datetime.now() - timedelta(days=days)
            
            # å…ˆæŸ¥æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œç”¨äºåç»­å‘é‡åŒ¹é…
            # é™åˆ¶æ•°é‡é˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼Œæ¯”å¦‚å–æœ€è¿‘ 2000 æ¡
            pool_stmt = (
                select(News)
                .where(News.publish_date >= start_date)
                .where(News.id.notin_(used_ids) if used_ids else True)
                .order_by(desc(News.heat_score))
                .limit(settings.TOPIC_RECALL_POOL_SIZE)
            )
            news_pool = (await db.execute(pool_stmt)).scalars().all()
            
            if not news_pool:
                logger.info("ğŸ“­ æ²¡æœ‰å¾…å¤„ç†çš„æ–°é—»ï¼Œè·³è¿‡ä¸“é¢˜ç”Ÿæˆ")
                return
                
            logger.info(f"ğŸ“Š å¾…å¤„ç†æ–°é—»æ± å¤§å°: {len(news_pool)}")
            
            # ç¡®ä¿æ± ä¸­æ–°é—»æœ‰å‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            pool_vecs = await self._ensure_news_embeddings_batch(db, news_pool)

            # 3. å‡†å¤‡ AI æç‚¼çš„ç§å­æ ‡é¢˜ï¼ˆTop 300ï¼‰
            # news_pool å·²ç»æ˜¯æŒ‰ heat_score æ’åºçš„
            # è¿‡æ»¤æ‰ä½çƒ­åº¦æ–°é—»
            min_heat = settings.TOPIC_NEWS_MIN_HEAT
            seed_news = [n for n in news_pool if (n.heat_score or 0) >= min_heat][:settings.TOPIC_AGGREGATION_TOP_N]
            
            if not seed_news:
                logger.info(f"ğŸ“­ ç»çƒ­åº¦è¿‡æ»¤(>{min_heat})åï¼Œæ— ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œè·³è¿‡ä¸“é¢˜ç”Ÿæˆ")
                return

            # æ ¼å¼åŒ–æ ‡é¢˜ï¼Œå¸¦ä¸Šçƒ­åº¦ä¿¡æ¯
            seed_titles = [f"[çƒ­åº¦:{float(n.heat_score or 0):.1f}] {(n.title or '').strip()}" for n in seed_news if (n.title or "").strip()]
            
            # 4. AI æç‚¼ä¸“é¢˜
            proposed_topics = await self.ai.propose_topics_from_titles(seed_titles)
            if not proposed_topics:
                logger.info("âš ï¸ AI æœªæç‚¼å‡ºä»»ä½•ä¸“é¢˜")
                return

            # è·å–ç°æœ‰çš„ Active ä¸“é¢˜ï¼Œç”¨äºæŸ¥é‡å’Œå»¶ä¼¸åˆ¤æ–­
            active_topics_stmt = select(Topic).where(Topic.status == "active")
            active_topics = (await db.execute(active_topics_stmt)).scalars().all()

            # 4.1 æ–°å¢ï¼šä¸“é¢˜è´¨é‡è¯„ä¼°ä¸è¿‡æ»¤
            # å°†ç°æœ‰ä¸“é¢˜è½¬ä¸ºç®€å•å­—å…¸ä¾› AI å‚è€ƒ
            existing_topics_data = [{"name": t.name, "description": t.summary or ""} for t in active_topics]
            proposed_topics = await self.ai.batch_evaluate_topic_quality(proposed_topics, existing_topics=existing_topics_data)
            
            if not proposed_topics:
                logger.info("âš ï¸ ç» AI è¯„ä¼°ï¼Œæ‰€æœ‰æç‚¼ä¸“é¢˜å‡è¿‡äºå®½æ³›æˆ–è´¨é‡ä¸ä½³ï¼Œè·³è¿‡")
                return

            # ç¡®ä¿ç°æœ‰ä¸“é¢˜æœ‰å‘é‡
            active_topic_vecs = await self._ensure_topic_embeddings(db, active_topics)

            # 5. å¤„ç†æ¯ä¸ªæç‚¼å‡ºçš„ä¸“é¢˜
            new_topics_created = 0
            updated_topics_count = 0
            
            # è®°å½•æœ¬è½®å·²å¤„ç†ï¼ˆåˆ›å»ºæˆ–æ›´æ–°ï¼‰çš„ä¸“é¢˜ID
            processed_topic_ids = set()

            # å‡†å¤‡å…³æ³¨å…³é”®è¯å‘é‡ (å¦‚æœæœ‰é…ç½®)
            follow_keywords = settings.FOLLOW_KEYWORDS
            keyword_vecs = []
            if follow_keywords:
                kw_list = [k.strip() for k in follow_keywords.split(",") if k.strip()]
                if kw_list:
                    logger.info(f"ğŸ” [Topic Filter] å¯ç”¨å…³é”®è¯è¿‡æ»¤: {kw_list}")
                    kw_embs = await self.ai.get_embeddings(kw_list)
                    keyword_vecs = [v for v in kw_embs if v]
            
            # === Phase 1: å¤„ç† AI æç‚¼çš„æ½œåœ¨ä¸“é¢˜ ===
            for p_topic in proposed_topics:
                t_name = p_topic.get("name", "")
                t_desc = p_topic.get("description", "")
                
                if not t_name:
                    continue
                    
                logger.info(f"ğŸ” [Phase 1] æ­£åœ¨è¯„ä¼°æç‚¼ä¸“é¢˜: {t_name}")
                
                # è®¡ç®—è¯¥æ½œåœ¨ä¸“é¢˜çš„å‘é‡
                t_txt = f"{t_name} {t_desc}"
                t_embs = await self.ai.get_embeddings([t_txt])
                t_vec = t_embs[0] if t_embs and t_embs[0] else []
                
                if not t_vec:
                    logger.warning(f"   âš ï¸ æ— æ³•ç”Ÿæˆå‘é‡: {t_name}")
                    continue

                # 5.0 å…³é”®è¯è¿‡æ»¤
                if keyword_vecs:
                    max_sim = max([self._cosine_similarity(t_vec, kv) for kv in keyword_vecs]) if keyword_vecs else 0
                    if max_sim < settings.FOLLOW_KEYWORDS_THRESHOLD:
                        logger.info(f"   â© ä¸“é¢˜ '{t_name}' ä¸å…³æ³¨å…³é”®è¯ç›¸å…³åº¦ä¸è¶³ ({max_sim:.2f} < {settings.FOLLOW_KEYWORDS_THRESHOLD})ï¼Œè·³è¿‡")
                        continue

                # 5.1 æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ä¸“é¢˜é‡å¤
                existing_topic_obj = None

                for existing_t, existing_vec in active_topic_vecs:
                    sim = self._cosine_similarity(t_vec, existing_vec)
                    # é™ä½é˜ˆå€¼è‡³ 0.6 ä»¥æ•æ‰æ›´å¤šæ½œåœ¨é‡å¤ï¼Œç„¶åäº¤ç»™ AI ç»†åˆ¤
                    if sim > 0.6: 
                        logger.info(f"   ğŸ”„ ä¸ç°æœ‰ä¸“é¢˜ '{existing_t.name}' ç›¸ä¼¼ (sim={sim:.2f})ï¼Œæ­£åœ¨è¿›è¡Œ AI äºŒæ¬¡æ ¸éªŒ...")
                        
                        is_duplicate, reason = await self.ai.check_topic_duplicate(
                            t_name, t_desc, existing_t.name, existing_t.summary or ""
                        )
                        
                        if is_duplicate:
                            logger.info(f"   âœ… AI ç¡®è®¤é‡å¤ (ç†ç”±: {reason})ï¼Œå°†åˆå¹¶åˆ°ç°æœ‰ä¸“é¢˜: {existing_t.name}")
                            existing_topic_obj = existing_t
                            processed_topic_ids.add(existing_t.id)
                            break
                        else:
                            logger.info(f"   âŒ AI åˆ¤å®šä¸ºä¸åŒäº‹ä»¶ (ç†ç”±: {reason})")
                
                # æ‰§è¡ŒåŒ¹é…å’Œæ›´æ–°
                result_topic = await self._match_and_update_topic(
                    db, t_name, t_desc, t_vec, existing_topic_obj, 
                    news_pool, pool_vecs, used_ids
                )
                
                if result_topic:
                    if existing_topic_obj:
                        updated_topics_count += 1
                    else:
                        new_topics_created += 1
                        # æ–°ä¸“é¢˜åŠ å…¥ active_topic_vecs ä»¥ä¾›åç»­ï¼ˆè™½ç„¶æœ¬è½® Phase 1 ä¸ä¼šå†å›å¤´ï¼Œä½†ä¸ºäº†é€»è¾‘å®Œæ•´ï¼‰
                        active_topic_vecs.append((result_topic, t_vec))
                        processed_topic_ids.add(result_topic.id)

            # === Phase 2: æ‰«æå…¶ä½™ç°æœ‰ä¸“é¢˜ ===
            logger.info("ğŸ” [Phase 2] æ‰«æå…¶ä½™ç°æœ‰ä¸“é¢˜ï¼Œå¯»æ‰¾æ½œåœ¨æ›´æ–°...")
            for existing_t, existing_vec in active_topic_vecs:
                if existing_t.id in processed_topic_ids:
                    continue
                
                # ä½¿ç”¨ç°æœ‰ä¸“é¢˜çš„ä¿¡æ¯è¿›è¡ŒåŒ¹é…
                # æ³¨æ„ï¼šç°æœ‰ä¸“é¢˜æ²¡æœ‰ t_desc å˜é‡ï¼Œä½¿ç”¨ summary æˆ– name
                logger.info(f"   Evaluating existing topic: {existing_t.name}")
                
                result_topic = await self._match_and_update_topic(
                    db, 
                    existing_t.name, 
                    existing_t.summary or existing_t.name, 
                    existing_vec, 
                    existing_t, 
                    news_pool, 
                    pool_vecs, 
                    used_ids
                )
                
                if result_topic:
                    updated_topics_count += 1
                    processed_topic_ids.add(result_topic.id)

            logger.info(f"âœ… ä¸“é¢˜åˆ·æ–°å®Œæˆï¼Œæ–°å»º {new_topics_created} ä¸ªï¼Œæ›´æ–° {updated_topics_count} ä¸ª")

            # æ˜¾å¼æ¸…ç†å¤§å¯¹è±¡ï¼Œå¸®åŠ© GC å›æ”¶
            del news_pool
            del pool_vecs
            del active_topics
            del active_topic_vecs
            import gc
            gc.collect()
            
    async def regenerate_topic_overview_action(self, db: AsyncSession, topic_id: int) -> Optional[str]:
        """
        æ‰‹åŠ¨è§¦å‘ï¼šé‡æ–°ç”Ÿæˆä¸“é¢˜ç»¼è¿°
        """
        topic = (await db.execute(select(Topic).where(Topic.id == topic_id))).scalar_one_or_none()
        if not topic:
            return None
            
        # è·å–è¯¥ä¸“é¢˜ä¸‹æ‰€æœ‰å…³è”çš„æ–°é—»
        all_items_stmt = (
            select(TopicTimelineItem)
            .where(TopicTimelineItem.topic_id == topic_id)
            .order_by(desc(TopicTimelineItem.event_time))
            .limit(50)
        )
        all_items = (await db.execute(all_items_stmt)).scalars().all()
        
        overview_materials = []
        for it in all_items:
            overview_materials.append({
                "title": it.news_title,
                "content": it.content or "" 
            })
            
        if not overview_materials:
            return "æš‚æ— ç›¸å…³æ–°é—»ï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°ã€‚"
            
        overview_text = await self.ai.generate_topic_overview(
            topic.name, 
            overview_materials
        )
        
        if overview_text:
            topic.record = overview_text
            # é¡ºä¾¿æ›´æ–° summary
            summary_prompt = (
                "è¯·æ ¹æ®ä»¥ä¸‹ä¸“é¢˜ç»¼è¿°ï¼Œæç‚¼ä¸€æ®µ **é«˜æµ“ç¼©çš„äº‹ä»¶æ¦‚è§ˆ**ï¼ˆ100-150å­—ï¼‰ã€‚\n"
                "è¦æ±‚ï¼š\n"
                "1. åŒ…å«äº‹ä»¶çš„æ ¸å¿ƒå†²çªï¼ˆWho did Whatï¼‰ã€‚\n"
                "2. åŒ…å«å…³é”®çš„èƒŒæ™¯ä¿¡æ¯ï¼ˆå¦‚æ¶‰åŠé‡‘é¢ã€ç‰©å“åç§°ï¼‰ã€‚\n"
                "3. åŒ…å«å½“å‰çš„æœ€æ–°çŠ¶æ€ã€‚\n"
                "4. çº¯æ–‡æœ¬ï¼Œæ— Markdownã€‚\n\n"
                f"{overview_text[:2000]}"
            )
            new_summary = await self.ai.chat_completion(summary_prompt)
            if new_summary:
                topic.summary = new_summary.replace("```", "").strip()
            
            db.add(topic)
            await db.commit()
            
        return overview_text

    async def _match_and_update_topic(
        self,
        db: AsyncSession,
        t_name: str,
        t_desc: str,
        t_vec: List[float],
        existing_topic_obj: Optional[Topic],
        news_pool: List[News],
        pool_vecs: Dict[int, List[float]],
        used_ids: Set[int]
    ) -> Optional[Topic]:
        """
        æ ¸å¿ƒé€»è¾‘ï¼šæ ¹æ®ä¸“é¢˜ä¿¡æ¯ï¼ˆåç§°ã€æè¿°ã€å‘é‡ï¼‰ï¼Œåœ¨ news_pool ä¸­å¯»æ‰¾åŒ¹é…æ–°é—»ï¼Œ
        ç» AI æ ¸éªŒåï¼Œåˆ›å»ºæ–°ä¸“é¢˜æˆ–æ›´æ–°æ—§ä¸“é¢˜ã€‚
        """
        is_duplicate = (existing_topic_obj is not None)
        
        # 1. å‘é‡åˆç­›å€™é€‰æ–°é—»
        candidates = []
        for n in news_pool:
            # è·³è¿‡å·²ç»åœ¨å½“å‰è½®æ¬¡å¤„ç†è¿‡çš„æ–°é—»
            if n.id in used_ids:
                continue
                
            n_vec = pool_vecs.get(n.id)
            if not n_vec:
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            sim = self._cosine_similarity(t_vec, n_vec)
            
            if sim > settings.TOPIC_MATCH_THRESHOLD: # åˆç­›é˜ˆå€¼
                candidates.append((n, sim))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        candidates.sort(key=lambda x: x[1], reverse=True)
        # å–å‰ 20 ä¸ªç»™ AI æ ¸éªŒ
        candidates = candidates[:settings.TOPIC_MATCH_MAX_CANDIDATES]
        
        # å¦‚æœæ˜¯æ–°ä¸“é¢˜ï¼Œä¸”å€™é€‰ä¸è¶³ï¼Œåˆ™è·³è¿‡ï¼›å¦‚æœæ˜¯åˆå¹¶æ—§ä¸“é¢˜ï¼Œå€™é€‰ä¸è¶³ä¹Ÿæ— å¦¨ï¼ˆåªæ˜¯æœ¬æ¬¡æ²¡æ›´æ–°ï¼‰
        if not is_duplicate and len(candidates) <= settings.TOPIC_MIN_NEWS_COUNT:
            logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] åˆç­›å€™é€‰æ–°é—»ä¸è¶³ ({len(candidates)} <= {settings.TOPIC_MIN_NEWS_COUNT})ï¼Œè·³è¿‡")
            return None
        
        if is_duplicate and not candidates:
            logger.info(f"   âš ï¸ [æ—§ä¸“é¢˜åˆå¹¶] æ— å€™é€‰æ–°é—»ï¼Œè·³è¿‡")
            return None

        # 2. AI æ‰¹é‡æ ¸éªŒ
        verify_tasks = []
        for n, sim in candidates:
            verify_tasks.append({
                "topic_name": t_name,
                "topic_summary": t_desc, # è¿™é‡Œç”¨ summary å­—æ®µä¼ é€’ description
                "news_title": n.title,
                "news_summary": n.summary or (n.content or "")[:200] or ""
            })
        
        verified_results = await self.ai.verify_topic_match_batch(verify_tasks)
        
        confirmed_news = []
        for idx, (is_match, reason) in enumerate(verified_results):
            if is_match:
                logger.info(f"   âœ… [Match] {candidates[idx][0].title[:30]}... (Reason: {reason})")
                confirmed_news.append(candidates[idx][0])
            else:
                # Optional: Log mismatch if verbose
                logger.info(f"   âŒ [Mismatch] {candidates[idx][0].title[:30]}... (Reason: {reason})")

        # === è§„åˆ™è°ƒæ•´ï¼šå¯¹äºå·²æœ‰ä¸“é¢˜ï¼Œä»…æ›´æ–°â€œä»Šæ—¥â€çš„æ–°é—» ===
        if is_duplicate:
            today_date = datetime.now().date()
            today_news = []
            for n in confirmed_news:
                # å‡è®¾ publish_date ä¸ºç©ºåˆ™è§†ä¸ºéä»Šæ—¥ï¼ˆæˆ–ä¿ç•™ï¼Ÿé€šå¸¸çˆ¬è™«æ•°æ®åº”æœ‰æ—¶é—´ï¼‰
                if n.publish_date and n.publish_date.date() == today_date:
                    today_news.append(n)
            
            if not today_news:
                logger.info(f"   â© [æ—§ä¸“é¢˜] ç»æ—¥æœŸè¿‡æ»¤åæ— ä»Šæ—¥æ–°é—»ï¼Œè·³è¿‡æ›´æ–°")
                return None
            
            if len(today_news) < len(confirmed_news):
                logger.info(f"   ğŸ—“ï¸ [Date Filter] è¿‡æ»¤éä»Šæ—¥æ–°é—»ï¼Œå‰©ä½™ {len(today_news)}/{len(confirmed_news)} æ¡")
            
            confirmed_news = today_news

        # å†æ¬¡æ£€æŸ¥æ•°é‡é™åˆ¶
        # æ–°ä¸“é¢˜ï¼šå¿…é¡»æ»¡è¶³æœ€å°æ•°é‡é™åˆ¶
        # ç”¨æˆ·è¦æ±‚ï¼šåª’ä½“æŠ¥é“ >= 3 (å³ count >= 3) => count < 3 åˆ™è·³è¿‡
        if not is_duplicate and len(confirmed_news) < settings.TOPIC_MIN_NEWS_COUNT:
            logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] AI æ ¸éªŒé€šè¿‡æ•°é‡ä¸è¶³ ({len(confirmed_news)} < {settings.TOPIC_MIN_NEWS_COUNT})ï¼Œè·³è¿‡")
            return None
            
        # æ£€æŸ¥çƒ­åº¦æŒ‡æ ‡ (ç”¨æˆ·è¦æ±‚: çƒ­åº¦ > 6)
        # è®¡ç®—çƒ­åº¦ï¼ˆå–æ–°é—»æœ€å¤§çƒ­åº¦ï¼‰
        max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
        if not is_duplicate and max_heat <= 6:
             logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] çƒ­åº¦ä¸è¶³ ({max_heat} <= 4)ï¼Œè·³è¿‡")
             return None
        
        # æ—§ä¸“é¢˜ï¼šä¸é™åˆ¶æœ€å°æ•°é‡ï¼Œåªè¦æœ‰æ–°çš„å°±åˆå¹¶
        if is_duplicate and not confirmed_news:
            return None
        
        # 3. åˆ›å»ºæˆ–æ›´æ–°ä¸“é¢˜
        current_topic_id = None
        topic_obj_to_return = None

        if is_duplicate:
            logger.info(f"   ğŸ”„ æ›´æ–°æ—§ä¸“é¢˜: {existing_topic_obj.name} (æ–°å¢ {len(confirmed_news)} æ¡æ–°é—»)")
            # æ›´æ–°æ—§ä¸“é¢˜çš„ update_time å’Œ heat_score
            max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
            current_max = existing_topic_obj.heat_score or 0
            if max_heat > current_max:
                existing_topic_obj.heat_score = max_heat
            
            new_end_time = max([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else None
            if new_end_time and (not existing_topic_obj.updated_time or new_end_time > existing_topic_obj.updated_time):
                existing_topic_obj.updated_time = new_end_time
            
            current_topic_id = existing_topic_obj.id
            topic_obj_to_return = existing_topic_obj
        else:
            logger.info(f"   âœ¨ åˆ›å»ºæ–°ä¸“é¢˜: {t_name} (åŒ…å« {len(confirmed_news)} æ¡æ–°é—»)")
            
            # è®¡ç®—çƒ­åº¦ï¼ˆå–æ–°é—»æœ€å¤§çƒ­åº¦ï¼‰
            max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
            # æœ€æ—©æ—¶é—´
            start_time = min([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else datetime.now()
            # æœ€æ–°æ—¶é—´
            end_time = max([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else datetime.now()

            new_topic = Topic(
                name=t_name,
                summary=t_desc,
                start_time=start_time,
                updated_time=end_time,
                heat_score=max_heat,
                embedding=t_vec,
                status="active"
            )
            db.add(new_topic)
            await db.flush()
            current_topic_id = new_topic.id
            topic_obj_to_return = new_topic
        
        # 4. è¡¥å…¨æ–°é—»è¯¦æƒ…ä¸ç”Ÿæˆæ—¶é—´è½´
        # 4.1 å…ˆæ£€æŸ¥å¹¶è¡¥å…¨æ–°é—»è¯¦æƒ…
        async with crawler_service.make_crawler() as crawler:
            for n in confirmed_news:
                if not n.content or len(n.content) < 100:
                    logger.info(f"   ğŸ“¥ æ­£åœ¨è¡¥å…¨æ–°é—»è¯¦æƒ…: {n.title[:20]}...")
                    try:
                        crawled = await crawler_service.crawl_content_with_instance(n.url, crawler)
                        if crawled and len(crawled) > 50:
                            n.content = crawled
                            # å†…å®¹æ›´æ–°äº†ï¼Œæ‘˜è¦æœ€å¥½ä¹Ÿåˆ·æ–°ä¸€ä¸‹ï¼Œå¦åˆ™æ—§æ‘˜è¦å¯èƒ½ä¸å‡†
                            fresh_summary = await self.ai.generate_summary(n.title, n.content, max_words=200)
                            if fresh_summary:
                                n.summary = fresh_summary
                            db.add(n)
                    except Exception as e:
                        logger.warning(f"   âš ï¸ è¡¥å…¨è¯¦æƒ…å¤±è´¥: {e}")
        
        await db.flush()

        # 4.2 ç”Ÿæˆæ ‡å‡†åŒ–çš„æ—¶é—´è½´å†…å®¹ (æŒ‰å¤©èšåˆ + AI åˆæˆ)
        # å°† confirmed_news æŒ‰æ—¥æœŸåˆ†ç»„
        news_by_date = defaultdict(list)
        for n in confirmed_news:
            d_str = (n.publish_date or datetime.now()).strftime("%Y-%m-%d")
            news_by_date[d_str].append({
                "id": n.id,
                "title": n.title,
                "summary": n.summary or (n.content or "")[:200],
                "source": n.source,
                "url": n.url,
                "publish_date": n.publish_date  # Added for precise time
            })
        
        # éå†æ¯ä¸€å¤©ï¼Œè°ƒç”¨ AI åˆæˆäº‹ä»¶
        current_topic_name = topic_obj_to_return.name if topic_obj_to_return else None
        
        for d_str, day_news in news_by_date.items():
            # 1. è·å–è¯¥å¤©å·²æœ‰çš„æ—¶é—´è½´èŠ‚ç‚¹ï¼ˆä¸ºäº†åˆå¹¶æ›´æ–°ï¼‰
            # æ³¨æ„ï¼šsqlite/pg å…¼å®¹æ€§ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ event_time å­˜çš„æ˜¯ datetime
            target_date = datetime.strptime(d_str, "%Y-%m-%d").date()
            
            # æ„é€ æŸ¥è¯¢èŒƒå›´ï¼šå½“å¤© 00:00:00 åˆ° 23:59:59
            day_start = datetime.combine(target_date, datetime.min.time())
            day_end = datetime.combine(target_date, datetime.max.time())
            
            existing_items_stmt = (
                select(TopicTimelineItem)
                .where(TopicTimelineItem.topic_id == current_topic_id)
                .where(TopicTimelineItem.event_time >= day_start)
                .where(TopicTimelineItem.event_time <= day_end)
            )
            existing_items = (await db.execute(existing_items_stmt)).scalars().all()
            
            # 2. æ”¶é›†è¯¥å¤©æ‰€æœ‰ç›¸å…³çš„æ–°é—» ID (æ—§ + æ–°)
            all_news_ids = set()
            for n in day_news:
                all_news_ids.add(n["id"])
            
            for it in existing_items:
                if it.news_id:
                    all_news_ids.add(it.news_id)
                if it.sources:
                    for s in it.sources:
                        if isinstance(s, dict) and s.get("id"):
                            all_news_ids.add(s["id"])
                            
            # 3. å¦‚æœæœ‰æ—§èŠ‚ç‚¹ï¼Œéœ€è¦é‡æ–°æ‹‰å–æ‰€æœ‰ç›¸å…³æ–°é—»çš„è¯¦æƒ…ï¼Œè¿›è¡Œå…¨é‡é‡ç”Ÿæˆ
            # å¦‚æœæ²¡æœ‰æ—§èŠ‚ç‚¹ï¼Œç›´æ¥ç”¨ day_news å³å¯
            final_news_list = []
            
            if existing_items:
                # æ‹‰å–æ‰€æœ‰æ¶‰åŠçš„æ–°é—»å¯¹è±¡
                news_stmt = select(News).where(News.id.in_(list(all_news_ids)))
                all_news_objs = (await db.execute(news_stmt)).scalars().all()
                
                for n in all_news_objs:
                    final_news_list.append({
                        "id": n.id,
                        "title": n.title,
                        "summary": n.summary or (n.content or "")[:200],
                        "source": n.source,
                        "url": n.url,
                        "publish_date": n.publish_date
                    })
            else:
                final_news_list = day_news

            # 4. è°ƒç”¨ AI åˆæˆï¼ˆå…¨é‡ï¼‰
            logger.info(f"   ğŸ”„ æ­£åœ¨é‡ç”Ÿæˆ {d_str} çš„æ—¶é—´è½´ (åŸºäº {len(final_news_list)} æ¡æ–°é—»)...")
            day_events = await self.ai.generate_daily_timeline_events(d_str, final_news_list, topic_name=current_topic_name)
            
            # ç¡¬æ€§è§„åˆ™ï¼šæ¯å¤©æœ€å¤šä¿ç•™ 2 ä¸ªèŠ‚ç‚¹
            if day_events and len(day_events) > 2:
                logger.info(f"   âš ï¸ [Rule] AI ç”Ÿæˆäº† {len(day_events)} ä¸ªèŠ‚ç‚¹ï¼Œå¼ºåˆ¶æˆªå–å‰ 2 ä¸ª")
                day_events = day_events[:2]

            # å¦‚æœ AI æ²¡æœ‰ç”Ÿæˆä»»ä½•äº‹ä»¶ï¼ˆå¤±è´¥æˆ–ä¸ºç©ºï¼‰ï¼Œåˆ™é™çº§å¤„ç†ï¼šé€‰æœ€é‡è¦çš„ 1-2 æ¡ä½œä¸ºä»£è¡¨
            if not day_events:
                logger.warning(f"   âš ï¸ {d_str} AI åˆæˆäº‹ä»¶å¤±è´¥ï¼Œé™çº§ä¸ºä½¿ç”¨ Top æ–°é—»")
                # æŒ‰ publish_date æ’åºï¼Œå–æœ€æ–°çš„
                final_news_list.sort(key=lambda x: x.get("publish_date") or datetime.min, reverse=True)
                # ç®€å•å–å‰ 2 æ¡
                for n_item in final_news_list[:2]:
                    day_events.append({
                        "content": n_item["summary"] or n_item["title"],
                        "source_ids": [n_item["id"]]
                    })

            # 5. åˆ é™¤æ—§èŠ‚ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå†™å…¥æ–°èŠ‚ç‚¹
            if existing_items:
                for old_it in existing_items:
                    await db.delete(old_it)
                await db.flush() # ç«‹å³æ‰§è¡Œåˆ é™¤

            # å…¥åº“ Timeline Items
            for event in day_events:
                content = event.get("content")
                if not content:
                    continue
                
                source_ids = event.get("source_ids", [])
                
                # æ„å»º sources åˆ—è¡¨
                sources_data = []
                # æ‰¾å‡ºå¯¹åº”çš„ news item info
                primary_news = None
                
                for nid in source_ids:
                    # åœ¨ final_news_list ä¸­æŸ¥æ‰¾
                    found = next((x for x in final_news_list if x["id"] == nid), None)
                    if found:
                        sources_data.append({
                            "id": found["id"],
                            "name": found["source"] or "æœªçŸ¥æ¥æº",
                            "url": found["url"],
                            "title": found["title"]
                        })
                        if not primary_news:
                            primary_news = found
                
                # å¦‚æœ source_ids ä¸ºç©ºæˆ–æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…œåº•ï¼ˆè™½ç„¶ä¸åº”è¯¥å‘ç”Ÿï¼‰
                if not primary_news and final_news_list:
                     primary_news = final_news_list[0]

                # Determine event time from primary news if available
                event_time = datetime.strptime(d_str, "%Y-%m-%d")
                if primary_news and primary_news.get("publish_date"):
                    event_time = primary_news["publish_date"]

                # åˆ›å»º item
                item = TopicTimelineItem(
                    topic_id=current_topic_id,
                    event_time=event_time,
                    content=content,
                    # å…¼å®¹æ—§å­—æ®µï¼Œå­˜å‚¨ä¸»è¦æ¥æº
                    news_id=primary_news["id"] if primary_news else None,
                    news_title=primary_news["title"] if primary_news else None,
                    source_name=primary_news["source"] if primary_news else None,
                    source_url=primary_news["url"] if primary_news else None,
                    # æ–°å­—æ®µï¼šå¤šæ¥æº
                    sources=sources_data
                )
                db.add(item)
                
                # æ ‡è®° used_ids
                for nid in source_ids:
                    used_ids.add(nid)

        await db.flush() # ç¡®ä¿ item å…¥åº“

        # 6. ç”Ÿæˆ/æ›´æ–°ä¸“é¢˜ç»¼è¿° (Overview) & ç®€è¦æè¿° (Summary)
        # è·å–è¯¥ä¸“é¢˜ä¸‹æ‰€æœ‰å…³è”çš„æ–°é—»ï¼ˆä¸ºäº†ç”Ÿæˆå…¨é¢çš„ç»¼è¿°ï¼‰
        # é™åˆ¶æ•°é‡ï¼Œå–çƒ­åº¦æœ€é«˜çš„ 50 æ¡
        all_items_stmt = (
            select(TopicTimelineItem)
            .where(TopicTimelineItem.topic_id == current_topic_id)
            .order_by(desc(TopicTimelineItem.event_time))
            .limit(50)
        )
        all_items = (await db.execute(all_items_stmt)).scalars().all()
        
        # æ”¶é›†ç”¨äºç”Ÿæˆç»¼è¿°çš„ç´ æ
        overview_materials = []
        for it in all_items:
            overview_materials.append({
                "title": it.news_title,
                "content": it.content or "" # ä½¿ç”¨ timeline çš„ AI æ‘˜è¦ä½œä¸ºç´ ææ›´å¥½
            })
        
        if overview_materials:
            # 1. ç”Ÿæˆå¤šç»´åº¦ç»¼è¿°
            # æ³¨æ„ï¼šå¦‚æœæ˜¯ Existing Topicï¼Œåå­—å¯èƒ½å’Œ t_name ä¸å®Œå…¨ä¸€æ ·ï¼ˆå¦‚æœæ˜¯ Phase 2ï¼‰ï¼Œä½†é€šå¸¸ Phase 2 ä¼ å…¥çš„ t_name å°±æ˜¯ existing.name
            target_name = existing_topic_obj.name if existing_topic_obj else t_name
            
            overview_text = await self.ai.generate_topic_overview(
                target_name, 
                overview_materials
            )
            
            # 2. æ›´æ–° summary (ç®€è¦æè¿°)
            if overview_text:
                new_summary = None
                # ä¸ºäº†èŠ‚çœ tokenï¼Œç›´æ¥è®© AI åŸºäº overview_text ç”Ÿæˆ summary
                summary_prompt = (
                    "è¯·æ ¹æ®ä»¥ä¸‹ä¸“é¢˜ç»¼è¿°ï¼Œæç‚¼ä¸€æ®µ **é«˜æµ“ç¼©çš„äº‹ä»¶æ¦‚è§ˆ**ï¼ˆ100-150å­—ï¼‰ã€‚\n"
                    "è¦æ±‚ï¼š\n"
                    "1. åŒ…å«äº‹ä»¶çš„æ ¸å¿ƒå†²çªï¼ˆWho did Whatï¼‰ã€‚\n"
                    "2. åŒ…å«å…³é”®çš„èƒŒæ™¯ä¿¡æ¯ï¼ˆå¦‚æ¶‰åŠé‡‘é¢ã€ç‰©å“åç§°ï¼‰ã€‚\n"
                    "3. åŒ…å«å½“å‰çš„æœ€æ–°çŠ¶æ€ã€‚\n"
                    "4. çº¯æ–‡æœ¬ï¼Œæ— Markdownã€‚\n"
                    "5. **ç›´æ¥è¾“å‡º**ï¼šä¸è¦åŒ…å«ä»»ä½•â€œå¥½çš„â€ã€â€œæ ¹æ®æ‚¨çš„è¦æ±‚â€ç­‰å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ã€‚\n\n"
                    f"{overview_text[:2000]}"
                )
                new_summary = await self.ai.chat_completion(summary_prompt, route_key="TOPIC_OVERVIEW")
                
                # æ›´æ–° Topic
                topic_to_update = existing_topic_obj if is_duplicate else topic_obj_to_return
                topic_to_update.record = overview_text
                if new_summary:
                    topic_to_update.summary = new_summary.replace("```", "").strip()
                
                db.add(topic_to_update)
            else:
                logger.warning(f"   âš ï¸ ä¸“é¢˜ç»¼è¿°ç”Ÿæˆå¤±è´¥ (None)ï¼Œè·³è¿‡ Summary æ›´æ–°")

        await db.commit()
        return topic_obj_to_return
            
    async def scheduled_topic_task(self) -> None:
        """
        Scheduled entry point.
        This runs independently if configured, but now we prefer pipeline orchestration.
        We can keep it but maybe it should just call refresh_topics.
        """
        logger.info("â° ä¸“é¢˜è¿½è¸ªå®šæ—¶ä»»åŠ¡å¯åŠ¨...")
        while True:
            try:
                if not await check_db_connection():
                    logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥å¼‚å¸¸ï¼Œä¸“é¢˜è¿½è¸ªä»»åŠ¡æš‚åœè¿è¡Œï¼Œç­‰å¾…æ¢å¤...")
                    await asyncio.sleep(60)
                    continue

                if not (settings.DATABASE_URL or "").strip():
                    logger.warning("âš ï¸ æœªé…ç½® DATABASE_URLï¼Œä¸“é¢˜è¿½è¸ªä»»åŠ¡æš‚åœè¿è¡Œ")
                    await asyncio.sleep(60)
                    continue

                # Run every 4 hours or similar
                # But user wants it after summary generation.
                # So this might be just a backup or manual trigger handler
                await asyncio.sleep(4 * 3600) 
                await self.refresh_topics()
            except AIConfigurationError as e:
                logger.error(f"ğŸ›‘ é…ç½®é”™è¯¯: {e} è¯·æ£€æŸ¥ config.yaml æ˜¯å¦é…ç½®æ­£ç¡®")
                logger.warning("âš ï¸ ä¸“é¢˜è¿½è¸ªä»»åŠ¡è¿›å…¥ç»´æŠ¤æ¨¡å¼ï¼Œæ¯ 5 åˆ†é’Ÿå°è¯•é‡å¯æœåŠ¡æ£€æŸ¥ä¸€æ¬¡...")
                await asyncio.sleep(300)
            except Exception as e:
                logger.error(f"Scheduled topic task error: {e}")
                await asyncio.sleep(300)

    # Helper methods
    async def _ensure_news_embeddings_batch(self, db: AsyncSession, news_list: List[News]) -> Dict[int, List[float]]:
        out = {}
        to_embed_indices = []
        texts = []
        
        for idx, n in enumerate(news_list):
            if n.embedding and len(n.embedding) > 0:
                out[n.id] = list(n.embedding)
            else:
                txt = " ".join([n.title or "", n.summary or "", (n.content or "")[:500]]).strip()
                texts.append(txt[:1000] if txt else (n.title or "")[:1000])
                to_embed_indices.append(idx)
        
        if texts:
            # Batch embedding call (chunking if needed)
            batch_size = 10
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i : i + batch_size]
                batch_indices = to_embed_indices[i : i + batch_size]
                try:
                    embs = await self.ai.get_embeddings(batch_texts)
                    for local_idx, emb in enumerate(embs):
                        original_idx = batch_indices[local_idx]
                        n = news_list[original_idx]
                        if emb:
                            n.embedding = emb
                            db.add(n)
                            out[n.id] = emb
                except Exception as e:
                    logger.error(f"   âš ï¸ æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")
            
            await db.flush()
        return out

    async def _ensure_topic_embeddings(self, db: AsyncSession, topics: List[Topic]) -> List[Tuple[Topic, List[float]]]:
        out = []
        to_embed = []
        for idx, t in enumerate(topics):
            if t.embedding and len(t.embedding) > 0:
                out.append((t, list(t.embedding)))
            else:
                txt = f"{t.name} {t.summary}"
                to_embed.append((idx, txt[:1000]))
                out.append((t, []))
        
        if to_embed:
            texts = [x[1] for x in to_embed]
            try:
                embs = await self.ai.get_embeddings(texts)
                for (idx, _), vec in zip(to_embed, embs):
                    if vec:
                        t = topics[idx]
                        t.embedding = vec
                        db.add(t)
                        out[idx] = (t, vec) # Update the tuple in out list
            except Exception as e:
                 logger.error(f"   âš ï¸ ä¸“é¢˜å‘é‡åŒ–å¤±è´¥: {e}")
            await db.flush()
        return out

    async def _ensure_news_summary(self, db: AsyncSession, news: News) -> None:
        if (news.summary or "").strip():
            return

        # Try to crawl content if missing
        if not news.content or len(news.content) < 50:
             try:
                content = await crawler_service.crawl_content(news.url)
                if content:
                    news.content = content
             except Exception:
                 pass
        
        content = news.content or ""
        if len(content) < 50:
            return # Too short to summarize
            
        try:
            summary = await self.ai.generate_summary(news.title, content, max_words=200)
            if summary:
                news.summary = summary
                db.add(news)
        except Exception:
            pass

# Global instance
from app.services.ai_service import ai_service
topic_service = TopicService(ai=ai_service)

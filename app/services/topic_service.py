# app/services/topic_service.py

from __future__ import annotations

import asyncio
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple

from collections import defaultdict
import numpy as np
from sqlalchemy import desc, select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import get_settings
from app.core.database import AsyncSessionLocal, check_db_connection
from app.core.logger import setup_logger
from app.core.exceptions import AIConfigurationError
from app.core.prompts import prompt_manager
from app.models.news import News
from app.models.topic import Topic, TopicTimelineItem
from app.services.ai_service import AIService
from app.services.crawler_service import crawler_service
from app.utils.tools import clean_html_tags

settings = get_settings()
logger = setup_logger("TopicService")


class TopicService:
    def __init__(self, ai: AIService) -> None:
        self.ai = ai

    @staticmethod
    def _cosine_similarity(a: List[float], b: List[float]) -> float:
        if not a or not b:
            return 0.0
        va = np.array(a, dtype=np.float32)
        vb = np.array(b, dtype=np.float32)
        na = float(np.linalg.norm(va))
        nb = float(np.linalg.norm(vb))
        if na <= 0 or nb <= 0:
            return 0.0
        return float(np.dot(va, vb) / (na * nb))

    def _find_candidate_news_by_vector(
        self,
        t_vec: List[float],
        news_pool: List[News],
        pool_vecs: Dict[int, List[float]],
        used_ids: Set[int],
        top_k: int = 10,
        threshold: float = 0.35
    ) -> List[Tuple[News, float]]:
        """
        æ ¹æ®å‘é‡ç›¸ä¼¼åº¦æŸ¥æ‰¾å€™é€‰æ–°é—» (ä¸æ‰§è¡Œ DB æ“ä½œï¼Œä¸è¿›è¡Œ AI äºŒæ¬¡æ ¸éªŒ)
        """
        candidates = []
        for n in news_pool:
            if n.id in used_ids:
                continue
            n_vec = pool_vecs.get(n.id)
            if not n_vec:
                continue
            
            sim = self._cosine_similarity(t_vec, n_vec)
            if sim > threshold:
                candidates.append((n, sim))
        
        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates[:top_k]

    async def refresh_topics(self) -> None:
        """
        ä¸“é¢˜è¿½è¸ªé€»è¾‘ï¼š
        1. æ‰¾å‡ºæœªå½’ç±»çš„æ–°é—»ï¼ˆNå¤©å†…ï¼‰ã€‚
        2. èšåˆæ ‡é¢˜è®© AI æç‚¼ä¸“é¢˜ã€‚
        3. å¯¹æç‚¼çš„ä¸“é¢˜è¿›è¡Œå‘é‡åŒ¹é…+AIæ ¸éªŒã€‚
        4. åªæœ‰æ–°é—»æ•° > 3 çš„ä¸“é¢˜æ‰åˆ›å»ºã€‚
        5. è¡¥å…¨è¯¦æƒ…ã€‚
        """
        if not (settings.DATABASE_URL or "").strip():
            return

        async with AsyncSessionLocal() as db:
            # 1. è·å–å·²å½’ç±»çš„æ–°é—»IDé›†åˆ
            # åŒæ—¶è·å– news_id å’Œ sources ä¸­çš„ ID
            used_stmt = select(TopicTimelineItem.news_id, TopicTimelineItem.sources)
            used_res = await db.execute(used_stmt)
            
            used_ids = set()
            for nid, srcs in used_res:
                if nid:
                    used_ids.add(nid)
                if srcs and isinstance(srcs, list):
                    for src in srcs:
                        if isinstance(src, dict) and "id" in src:
                            try:
                                used_ids.add(int(src["id"]))
                            except (ValueError, TypeError):
                                pass
            
            # 2. è·å–å€™é€‰æ–°é—»æ± ï¼ˆNå¤©å†…ï¼Œæœªå½’ç±»ï¼‰
            days = settings.TOPIC_LOOKBACK_DAYS
            start_date = datetime.now() - timedelta(days=days)
            
            # å…ˆæŸ¥æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œç”¨äºåç»­å‘é‡åŒ¹é…
            # é™åˆ¶æ•°é‡é˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼Œæ¯”å¦‚å–æœ€è¿‘ 2000 æ¡
            pool_stmt = (
                select(News)
                .where(News.publish_date >= start_date)
                .where(News.id.notin_(used_ids) if used_ids else True)
                .order_by(desc(News.heat_score))
                .limit(settings.TOPIC_RECALL_POOL_SIZE)
            )
            news_pool = (await db.execute(pool_stmt)).scalars().all()
            
            if not news_pool:
                logger.info("ğŸ“­ æ²¡æœ‰å¾…å¤„ç†çš„æ–°é—»ï¼Œè·³è¿‡ä¸“é¢˜ç”Ÿæˆ")
                return
                
            logger.info(f"ğŸ“Š å¾…å¤„ç†æ–°é—»æ± å¤§å°: {len(news_pool)}")
            
            # ç¡®ä¿æ± ä¸­æ–°é—»æœ‰å‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            pool_vecs = await self._ensure_news_embeddings_batch(db, news_pool)

            # 3. å‡†å¤‡ AI æç‚¼çš„ç§å­æ ‡é¢˜ï¼ˆTop 300ï¼‰
            # news_pool å·²ç»æ˜¯æŒ‰ heat_score æ’åºçš„
            # è¿‡æ»¤æ‰ä½çƒ­åº¦æ–°é—»
            min_heat = settings.TOPIC_NEWS_MIN_HEAT
            seed_news = [n for n in news_pool if (n.heat_score or 0) >= min_heat][:settings.TOPIC_AGGREGATION_TOP_N]
            
            if not seed_news:
                logger.info(f"ğŸ“­ ç»çƒ­åº¦è¿‡æ»¤(>{min_heat})åï¼Œæ— ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œè·³è¿‡ä¸“é¢˜ç”Ÿæˆ")
                return

            # æ ¼å¼åŒ–æ ‡é¢˜ï¼Œå¸¦ä¸Šçƒ­åº¦ä¿¡æ¯
            seed_titles = [f"[çƒ­åº¦:{float(n.heat_score or 0):.1f}] {(n.title or '').strip()}" for n in seed_news if (n.title or "").strip()]
            
            # 4. AI æç‚¼ä¸“é¢˜
            proposed_topics = await self.ai.propose_topics_from_titles(seed_titles)
            if not proposed_topics:
                logger.info("âš ï¸ AI æœªæç‚¼å‡ºä»»ä½•ä¸“é¢˜")
                proposed_topics = []

            # è·å–ç°æœ‰çš„ Active ä¸“é¢˜ï¼Œç”¨äºæŸ¥é‡å’Œå»¶ä¼¸åˆ¤æ–­
            active_topics_stmt = select(Topic).where(Topic.status == "active")
            active_topics = (await db.execute(active_topics_stmt)).scalars().all()

            # 4.1 ä¼˜åŒ–ï¼šè·³è¿‡åŸºäºåˆå§‹æè¿°çš„åˆæ­¥è¿‡æ»¤
            # ç†ç”±ï¼šåˆå§‹æè¿°æ˜¯ AI åŸºäºæ ‡é¢˜ç”Ÿæˆçš„ï¼Œå¯èƒ½å­˜åœ¨å¹»è§‰ã€‚ä¸å…¶æµªè´¹ Token è¯„ä¼°å¹»è§‰ï¼Œ
            # ä¸å¦‚ç›´æ¥é€šè¿‡å‘é‡æœç´¢çœ‹æ˜¯å¦æœ‰çœŸå®æ–°é—»æ”¯æ’‘ã€‚å¦‚æœå‘é‡æœä¸åˆ°ï¼Œè‡ªç„¶ä¼šè¢«åç»­é€»è¾‘æ·˜æ±°ã€‚
            # existing_topics_data = [...] # ç§»è‡³éœ€è¦æ—¶å†æ„å»º
            
            # ç¡®ä¿ç°æœ‰ä¸“é¢˜æœ‰å‘é‡
            active_topic_vecs = await self._ensure_topic_embeddings(db, active_topics)

            # 5. å¤„ç†æ¯ä¸ªæç‚¼å‡ºçš„ä¸“é¢˜
            new_topics_created = 0
            updated_topics_count = 0
            
            # è®°å½•æœ¬è½®å·²å¤„ç†ï¼ˆåˆ›å»ºæˆ–æ›´æ–°ï¼‰çš„ä¸“é¢˜ID
            processed_topic_ids = set()

            # å‡†å¤‡å…³æ³¨å…³é”®è¯å‘é‡ (å¦‚æœæœ‰é…ç½®)
            follow_keywords = settings.FOLLOW_KEYWORDS
            keyword_vecs = []
            if follow_keywords:
                kw_list = [k.strip() for k in follow_keywords.split(",") if k.strip()]
                if kw_list:
                    logger.info(f"ğŸ” [Topic Filter] å¯ç”¨å…³é”®è¯è¿‡æ»¤: {kw_list}")
                    kw_embs = await self.ai.get_embeddings(kw_list)
                    keyword_vecs = [v for v in kw_embs if v]
            
            # === Phase 1: å¤„ç† AI æç‚¼çš„æ½œåœ¨ä¸“é¢˜ ===
            for p_topic in proposed_topics:
                t_name = p_topic.get("name", "")
                t_desc = p_topic.get("description", "") # åˆå§‹æè¿°
                
                if not t_name:
                    continue
                    
                logger.info(f"ğŸ” [Phase 1] æ­£åœ¨è¯„ä¼°æç‚¼ä¸“é¢˜: {t_name}")
                
                # è®¡ç®—è¯¥æ½œåœ¨ä¸“é¢˜çš„å‘é‡
                t_txt = f"{t_name} {t_desc}"
                t_embs = await self.ai.get_embeddings([t_txt])
                t_vec = t_embs[0] if t_embs and t_embs[0] else []
                
                if not t_vec:
                    logger.warning(f"   âš ï¸ æ— æ³•ç”Ÿæˆå‘é‡: {t_name}")
                    continue

                # 5.0 å…³é”®è¯è¿‡æ»¤
                if keyword_vecs:
                    max_sim = max([self._cosine_similarity(t_vec, kv) for kv in keyword_vecs]) if keyword_vecs else 0
                    if max_sim < settings.FOLLOW_KEYWORDS_THRESHOLD:
                        logger.info(f"   â© ä¸“é¢˜ '{t_name}' ä¸å…³æ³¨å…³é”®è¯ç›¸å…³åº¦ä¸è¶³ ({max_sim:.2f} < {settings.FOLLOW_KEYWORDS_THRESHOLD})ï¼Œè·³è¿‡")
                        continue
                
                # --- æ–°å¢æ­¥éª¤ï¼šåŸºäºå‘é‡é¢„å…ˆæŸ¥æ‰¾å…³è”æ–°é—»ï¼Œå¹¶ç”ŸæˆçœŸå®æ‘˜è¦ ---
                # 1. é¢„æŸ¥æ‰¾å€™é€‰æ–°é—» (Vector Search Only)
                pre_candidates = self._find_candidate_news_by_vector(
                    t_vec, news_pool, pool_vecs, used_ids, top_k=10, threshold=0.35
                )
                
                # å¦‚æœå€™é€‰æ–°é—»å¤ªå°‘ï¼Œè¯´æ˜å¯èƒ½æ˜¯å¹»è§‰æˆ–æ— å®è¯çš„ä¸“é¢˜ï¼Œç›´æ¥è·³è¿‡
                if len(pre_candidates) < 2:
                    logger.info(f"   â© ä¸“é¢˜ '{t_name}' é¢„æŸ¥æ‰¾å€™é€‰æ–°é—»ä¸è¶³ ({len(pre_candidates)} < 2)ï¼Œè·³è¿‡")
                    continue
                
                # 2. ç”ŸæˆçœŸå®æ‘˜è¦ (Initial Summary Generation)
                logger.info(f"   ğŸ“ æ­£åœ¨ä¸ºä¸“é¢˜ '{t_name}' ç”ŸæˆåŸºäºäº‹å®çš„åˆå§‹æ‘˜è¦ (Sample: {len(pre_candidates)})...")
                overview_materials = [{"title": n.title, "content": n.content or ""} for n, _ in pre_candidates]
                
                # ä½¿ç”¨æ–°çš„è½»é‡çº§æ‘˜è¦ç”Ÿæˆ (é»˜è®¤ main æ¨¡å‹)
                generated_summary = await self.ai.generate_topic_initial_summary(t_name, overview_materials)
                if generated_summary:
                    generated_summary = generated_summary.replace("```", "").strip()
                
                # ä½¿ç”¨ç”Ÿæˆçš„æ‘˜è¦æ›¿æ¢åˆå§‹æè¿° (å¦‚æœç”Ÿæˆå¤±è´¥åˆ™æ²¿ç”¨åˆå§‹æè¿°)
                final_desc = generated_summary if generated_summary else t_desc
                logger.info(f"   ğŸ“ ç”Ÿæˆæ‘˜è¦: {final_desc[:50]}...")
                
                # 3. äºŒæ¬¡è´¨é‡å®¡æ ¸ (Quality Audit with Real Summary)
                # å¤ç”¨ batch_evaluate ä½†åªä¼ ä¸€ä¸ª
                # æ„å»º existing_topics_data ä¾›å‚è€ƒ (Lazy build)
                existing_topics_data = [
                    {"name": t.name, "description": (t.summary or "")[:100]} 
                    for t in active_topics
                ]
                
                audit_list = [{"name": t_name, "description": final_desc}]
                valid_topics = await self.ai.batch_evaluate_topic_quality(audit_list, existing_topics=existing_topics_data)
                
                if not valid_topics:
                    logger.info(f"   âŒ ä¸“é¢˜ '{t_name}' åœ¨åŸºäºçœŸå®æ‘˜è¦çš„äºŒæ¬¡å®¡æ ¸ä¸­æœªé€šè¿‡ï¼Œè·³è¿‡")
                    continue
                
                # ----------------------------------------------------

                # 5.1 æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ä¸“é¢˜é‡å¤ (ä½¿ç”¨æ–°çš„ final_desc)
                existing_topic_obj = None

                for existing_t, existing_vec in active_topic_vecs:
                    sim = self._cosine_similarity(t_vec, existing_vec)
                    # é™ä½é˜ˆå€¼è‡³ 0.6 ä»¥æ•æ‰æ›´å¤šæ½œåœ¨é‡å¤ï¼Œç„¶åäº¤ç»™ AI ç»†åˆ¤
                    if sim > 0.6: 
                        logger.info(f"   ğŸ”„ ä¸ç°æœ‰ä¸“é¢˜ '{existing_t.name}' ç›¸ä¼¼ (sim={sim:.2f})ï¼Œæ­£åœ¨è¿›è¡Œ AI äºŒæ¬¡æ ¸éªŒ...")
                        
                        # ä¼˜åŒ–ï¼šé™åˆ¶ä¼ å…¥ AI çš„æ–‡æœ¬é•¿åº¦
                        is_duplicate, reason = await self.ai.check_topic_duplicate(
                            t_name, 
                            final_desc[:1000], 
                            existing_t.name, 
                            (existing_t.summary or "")[:1000]
                        )
                        
                        if is_duplicate:
                            logger.info(f"   âœ… AI ç¡®è®¤é‡å¤ (ç†ç”±: {reason})ï¼Œå°†åˆå¹¶åˆ°ç°æœ‰ä¸“é¢˜: {existing_t.name}")
                            existing_topic_obj = existing_t
                            processed_topic_ids.add(existing_t.id)
                            break
                        else:
                            logger.info(f"   âŒ AI åˆ¤å®šä¸ºä¸åŒäº‹ä»¶ (ç†ç”±: {reason})")
                
                # æ‰§è¡ŒåŒ¹é…å’Œæ›´æ–° (ä¼ é€’ final_desc ä½œä¸º summary)
                result_topic = await self._match_and_update_topic(
                    db, t_name, final_desc, t_vec, existing_topic_obj, 
                    news_pool, pool_vecs, used_ids,
                    initial_summary=generated_summary
                )
                
                if result_topic:
                    # å¦‚æœæ˜¯æ–°åˆ›å»ºçš„ä¸“é¢˜ï¼Œä¸”æ²¡æœ‰ç°æˆçš„ recordï¼Œå¯ä»¥å°† initial summary å…ˆå­˜å…¥ recordï¼Œ
                    # æˆ–è€…ä¿æŒ record ä¸ºç©ºç­‰å¾…åç»­ç”Ÿæˆ Overviewã€‚
                    # è¿™é‡Œä¸ºäº†æ•°æ®å®Œæ•´æ€§ï¼Œæš‚ä¸”å°† summary å­˜å…¥ recordï¼Œé¿å…ä¸ºç©ºã€‚
                    if not existing_topic_obj and not result_topic.record and generated_summary:
                         result_topic.record = generated_summary
                         db.add(result_topic)
                         await db.flush()
                    
                    if existing_topic_obj:
                        updated_topics_count += 1
                    else:
                        new_topics_created += 1
                        # æ–°ä¸“é¢˜åŠ å…¥ active_topic_vecs ä»¥ä¾›åç»­ï¼ˆè™½ç„¶æœ¬è½® Phase 1 ä¸ä¼šå†å›å¤´ï¼Œä½†ä¸ºäº†é€»è¾‘å®Œæ•´ï¼‰
                        active_topic_vecs.append((result_topic, t_vec))
                        processed_topic_ids.add(result_topic.id)

            # === Phase 2: æ‰«æå…¶ä½™ç°æœ‰ä¸“é¢˜ ===
            logger.info("ğŸ” [Phase 2] æ‰«æå…¶ä½™ç°æœ‰ä¸“é¢˜ï¼Œå¯»æ‰¾æ½œåœ¨æ›´æ–°...")
            
            # è§„åˆ™ï¼šå¯¹äºæœ€è¿‘ N å¤©å†…æ›´æ–°è¿‡çš„æ´»è·ƒä¸“é¢˜ï¼Œå°è¯•ä»é«˜çƒ­åº¦æ–°é—»æ± ä¸­å¯»æ‰¾åŒ¹é…æ›´æ–°
            check_cutoff_date = datetime.now() - timedelta(days=settings.TOPIC_UPDATE_LOOKBACK_DAYS)
            
            for existing_t, existing_vec in active_topic_vecs:
                if existing_t.id in processed_topic_ids:
                    continue

                # æ£€æŸ¥ä¸“é¢˜æ›´æ–°æ—¶é—´ï¼Œå¦‚æœå¤ªä¹…æ²¡æ›´æ–°ï¼ˆä¸”ä¸æ˜¯æœ¬æ¬¡æ–°åˆ›å»ºçš„ï¼‰ï¼Œåˆ™è·³è¿‡ä»¥èŠ‚çœèµ„æº
                if existing_t.updated_time and existing_t.updated_time < check_cutoff_date:
                    continue
                
                # ä½¿ç”¨ç°æœ‰ä¸“é¢˜çš„ä¿¡æ¯è¿›è¡ŒåŒ¹é…
                # æ³¨æ„ï¼šç°æœ‰ä¸“é¢˜æ²¡æœ‰ t_desc å˜é‡ï¼Œä½¿ç”¨ summary æˆ– name
                logger.info(f"   Evaluating existing topic: {existing_t.name}")
                
                result_topic = await self._match_and_update_topic(
                    db, 
                    existing_t.name, 
                    existing_t.summary or existing_t.name, 
                    existing_vec, 
                    existing_t, 
                    news_pool, 
                    pool_vecs, 
                    used_ids
                )
                
                if result_topic:
                    updated_topics_count += 1
                    processed_topic_ids.add(result_topic.id)

            logger.info(f"âœ… ä¸“é¢˜åˆ·æ–°å®Œæˆï¼Œæ–°å»º {new_topics_created} ä¸ªï¼Œæ›´æ–° {updated_topics_count} ä¸ª")

            # æ˜¾å¼æ¸…ç†å¤§å¯¹è±¡ï¼Œå¸®åŠ© GC å›æ”¶
            logger.info("ğŸ§¹ [TopicService] æ­£åœ¨æ‰§è¡Œèµ„æºé‡Šæ”¾ä¸å†…å­˜æ¸…ç†...")
            if 'news_pool' in locals(): del news_pool
            if 'pool_vecs' in locals(): del pool_vecs
            if 'active_topics' in locals(): del active_topics
            if 'active_topic_vecs' in locals(): del active_topic_vecs
            
            import gc
            gc.collect()
            logger.info("âœ… [TopicService] å†…å­˜æ¸…ç†å®Œæˆ")
            
    async def run_topic_scan_in_background(self, topic_id: int, include_used: bool = False):
        """
        åå°ä»»åŠ¡ï¼šä¸ºæŒ‡å®šä¸“é¢˜æ‰§è¡Œæ‰«æ
        """
        try:
            async with AsyncSessionLocal() as db:
                topic = await db.get(Topic, topic_id)
                if topic:
                    await self._trigger_topic_scan(db, topic, include_used=include_used)
        except Exception as e:
            logger.error(f"åå°æ‰«æä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    async def create_manual_topic(self, db: AsyncSession, name: str, trigger_scan: bool = True) -> Topic:
        """
        æ‰‹åŠ¨åˆ›å»ºä¸“é¢˜
        :param trigger_scan: æ˜¯å¦ç«‹å³è§¦å‘æ‰«æï¼ˆé»˜è®¤ Trueï¼‰ã€‚å¦‚æœåœ¨ API è°ƒç”¨ä¸­ï¼Œå»ºè®®è®¾ä¸º False å¹¶ä½¿ç”¨åå°ä»»åŠ¡ã€‚
        """
        # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåä¸“é¢˜
        existing = (await db.execute(select(Topic).where(Topic.name == name))).scalar_one_or_none()
        if existing:
            raise ValueError(f"ä¸“é¢˜ '{name}' å·²å­˜åœ¨ï¼Œæ— æ³•é‡å¤åˆ›å»ºã€‚")

        # 2. åˆ›å»ºæ–°ä¸“é¢˜
        # ç”Ÿæˆå‘é‡
        t_embs = await self.ai.get_embeddings([name])
        t_vec = t_embs[0] if t_embs and t_embs[0] else []
        
        new_topic = Topic(
            name=name,
            summary="æ‰‹åŠ¨åˆ›å»ºä¸“é¢˜ï¼Œæ­£åœ¨æ‰«æç›¸å…³æ–°é—»...",
            start_time=datetime.now(),
            updated_time=datetime.now(),
            heat_score=0,
            embedding=t_vec,
            status="active"
        )
        db.add(new_topic)
        await db.flush()
        logger.info(f"æ‰‹åŠ¨åˆ›å»ºä¸“é¢˜ '{name}' æˆåŠŸ (ID: {new_topic.id})")
        target_topic = new_topic

        # 3. è§¦å‘æ‰«ææ›´æ–° (å¤ç”¨ Phase 2 é€»è¾‘)
        if trigger_scan:
            await self._trigger_topic_scan(db, target_topic, include_used=True) # æ‰‹åŠ¨åˆ›å»ºé»˜è®¤å…è®¸æŠ¢å /åŒ…å«å·²å½’ç±»æ–°é—»
            
        return target_topic

    async def update_topic_name(self, db: AsyncSession, topic_id: int, new_name: str, trigger_scan: bool = True) -> Topic:
        """
        æ›´æ–°ä¸“é¢˜åç§°
        :param trigger_scan: æ˜¯å¦ç«‹å³è§¦å‘æ‰«æã€‚
        """
        topic = (await db.execute(select(Topic).where(Topic.id == topic_id))).scalar_one_or_none()
        if not topic:
            raise ValueError("ä¸“é¢˜ä¸å­˜åœ¨")
            
        # æŸ¥é‡
        existing = (await db.execute(select(Topic).where(Topic.name == new_name))).scalar_one_or_none()
        if existing and existing.id != topic_id:
            raise ValueError(f"ä¸“é¢˜å '{new_name}' å·²è¢«å…¶ä»–ä¸“é¢˜å ç”¨")
            
        old_name = topic.name
        topic.name = new_name
        
        # é‡æ–°ç”Ÿæˆå‘é‡
        t_embs = await self.ai.get_embeddings([new_name])
        t_vec = t_embs[0] if t_embs and t_embs[0] else []
        topic.embedding = t_vec
        
        db.add(topic)
        await db.flush()
        
        logger.info(f"ä¸“é¢˜ '{old_name}' é‡å‘½åä¸º '{new_name}'ï¼Œå·²æ›´æ–°å‘é‡")
        
        # è§¦å‘æ‰«æ
        if trigger_scan:
            logger.info("æ­£åœ¨è§¦å‘é‡æ–°æ‰«æ...")
            await self._trigger_topic_scan(db, topic, include_used=True) # é‡å‘½åä¹Ÿå…è®¸æ‰©å¤§èŒƒå›´
        
        return topic

    async def _trigger_topic_scan(self, db: AsyncSession, target_topic: Topic, include_used: bool = False):
        """
        å†…éƒ¨å¤ç”¨é€»è¾‘ï¼šå¯¹æŒ‡å®šä¸“é¢˜æ‰§è¡Œ Phase 2 æ‰«æ
        :param include_used: æ˜¯å¦åŒ…å«å·²ç»è¢«å…¶ä»–ä¸“é¢˜å½’ç±»çš„æ–°é—»ã€‚
                             æ‰‹åŠ¨è§¦å‘ï¼ˆåˆ›å»º/æ”¹åï¼‰æ—¶å»ºè®®ä¸º Trueï¼Œä»¥ä¾¿å°†ç›¸å…³æ–°é—»â€œå¸çº³â€è¿›æ¥ã€‚
        """
        # å‡†å¤‡æ’é™¤åˆ—è¡¨
        exclude_ids = set()
        
        if include_used:
            # å¦‚æœå…è®¸åŒ…å«å·²å½’ç±»æ–°é—»ï¼Œåˆ™åªæ’é™¤ *å½“å‰ä¸“é¢˜* å·²ç»æœ‰çš„æ–°é—»ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            current_stmt = select(TopicTimelineItem.news_id, TopicTimelineItem.sources).where(TopicTimelineItem.topic_id == target_topic.id)
            current_res = await db.execute(current_stmt)
            
            exclude_ids = set()
            for nid, srcs in current_res:
                if nid:
                    exclude_ids.add(nid)
                if srcs and isinstance(srcs, list):
                    for src in srcs:
                        if isinstance(src, dict) and "id" in src:
                            try:
                                exclude_ids.add(int(src["id"]))
                            except (ValueError, TypeError):
                                pass
            logger.info(f"ğŸ” [Scan] æ¨¡å¼: åŒ…å«å·²å½’ç±»æ–°é—» (åªæ’é™¤å½“å‰ä¸“é¢˜å·²æœ‰çš„ {len(exclude_ids)} æ¡)")
        else:
            # é»˜è®¤æ¨¡å¼ï¼šæ’é™¤æ‰€æœ‰å·²å½’ç±»æ–°é—»
            used_stmt = select(TopicTimelineItem.news_id, TopicTimelineItem.sources)
            used_res = await db.execute(used_stmt)
            
            exclude_ids = set()
            for nid, srcs in used_res:
                if nid:
                    exclude_ids.add(nid)
                if srcs and isinstance(srcs, list):
                    for src in srcs:
                        if isinstance(src, dict) and "id" in src:
                            try:
                                exclude_ids.add(int(src["id"]))
                            except (ValueError, TypeError):
                                pass
            logger.info(f"ğŸ” [Scan] æ¨¡å¼: æ’é™¤æ‰€æœ‰å·²å½’ç±»æ–°é—» (å…± {len(exclude_ids)} æ¡)")

        days = settings.TOPIC_LOOKBACK_DAYS
        start_date = datetime.now() - timedelta(days=days)
        
        pool_stmt = (
            select(News)
            .where(News.publish_date >= start_date)
            .where(News.id.notin_(exclude_ids) if exclude_ids else True)
            .order_by(desc(News.heat_score))
            .limit(settings.TOPIC_RECALL_POOL_SIZE)
        )
        news_pool = (await db.execute(pool_stmt)).scalars().all()
        
        if not news_pool:
            logger.info("ğŸ“­ æ²¡æœ‰å¾…å¤„ç†çš„æ–°é—»ï¼Œè·³è¿‡ä¸“é¢˜æ‰«æ")
            return

        # ç¡®ä¿æ± ä¸­æ–°é—»æœ‰å‘é‡
        pool_vecs = await self._ensure_news_embeddings_batch(db, news_pool)
        
        # ç¡®ä¿ä¸“é¢˜æœ‰å‘é‡
        if not target_topic.embedding:
             logger.warning("ä¸“é¢˜æ— å‘é‡ï¼Œè·³è¿‡æ‰«æ")
             return

        # æ‰§è¡ŒåŒ¹é…
        logger.info(f"ğŸ” [Scan] æ­£åœ¨ä¸ºä¸“é¢˜ '{target_topic.name}' æ‰«æç›¸å…³æ–°é—» (Pool: {len(news_pool)})...")
        
        # æ‰‹åŠ¨/å®šå‘æ‰«ææ—¶ï¼Œé€‚å½“é™ä½å‘é‡åŒ¹é…é˜ˆå€¼ï¼ˆ0.6 -> 0.38ï¼‰ï¼Œä¾é  AI äºŒæ¬¡æ ¸éªŒæ¥æŠŠå…³
        # è¿™æ ·èƒ½å¬å›æ›´å¤šè¯­ä¹‰ç›¸å…³ä½†å‘é‡è·ç¦»ç¨è¿œçš„æ–°é—»
        manual_threshold = 0.20
        
        result_topic = await self._match_and_update_topic(
            db, 
            target_topic.name, 
            target_topic.summary or target_topic.name, 
            target_topic.embedding, 
            target_topic, 
            news_pool, 
            pool_vecs, 
            exclude_ids, # ä¼ é€’ä½œä¸º used_idsï¼Œç”¨äºå†…éƒ¨è¿‡æ»¤
            match_threshold=manual_threshold
        )
        
        if result_topic:
             logger.info(f"âœ… ä¸“é¢˜ '{target_topic.name}' æ‰«ææ›´æ–°å®Œæˆ")
        else:
             logger.info(f"ä¸“é¢˜ '{target_topic.name}' æœªåŒ¹é…åˆ°æ–°çš„ç›¸å…³æ–°é—»")

    async def regenerate_topic_overview_action(self, db: AsyncSession, topic_id: int) -> Optional[str]:
        """
        æ‰‹åŠ¨è§¦å‘ï¼šé‡æ–°ç”Ÿæˆä¸“é¢˜ç»¼è¿°
        """
        topic = (await db.execute(select(Topic).where(Topic.id == topic_id))).scalar_one_or_none()
        if not topic:
            return None
            
        # è·å–è¯¥ä¸“é¢˜ä¸‹æ‰€æœ‰å…³è”çš„æ–°é—»
        all_items_stmt = (
            select(TopicTimelineItem)
            .where(TopicTimelineItem.topic_id == topic_id)
            .order_by(desc(TopicTimelineItem.event_time))
            .limit(50)
        )
        all_items = (await db.execute(all_items_stmt)).scalars().all()
        
        overview_materials = []
        for it in all_items:
            overview_materials.append({
                "title": it.news_title,
                "content": it.content or "" 
            })
            
        if not overview_materials:
            return "æš‚æ— ç›¸å…³æ–°é—»ï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°ã€‚"
            
        overview_text = await self.ai.generate_topic_overview(
            topic.name, 
            overview_materials
        )
        
        if overview_text:
            topic.record = overview_text
            # é¡ºä¾¿æ›´æ–° summary
            summary_prompt = prompt_manager.get_user_prompt("topic_overview_summary", overview_text=overview_text[:2000])
            new_summary = await self.ai.chat_completion(summary_prompt)
            if new_summary:
                topic.summary = new_summary.replace("```", "").strip()
            
            db.add(topic)
            await db.commit()
            
        return overview_text

    async def _match_and_update_topic(
        self,
        db: AsyncSession,
        t_name: str,
        t_desc: str,
        t_vec: List[float],
        existing_topic_obj: Optional[Topic],
        news_pool: List[News],
        pool_vecs: Dict[int, List[float]],
        used_ids: Set[int],
        match_threshold: float = settings.TOPIC_MATCH_THRESHOLD,
        initial_summary: Optional[str] = None
    ) -> Optional[Topic]:
        """
        æ ¸å¿ƒé€»è¾‘ï¼šæ ¹æ®ä¸“é¢˜ä¿¡æ¯ï¼ˆåç§°ã€æè¿°ã€å‘é‡ï¼‰ï¼Œåœ¨ news_pool ä¸­å¯»æ‰¾åŒ¹é…æ–°é—»ï¼Œ
        ç» AI æ ¸éªŒåï¼Œåˆ›å»ºæ–°ä¸“é¢˜æˆ–æ›´æ–°æ—§ä¸“é¢˜ã€‚
        """
        is_duplicate = (existing_topic_obj is not None)
        
        # 1. å‘é‡åˆç­›å€™é€‰æ–°é—»
        candidates = []
        max_sim_found = 0.0
        
        for n in news_pool:
            # è·³è¿‡å·²ç»åœ¨å½“å‰è½®æ¬¡å¤„ç†è¿‡çš„æ–°é—»
            if n.id in used_ids:
                continue
                
            n_vec = pool_vecs.get(n.id)
            if not n_vec:
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            sim = self._cosine_similarity(t_vec, n_vec)
            if sim > max_sim_found:
                max_sim_found = sim
            
            if sim > match_threshold: # åˆç­›é˜ˆå€¼
                candidates.append((n, sim))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        candidates.sort(key=lambda x: x[1], reverse=True)
        # å–å‰ 20 ä¸ªç»™ AI æ ¸éªŒ
        candidates = candidates[:settings.TOPIC_MATCH_MAX_CANDIDATES]
        
        # ä¼˜åŒ–ï¼šå¦‚æœæ˜¯æ—§ä¸“é¢˜æ›´æ–°ï¼Œè¿‡æ»¤æ‰å‘å¸ƒæ—¶é—´è¶…è¿‡ 24 å°æ—¶çš„æ–°é—»
        # é¿å…å°†å‡ å¤©å‰çš„æ—§æ–°é—»ä½œä¸ºâ€œæ–°åŠ¨æ€â€æ›´æ–°è¿›å»
        if is_duplicate:
            cutoff_time = datetime.now() - timedelta(hours=24)
            original_count = len(candidates)
            candidates = [c for c in candidates if c[0].publish_date and c[0].publish_date >= cutoff_time]
            
            if len(candidates) < original_count:
                logger.info(f"   ğŸ§¹ [æ—§ä¸“é¢˜] è¿‡æ»¤äº† {original_count - len(candidates)} æ¡è¿‡æ—§æ–°é—» (< {cutoff_time.strftime('%m-%d %H:%M')})")
            
            if not candidates:
                logger.info(f"   â© [æ—§ä¸“é¢˜] æ— è¿‘æœŸå€™é€‰æ–°é—»ï¼Œè·³è¿‡")
                return None
        
        # å¦‚æœæ˜¯æ–°ä¸“é¢˜ï¼Œä¸”å€™é€‰ä¸è¶³ï¼Œåˆ™è·³è¿‡ï¼›å¦‚æœæ˜¯åˆå¹¶æ—§ä¸“é¢˜ï¼Œå€™é€‰ä¸è¶³ä¹Ÿæ— å¦¨ï¼ˆåªæ˜¯æœ¬æ¬¡æ²¡æ›´æ–°ï¼‰
        if not is_duplicate and len(candidates) <= settings.TOPIC_MIN_NEWS_COUNT:
            logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] åˆç­›å€™é€‰æ–°é—»ä¸è¶³ ({len(candidates)} <= {settings.TOPIC_MIN_NEWS_COUNT} / MaxSim: {max_sim_found:.3f})ï¼Œè·³è¿‡")
            return None
        
        if is_duplicate and not candidates:
            logger.info(f"   âš ï¸ [æ—§ä¸“é¢˜åˆå¹¶] æ— å€™é€‰æ–°é—» (MaxSim: {max_sim_found:.3f} / Threshold: {match_threshold})ï¼Œè·³è¿‡")
            return None

        # 2. AI æ‰¹é‡æ ¸éªŒ
        verify_tasks = []
        for n, sim in candidates:
            verify_tasks.append({
                "topic_name": t_name,
                "topic_summary": t_desc, # è¿™é‡Œç”¨ summary å­—æ®µä¼ é€’ description
                "news_title": n.title,
                "news_summary": n.summary or (n.content or "")[:200] or ""
            })
        
        verified_results = await self.ai.verify_topic_match_batch(verify_tasks)
        
        confirmed_news = []
        for idx, (is_match, reason) in enumerate(verified_results):
            if is_match:
                logger.info(f"   âœ… [Match] {candidates[idx][0].title[:30]}... (Reason: {reason})")
                confirmed_news.append(candidates[idx][0])
            else:
                # å¯é€‰: è¯¦ç»†æ¨¡å¼ä¸‹è®°å½•ä¸åŒ¹é…ä¿¡æ¯
                logger.info(f"   âŒ [Mismatch] {candidates[idx][0].title[:30]}... (Reason: {reason})")

        # === è§„åˆ™è°ƒæ•´ï¼šå¯¹äºå·²æœ‰ä¸“é¢˜ï¼Œä½¿ç”¨ Top N çƒ­åº¦æ–°é—»æ± è¿›è¡Œæ›´æ–° ===
        if is_duplicate:
             if not confirmed_news:
                 logger.info(f"   â© [æ—§ä¸“é¢˜] åœ¨ Top {settings.TOPIC_AGGREGATION_TOP_N} æ–°é—»ä¸­æœªæ‰¾åˆ°åŒ¹é…é¡¹")
                 return None

        # å†æ¬¡æ£€æŸ¥æ•°é‡é™åˆ¶
        # æ–°ä¸“é¢˜ï¼šå¿…é¡»æ»¡è¶³æœ€å°æ•°é‡é™åˆ¶
        # ç”¨æˆ·è¦æ±‚ï¼šåª’ä½“æŠ¥é“ >= 3 (å³ count >= 3) => count < 3 åˆ™è·³è¿‡
        if not is_duplicate and len(confirmed_news) < settings.TOPIC_MIN_NEWS_COUNT:
            logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] AI æ ¸éªŒé€šè¿‡æ•°é‡ä¸è¶³ ({len(confirmed_news)} < {settings.TOPIC_MIN_NEWS_COUNT})ï¼Œè·³è¿‡")
            return None
            
        # æ£€æŸ¥çƒ­åº¦æŒ‡æ ‡ ( > 3.5 ä»¥å…è®¸æ™®é€šæ–°é—»æˆä¸“é¢˜)
        # è®¡ç®—çƒ­åº¦ï¼ˆå–æ–°é—»æœ€å¤§çƒ­åº¦ï¼‰
        max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
        if not is_duplicate and max_heat < 3.5:
             logger.info(f"   âš ï¸ [æ–°ä¸“é¢˜] çƒ­åº¦ä¸è¶³ ({max_heat} < 3.5)ï¼Œè·³è¿‡")
             return None
        
        # æ—§ä¸“é¢˜ï¼šä¸é™åˆ¶æœ€å°æ•°é‡ï¼Œåªè¦æœ‰æ–°çš„å°±åˆå¹¶
        if is_duplicate and not confirmed_news:
            return None
        
        # 3. åˆ›å»ºæˆ–æ›´æ–°ä¸“é¢˜
        current_topic_id = None
        topic_obj_to_return = None

        if is_duplicate:
            logger.info(f"   ğŸ”„ æ›´æ–°æ—§ä¸“é¢˜: {existing_topic_obj.name} (æ–°å¢ {len(confirmed_news)} æ¡æ–°é—»)")
            # æ›´æ–°æ—§ä¸“é¢˜çš„ update_time å’Œ heat_score
            max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
            current_max = existing_topic_obj.heat_score or 0
            if max_heat > current_max:
                existing_topic_obj.heat_score = max_heat
            
            new_end_time = max([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else None
            if new_end_time and (not existing_topic_obj.updated_time or new_end_time > existing_topic_obj.updated_time):
                existing_topic_obj.updated_time = new_end_time
            
            current_topic_id = existing_topic_obj.id
            topic_obj_to_return = existing_topic_obj
        else:
            logger.info(f"   âœ¨ åˆ›å»ºæ–°ä¸“é¢˜: {t_name} (åŒ…å« {len(confirmed_news)} æ¡æ–°é—»)")
            
            # è®¡ç®—çƒ­åº¦ï¼ˆå–æ–°é—»æœ€å¤§çƒ­åº¦ï¼‰
            max_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
            # æœ€æ—©æ—¶é—´
            start_time = min([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else datetime.now()
            # æœ€æ–°æ—¶é—´
            end_time = max([n.publish_date for n in confirmed_news if n.publish_date]) if confirmed_news else datetime.now()

            new_topic = Topic(
                name=t_name,
                summary=t_desc,
                start_time=start_time,
                updated_time=end_time,
                heat_score=max_heat,
                embedding=t_vec,
                status="active"
            )
            db.add(new_topic)
            await db.flush()
            current_topic_id = new_topic.id
            topic_obj_to_return = new_topic
        
        # 4. è¡¥å…¨æ–°é—»è¯¦æƒ…ä¸ç”Ÿæˆæ—¶é—´è½´
        # 4.1 å…ˆæ£€æŸ¥å¹¶è¡¥å…¨æ–°é—»è¯¦æƒ…
        async with crawler_service.make_crawler() as crawler:
            for n in confirmed_news:
                if not n.content or len(n.content) < 100:
                    logger.info(f"   ğŸ“¥ æ­£åœ¨è¡¥å…¨æ–°é—»è¯¦æƒ…: {n.title[:20]}...")
                    try:
                        crawled = await crawler_service.crawl_content_with_instance(n.url, crawler)
                        if crawled and len(crawled) > 50:
                            n.content = crawled
                            # å†…å®¹æ›´æ–°äº†ï¼Œæ‘˜è¦æœ€å¥½ä¹Ÿåˆ·æ–°ä¸€ä¸‹ï¼Œå¦åˆ™æ—§æ‘˜è¦å¯èƒ½ä¸å‡†
                            fresh_summary = await self.ai.generate_summary(n.title, n.content, max_words=200)
                            if fresh_summary:
                                n.summary = fresh_summary
                            db.add(n)
                    except Exception as e:
                        logger.warning(f"   âš ï¸ è¡¥å…¨è¯¦æƒ…å¤±è´¥: {e}")
        
        await db.flush()

        # 4.2 ç”Ÿæˆæ ‡å‡†åŒ–çš„æ—¶é—´è½´å†…å®¹ (æŒ‰å¤©èšåˆ + AI åˆæˆ)
        # å°† confirmed_news æŒ‰æ—¥æœŸåˆ†ç»„
        news_by_date = defaultdict(list)
        for n in confirmed_news:
            d_str = (n.publish_date or datetime.now()).strftime("%Y-%m-%d")
            news_by_date[d_str].append({
                "id": n.id,
                "title": n.title,
                "summary": n.summary or (n.content or "")[:200],
                "source": n.source,
                "url": n.url,
                "publish_date": n.publish_date  # ä¸ºäº†ç²¾ç¡®æ—¶é—´æ·»åŠ 
            })
        
        # éå†æ¯ä¸€å¤©ï¼Œè°ƒç”¨ AI åˆæˆäº‹ä»¶
        current_topic_name = topic_obj_to_return.name if topic_obj_to_return else None
        
        for d_str, day_news in news_by_date.items():
            # 1. è·å–è¯¥å¤©å·²æœ‰çš„æ—¶é—´è½´èŠ‚ç‚¹ï¼ˆä¸ºäº†åˆå¹¶æ›´æ–°ï¼‰
            # æ³¨æ„ï¼šsqlite/pg å…¼å®¹æ€§ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ event_time å­˜çš„æ˜¯ datetime
            target_date = datetime.strptime(d_str, "%Y-%m-%d").date()
            
            # æ„é€ æŸ¥è¯¢èŒƒå›´ï¼šå½“å¤© 00:00:00 åˆ° 23:59:59
            day_start = datetime.combine(target_date, datetime.min.time())
            day_end = datetime.combine(target_date, datetime.max.time())
            
            existing_items_stmt = (
                select(TopicTimelineItem)
                .where(TopicTimelineItem.topic_id == current_topic_id)
                .where(TopicTimelineItem.event_time >= day_start)
                .where(TopicTimelineItem.event_time <= day_end)
            )
            existing_items = (await db.execute(existing_items_stmt)).scalars().all()
            
            # 2. æ”¶é›†è¯¥å¤©æ‰€æœ‰ç›¸å…³çš„æ–°é—» ID (æ—§ + æ–°)
            all_news_ids = set()
            for n in day_news:
                all_news_ids.add(n["id"])
            
            for it in existing_items:
                if it.news_id:
                    all_news_ids.add(it.news_id)
                if it.sources:
                    for s in it.sources:
                        if isinstance(s, dict) and s.get("id"):
                            all_news_ids.add(s["id"])
                            
            # 3. å¦‚æœæœ‰æ—§èŠ‚ç‚¹ï¼Œéœ€è¦é‡æ–°æ‹‰å–æ‰€æœ‰ç›¸å…³æ–°é—»çš„è¯¦æƒ…ï¼Œè¿›è¡Œå…¨é‡é‡ç”Ÿæˆ
            # å¦‚æœæ²¡æœ‰æ—§èŠ‚ç‚¹ï¼Œç›´æ¥ç”¨ day_news å³å¯
            final_news_list = []
            
            if existing_items:
                # æ‹‰å–æ‰€æœ‰æ¶‰åŠçš„æ–°é—»å¯¹è±¡
                news_stmt = select(News).where(News.id.in_(list(all_news_ids)))
                all_news_objs = (await db.execute(news_stmt)).scalars().all()
                
                for n in all_news_objs:
                    final_news_list.append({
                        "id": n.id,
                        "title": n.title,
                        "summary": n.summary or (n.content or "")[:200],
                        "source": n.source,
                        "url": n.url,
                        "publish_date": n.publish_date
                    })
            else:
                final_news_list = day_news

            # 4. è°ƒç”¨ AI åˆæˆï¼ˆå…¨é‡ï¼‰
            logger.info(f"   ğŸ”„ æ­£åœ¨é‡ç”Ÿæˆ {d_str} çš„æ—¶é—´è½´ (åŸºäº {len(final_news_list)} æ¡æ–°é—»)...")
            day_events = await self.ai.generate_daily_timeline_events(d_str, final_news_list, topic_name=current_topic_name)
            
            # ç¡¬æ€§è§„åˆ™ï¼šæ¯å¤©æœ€å¤šä¿ç•™ 2 ä¸ªèŠ‚ç‚¹
            if day_events and len(day_events) > 2:
                logger.info(f"   âš ï¸ [Rule] AI ç”Ÿæˆäº† {len(day_events)} ä¸ªèŠ‚ç‚¹ï¼Œå¼ºåˆ¶æˆªå–å‰ 2 ä¸ª")
                day_events = day_events[:2]

            # å¦‚æœ AI æ²¡æœ‰ç”Ÿæˆä»»ä½•äº‹ä»¶ï¼ˆå¤±è´¥æˆ–ä¸ºç©ºï¼‰ï¼Œåˆ™é™çº§å¤„ç†ï¼šé€‰æœ€é‡è¦çš„ 1-2 æ¡ä½œä¸ºä»£è¡¨
            if not day_events:
                logger.warning(f"   âš ï¸ {d_str} AI åˆæˆäº‹ä»¶å¤±è´¥ï¼Œé™çº§ä¸ºä½¿ç”¨ Top æ–°é—»")
                # æŒ‰ publish_date æ’åºï¼Œå–æœ€æ–°çš„
                final_news_list.sort(key=lambda x: x.get("publish_date") or datetime.min, reverse=True)
                # ç®€å•å–å‰ 2 æ¡
                for n_item in final_news_list[:2]:
                    day_events.append({
                        "content": n_item["summary"] or n_item["title"],
                        "source_ids": [n_item["id"]]
                    })

            # 5. åˆ é™¤æ—§èŠ‚ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå†™å…¥æ–°èŠ‚ç‚¹
            if existing_items:
                for old_it in existing_items:
                    await db.delete(old_it)
                await db.flush() # ç«‹å³æ‰§è¡Œåˆ é™¤

            # å…¥åº“ Timeline Items
            for event in day_events:
                content = event.get("content")
                if not content:
                    continue
                
                source_ids = event.get("source_ids", [])
                
                # æ„å»º sources åˆ—è¡¨
                sources_data = []
                # æ‰¾å‡ºå¯¹åº”çš„ news item info
                primary_news = None
                
                for nid in source_ids:
                    # åœ¨ final_news_list ä¸­æŸ¥æ‰¾
                    found = next((x for x in final_news_list if x["id"] == nid), None)
                    if found:
                        sources_data.append({
                            "id": found["id"],
                            "name": found["source"] or "æœªçŸ¥æ¥æº",
                            "url": found["url"],
                            "title": found["title"]
                        })
                        if not primary_news:
                            primary_news = found
                
                # å¦‚æœ source_ids ä¸ºç©ºæˆ–æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…œåº•ï¼ˆè™½ç„¶ä¸åº”è¯¥å‘ç”Ÿï¼‰
                if not primary_news and final_news_list:
                     primary_news = final_news_list[0]

                # å¦‚æœå¯ç”¨ï¼Œä»ä¸»è¦æ–°é—»ç¡®å®šäº‹ä»¶æ—¶é—´
                event_time = datetime.strptime(d_str, "%Y-%m-%d")
                if primary_news and primary_news.get("publish_date"):
                    event_time = primary_news["publish_date"]

                # åˆ›å»º item
                item = TopicTimelineItem(
                    topic_id=current_topic_id,
                    event_time=event_time,
                    content=content,
                    # å…¼å®¹æ—§å­—æ®µï¼Œå­˜å‚¨ä¸»è¦æ¥æº
                    news_id=primary_news["id"] if primary_news else None,
                    news_title=primary_news["title"] if primary_news else None,
                    source_name=primary_news["source"] if primary_news else None,
                    source_url=primary_news["url"] if primary_news else None,
                    # æ–°å­—æ®µï¼šå¤šæ¥æº
                    sources=sources_data
                )
                db.add(item)
                
                # æ ‡è®° used_ids
                for nid in source_ids:
                    used_ids.add(nid)

        await db.flush() # ç¡®ä¿ item å…¥åº“

        # 6. ç”Ÿæˆ/æ›´æ–°ä¸“é¢˜ç»¼è¿° (Overview) & ç®€è¦æè¿° (Summary)
        # ä¼˜åŒ–ç­–ç•¥ï¼š
        # 1. å¦‚æœæ˜¯æ—§ä¸“é¢˜æ›´æ–°ï¼Œä¸”æ–°å¢æ–°é—»å¾ˆå°‘/çƒ­åº¦ä½ï¼Œåˆ™è·³è¿‡ Overview æ›´æ–°ï¼ˆèŠ‚çœå¤§é‡ Tokenï¼‰
        # 2. å¦‚æœæ˜¯æ–°ä¸“é¢˜ï¼Œå¿…é¡»ç”Ÿæˆ Overviewã€‚
        # 3. ç”Ÿæˆ Summary æ—¶ï¼Œå¦‚æœå·²æœ‰ initial_summary ä¸”æ˜¯æ–°ä¸“é¢˜ï¼Œåˆ™ç›´æ¥å¤ç”¨ã€‚

        should_update_overview = True
        
        if is_duplicate:
            # æ£€æŸ¥æ–°å¢æ–°é—»çš„é‡è¦æ€§
            new_news_count = len(confirmed_news)
            max_new_heat = max([float(n.heat_score or 0) for n in confirmed_news]) if confirmed_news else 0
            
            # é˜ˆå€¼ï¼šå¦‚æœæ–°å¢å°‘äº 3 æ¡ï¼Œä¸”æœ€å¤§çƒ­åº¦ä¸è¶…è¿‡ 6.0ï¼Œåˆ™è®¤ä¸ºå˜æ›´ä¸æ˜¾è‘—ï¼Œä¸é‡å†™ç»¼è¿°
            # (Timeline å·²ç»æ›´æ–°äº†ï¼Œç”¨æˆ·ä¾ç„¶å¯ä»¥çœ‹åˆ°æ–°åŠ¨æ€ï¼Œåªæ˜¯ Overview æ–‡æœ¬ä¸å˜)
            if new_news_count < 3 and max_new_heat < 6.0:
                logger.info(f"   â© [Overview] æ–°å¢å†…å®¹è¾ƒå°‘ (Count={new_news_count}, MaxHeat={max_new_heat:.1f})ï¼Œè·³è¿‡ç»¼è¿°é‡å†™")
                should_update_overview = False
        
        if should_update_overview:
            # è·å–è¯¥ä¸“é¢˜ä¸‹æ‰€æœ‰å…³è”çš„æ–°é—»ï¼ˆä¸ºäº†ç”Ÿæˆå…¨é¢çš„ç»¼è¿°ï¼‰
            # é™åˆ¶æ•°é‡ï¼Œå–çƒ­åº¦æœ€é«˜çš„ 50 æ¡
            all_items_stmt = (
                select(TopicTimelineItem)
                .where(TopicTimelineItem.topic_id == current_topic_id)
                .order_by(desc(TopicTimelineItem.event_time))
                .limit(50)
            )
            all_items = (await db.execute(all_items_stmt)).scalars().all()
            
            # æ”¶é›†ç”¨äºç”Ÿæˆç»¼è¿°çš„ç´ æ
            overview_materials = []
            for it in all_items:
                # ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨ summary æˆ– content çš„æˆªæ–­ç‰ˆæœ¬
                # TopicTimelineItem.content é€šå¸¸æ˜¯æ—¶é—´è½´äº‹ä»¶çš„æè¿°ï¼Œæœ¬èº«æ¯”è¾ƒç²¾ç®€
                # ä½†å¦‚æœå®ƒåŒ…å«å¾ˆé•¿çš„å¼•ç”¨ï¼Œè¿˜æ˜¯é™åˆ¶ä¸€ä¸‹ä¸ºå¥½
                content_val = it.content or ""
                overview_materials.append({
                    "title": it.news_title,
                    "content": content_val[:500] # ä½¿ç”¨ timeline çš„ AI æ‘˜è¦ä½œä¸ºç´ æï¼Œé™åˆ¶é•¿åº¦
                })
            
            if overview_materials:
                # 1. ç”Ÿæˆå¤šç»´åº¦ç»¼è¿°
                # æ³¨æ„ï¼šå¦‚æœæ˜¯ Existing Topicï¼Œåå­—å¯èƒ½å’Œ t_name ä¸å®Œå…¨ä¸€æ ·ï¼ˆå¦‚æœæ˜¯ Phase 2ï¼‰ï¼Œä½†é€šå¸¸ Phase 2 ä¼ å…¥çš„ t_name å°±æ˜¯ existing.name
                target_name = existing_topic_obj.name if existing_topic_obj else t_name
                
                overview_text = await self.ai.generate_topic_overview(
                    target_name, 
                    overview_materials
                )
                
                # 2. æ›´æ–° summary (ç®€è¦æè¿°)
                if overview_text:
                    new_summary = None
                    
                    # ä¼˜åŒ–ï¼šå¦‚æœæ˜¯æ–°ä¸“é¢˜ï¼Œä¸”å¤–éƒ¨ä¼ å…¥äº† initial_summaryï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸å†è°ƒç”¨ AI
                    if not is_duplicate and initial_summary:
                        logger.info("   âœ… [Summary] å¤ç”¨åˆå§‹æ‘˜è¦ï¼Œè·³è¿‡äºŒæ¬¡ç”Ÿæˆ")
                        new_summary = initial_summary
                    else:
                        # ä¸ºäº†èŠ‚çœ tokenï¼Œç›´æ¥è®© AI åŸºäº overview_text ç”Ÿæˆ summary
                        summary_prompt = prompt_manager.get_user_prompt("topic_overview_summary", overview_text=overview_text[:2000])
                        new_summary = await self.ai.chat_completion(summary_prompt, route_key="TOPIC_OVERVIEW")
                    
                    # æ›´æ–° Topic
                    topic_to_update = existing_topic_obj if is_duplicate else topic_obj_to_return
                    topic_to_update.record = overview_text
                    if new_summary:
                        topic_to_update.summary = new_summary.replace("```", "").strip()
                    
                    db.add(topic_to_update)
                else:
                    logger.warning(f"   âš ï¸ ä¸“é¢˜ç»¼è¿°ç”Ÿæˆå¤±è´¥ (None)ï¼Œè·³è¿‡ Summary æ›´æ–°")

        await db.commit()
        return topic_obj_to_return
            
    async def scheduled_topic_task(self) -> None:
        """
        åå°å®šæ—¶ä»»åŠ¡ï¼šå‘¨æœŸæ€§è‡ªåŠ¨æ‰§è¡Œä¸“é¢˜è¿½è¸ªï¼ˆç”Ÿæˆæ–°ä¸“é¢˜/æ›´æ–°æ—§ä¸“é¢˜ï¼‰ã€‚
        ä½œä¸ºç‹¬ç«‹å®ˆæŠ¤è¿›ç¨‹è¿è¡Œï¼Œç¡®ä¿å³ä½¿æ²¡æœ‰å¤–éƒ¨è§¦å‘ï¼Œç³»ç»Ÿä¹Ÿèƒ½æŒ‰é…ç½®çš„æ—¶é—´é—´éš”è‡ªåŠ¨åˆ·æ–°ä¸“é¢˜æ•°æ®ã€‚
        """
        logger.info("â° ä¸“é¢˜è¿½è¸ªå®šæ—¶ä»»åŠ¡å¯åŠ¨...")
        while True:
            try:
                if not await check_db_connection():
                    logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥å¼‚å¸¸ï¼Œä¸“é¢˜è¿½è¸ªä»»åŠ¡æš‚åœè¿è¡Œï¼Œç­‰å¾…æ¢å¤...")
                    await asyncio.sleep(60)
                    continue

                if not (settings.DATABASE_URL or "").strip():
                    logger.warning("âš ï¸ æœªé…ç½® DATABASE_URLï¼Œä¸“é¢˜è¿½è¸ªä»»åŠ¡æš‚åœè¿è¡Œ")
                    await asyncio.sleep(60)
                    continue

                # è¯»å–é…ç½®çš„é—´éš”æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤ä¸º 4 å°æ—¶
                interval = getattr(settings, "TOPIC_SCHEDULE_INTERVAL_HOURS", 4)
                await asyncio.sleep(interval * 3600) 
                
                await self.refresh_topics()
            except AIConfigurationError as e:
                logger.error(f"ğŸ›‘ é…ç½®é”™è¯¯: {e} è¯·æ£€æŸ¥ config.yaml æ˜¯å¦é…ç½®æ­£ç¡®")
                logger.warning("âš ï¸ ä¸“é¢˜è¿½è¸ªä»»åŠ¡è¿›å…¥ç»´æŠ¤æ¨¡å¼ï¼Œæ¯ 5 åˆ†é’Ÿå°è¯•é‡å¯æœåŠ¡æ£€æŸ¥ä¸€æ¬¡...")
                await asyncio.sleep(300)
            except Exception as e:
                logger.error(f"ä¸“é¢˜è¿½è¸ªå®šæ—¶ä»»åŠ¡é”™è¯¯: {e}")
                await asyncio.sleep(300)

    # è¾…åŠ©æ–¹æ³•
    async def _ensure_news_embeddings_batch(self, db: AsyncSession, news_list: List[News]) -> Dict[int, List[float]]:
        out = {}
        to_embed_indices = []
        texts = []
        
        for idx, n in enumerate(news_list):
            if n.embedding and len(n.embedding) > 0:
                out[n.id] = list(n.embedding)
            else:
                txt = " ".join([n.title or "", n.summary or "", (n.content or "")[:500]]).strip()
                texts.append(txt[:1000] if txt else (n.title or "")[:1000])
                to_embed_indices.append(idx)
        
        if texts:
            # æ‰¹é‡å‘é‡åŒ–è°ƒç”¨ï¼ˆå¦‚æœéœ€è¦åˆ†å—ï¼‰
            batch_size = 10
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i : i + batch_size]
                batch_indices = to_embed_indices[i : i + batch_size]
                try:
                    embs = await self.ai.get_embeddings(batch_texts)
                    for local_idx, emb in enumerate(embs):
                        original_idx = batch_indices[local_idx]
                        n = news_list[original_idx]
                        if emb:
                            n.embedding = emb
                            db.add(n)
                            out[n.id] = emb
                except Exception as e:
                    logger.error(f"   âš ï¸ æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")
            
            await db.flush()
        return out

    async def _ensure_topic_embeddings(self, db: AsyncSession, topics: List[Topic]) -> List[Tuple[Topic, List[float]]]:
        out = []
        to_embed = []
        for idx, t in enumerate(topics):
            if t.embedding and len(t.embedding) > 0:
                out.append((t, list(t.embedding)))
            else:
                txt = f"{t.name} {t.summary}"
                to_embed.append((idx, txt[:1000]))
                out.append((t, []))
        
        if to_embed:
            texts = [x[1] for x in to_embed]
            try:
                embs = await self.ai.get_embeddings(texts)
                for (idx, _), vec in zip(to_embed, embs):
                    if vec:
                        t = topics[idx]
                        t.embedding = vec
                        db.add(t)
                        out[idx] = (t, vec) # æ›´æ–°è¾“å‡ºåˆ—è¡¨ä¸­çš„å…ƒç»„
            except Exception as e:
                 logger.error(f"   âš ï¸ ä¸“é¢˜å‘é‡åŒ–å¤±è´¥: {e}")
            await db.flush()
        return out

    async def _ensure_news_summary(self, db: AsyncSession, news: News) -> None:
        if (news.summary or "").strip():
            return

        # å¦‚æœç¼ºå¤±ï¼Œå°è¯•æŠ“å–å†…å®¹
        if not news.content or len(news.content) < 50:
             try:
                content = await crawler_service.crawl_content(news.url)
                if content:
                    # æŠ“å–åç«‹å³æ¸…æ´—
                    cleaned = clean_html_tags(content)
                    if len(cleaned) > 50:
                        news.content = cleaned
             except Exception:
                 pass
        
        content = news.content or ""
        if len(content) < 50:
            return # å†…å®¹å¤ªçŸ­ï¼Œæ— æ³•æ€»ç»“
            
        try:
            summary = await self.ai.generate_summary(news.title, content, max_words=200)
            if summary:
                news.summary = summary
                db.add(news)
        except Exception:
            pass

# å…¨å±€å®ä¾‹
from app.services.ai_service import ai_service
topic_service = TopicService(ai=ai_service)

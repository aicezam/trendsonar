"""
æœ¬æ–‡ä»¶ç”¨äºç”Ÿæˆèˆ†æƒ…æŠ¥è¡¨ï¼ˆå…¨å±€/å…³é”®è¯ï¼‰ï¼Œå¹¶æä¾›æŠ¥è¡¨ç¼“å­˜ã€å†å²è®°å½•ä¸å›¾è¡¨æ•°æ®èšåˆèƒ½åŠ›ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `ReportService`: æŠ¥è¡¨ç”Ÿæˆä¸ç¼“å­˜æœåŠ¡
- `report_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import gc
import asyncio
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from time import monotonic
from typing import Any, Dict, List, Optional, Tuple, AsyncIterator

import numpy as np
from sqlalchemy import and_, case, cast, delete, desc, func, literal, or_, select, true
from sqlalchemy.orm import defer
from sqlalchemy.dialects.postgresql import JSONB

from app.core.database import AsyncSessionLocal, check_db_connection
from app.core.config import get_settings
from app.core.exceptions import AIConfigurationError
from app.core.logger import logger
from app.core.prompts import prompt_manager
from app.models.news import News
from app.models.report import ReportCache

settings = get_settings()
from app.services.ai_service import ai_service


class ReportService:
    """
    è¾“å…¥:
    - æ•°æ®åº“ä¸­çš„æ–°é—»æ•°æ®ä¸åˆ†ææ¡ä»¶ï¼ˆæ—¶é—´/åˆ†ç±»/åœ°åŒº/æ¥æºç­‰ï¼‰

    è¾“å‡º:
    - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ä¸å†å²ç¼“å­˜è®°å½•

    ä½œç”¨:
    - ç”Ÿæˆèˆ†æƒ…åˆ†ææŠ¥è¡¨ï¼ˆå…¨å±€/å…³é”®è¯ï¼‰ï¼Œå¹¶å†™å…¥æ•°æ®åº“ç¼“å­˜ä¾›å‰ç«¯å±•ç¤º
    """

    def __init__(self) -> None:
        self._chart_cache: Dict[Tuple[Any, ...], Tuple[float, Any]] = {}
        self._global_cache: Optional[Tuple[float, str, Dict[str, Any]]] = None

    def clear_local_cache(self) -> None:
        """
        æ¸…ç©ºæœ¬åœ°å†…å­˜ç¼“å­˜
        """
        self._chart_cache.clear()
        self._global_cache = None
        
    def _cleanup_chart_cache(self) -> None:
        """
        æ¸…ç†å›¾è¡¨ç¼“å­˜ï¼šä¼˜å…ˆç§»é™¤è¿‡æœŸé¡¹ï¼Œè‹¥ä»è¶…é™åˆ™ç§»é™¤æœ€æ—©çš„é¡¹
        """
        now = monotonic()
        # 1. ç§»é™¤è¶…è¿‡ 300s çš„è€æ—§ç¼“å­˜
        keys_to_remove = [k for k, v in self._chart_cache.items() if now - v[0] > 300]
        for k in keys_to_remove:
            self._chart_cache.pop(k, None)

        # 2. è‹¥ä»è¶…é™ï¼Œä¿ç•™æœ€æ–°çš„ 128 æ¡
        if len(self._chart_cache) > 256:
            sorted_items = sorted(self._chart_cache.items(), key=lambda x: x[1][0])
            self._chart_cache = dict(sorted_items[-128:])

    def _build_news_filters(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
    ) -> List[Any]:
        filters: List[Any] = []
        if keyword:
            filters.append(News.title.ilike(f"%{keyword}%"))
        if category and category != "all":
            if "," in category:
                filters.append(News.category.in_(category.split(",")))
            else:
                filters.append(News.category == category)
        if region and region != "all":
            selected_regions = region.split(",")
            region_conditions = [News.region.ilike(f"%{r}%") for r in selected_regions]
            filters.append(or_(*region_conditions))
        if source and source != "all":
            if "," in source:
                filters.append(News.source.in_(source.split(",")))
            else:
                filters.append(News.source == source)
        if start_date:
            try:
                s_dt = datetime.strptime(start_date, "%Y-%m-%d")
                filters.append(News.publish_date >= s_dt)
            except Exception:
                pass
        if end_date:
            try:
                e_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
                filters.append(News.publish_date < e_dt)
            except Exception:
                pass
        return filters

    async def _get_word_cloud_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        try:
            jsonb_keywords = case(
                (func.jsonb_typeof(cast(News.keywords, JSONB)) == "array", cast(News.keywords, JSONB)),
                else_=cast(literal("[]"), JSONB),
            )
            kw = func.jsonb_array_elements_text(jsonb_keywords).table_valued("value").alias("kw")

            async with AsyncSessionLocal() as db:
                stmt = (
                    select(kw.c.value.label("name"), func.count().label("value"))
                    .select_from(News)
                    .join(kw, true())
                )
                if filters:
                    stmt = stmt.where(and_(*filters))
                stmt = stmt.where(func.length(func.trim(kw.c.value)) > 0)
                stmt = stmt.where(func.lower(func.trim(kw.c.value)).notin_(["æ— å†…å®¹", "null", "ç©º", "none", ""]))
                stmt = stmt.group_by(kw.c.value).order_by(desc("value")).limit(50)

                result = await db.execute(stmt)
                rows = result.all()

            data = [{"name": str(name), "value": int(value)} for name, value in rows]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(è¯äº‘)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | æ•°é‡={len(data)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data
        except Exception:
            async with AsyncSessionLocal() as db:
                stmt = select(News.keywords)
                if filters:
                    stmt = stmt.where(and_(*filters))
                result = await db.execute(stmt)
                keywords_values = result.scalars().all()

            all_keywords: List[str] = []
            for kws in keywords_values:
                if not kws or not isinstance(kws, list):
                    continue
                valid_kws = [
                    k.strip()
                    for k in kws
                    if k
                    and isinstance(k, str)
                    and k.strip()
                    and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                ]
                all_keywords.extend(valid_kws)

            word_counts = Counter(all_keywords)
            data = [{"name": k, "value": v} for k, v in word_counts.most_common(50)]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(è¯äº‘)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | è¡Œæ•°={len(keywords_values)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data

    async def _get_source_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        async with AsyncSessionLocal() as db:
            stmt = select(News.source.label("name"), func.count().label("value")).select_from(News)
            if filters:
                stmt = stmt.where(and_(*filters))
            stmt = stmt.where(func.length(func.trim(News.source)) > 0)
            stmt = stmt.group_by(News.source).order_by(desc("value")).limit(10)
            result = await db.execute(stmt)
            rows = result.all()

        data = [{"name": str(name), "value": int(value)} for name, value in rows]
        elapsed = monotonic() - t0
        if elapsed > 0.5:
            logger.info(
                f"å›¾è¡¨æ•°æ®(æ¥æº)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | æ•°é‡={len(data)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
            )
        return data

    async def _get_sentiment_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> Dict[str, Any]:
        if not await check_db_connection(verbose=False):
             return {"sentiment_dist": [], "neg_keywords": []}

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        label_expr = case(
            (News.sentiment_label.in_(["æ­£é¢", "ä¸­ç«‹", "è´Ÿé¢"]), News.sentiment_label),
            else_=literal("ä¸­ç«‹"),
        ).label("name")

        try:
            jsonb_keywords = case(
                (func.jsonb_typeof(cast(News.keywords, JSONB)) == "array", cast(News.keywords, JSONB)),
                else_=cast(literal("[]"), JSONB),
            )
            kw = func.jsonb_array_elements_text(jsonb_keywords).table_valued("value").alias("kw")

            async with AsyncSessionLocal() as db:
                stmt_dist = select(label_expr, func.count().label("value")).select_from(News)
                if filters:
                    stmt_dist = stmt_dist.where(and_(*filters))
                stmt_dist = stmt_dist.group_by(label_expr)
                dist_rows = (await db.execute(stmt_dist)).all()

                stmt_neg = select(kw.c.value.label("name"), func.count().label("value")).select_from(News).join(kw, true())
                if filters:
                    stmt_neg = stmt_neg.where(and_(*filters))
                stmt_neg = stmt_neg.where(News.sentiment_label == "è´Ÿé¢")
                stmt_neg = stmt_neg.where(func.length(func.trim(kw.c.value)) > 0)
                stmt_neg = stmt_neg.where(func.lower(func.trim(kw.c.value)).notin_(["æ— å†…å®¹", "null", "ç©º", "none", ""]))
                stmt_neg = stmt_neg.group_by(kw.c.value).order_by(desc("value")).limit(10)
                neg_rows = (await db.execute(stmt_neg)).all()

            dist_map = {str(name): int(value) for name, value in dist_rows}
            sentiment_dist = [
                {"name": "æ­£é¢", "value": int(dist_map.get("æ­£é¢", 0))},
                {"name": "ä¸­ç«‹", "value": int(dist_map.get("ä¸­ç«‹", 0))},
                {"name": "è´Ÿé¢", "value": int(dist_map.get("è´Ÿé¢", 0))},
            ]
            neg_keywords = [str(name) for name, _ in neg_rows]

            data = {"sentiment_dist": sentiment_dist, "neg_keywords": neg_keywords}
            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(æƒ…æ„Ÿåˆ†å¸ƒ)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | negN={len(neg_keywords)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data
        except Exception:
            async with AsyncSessionLocal() as db:
                stmt = select(News.sentiment_label, News.keywords).select_from(News)
                if filters:
                    stmt = stmt.where(and_(*filters))
                result = await db.execute(stmt)
                rows = result.all()

            sentiment_counts = {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0}
            negative_keywords: List[str] = []
            for label, kws in rows:
                label_str = label if label in sentiment_counts else "ä¸­ç«‹"
                sentiment_counts[label_str] += 1
                if label_str == "è´Ÿé¢" and kws and isinstance(kws, list):
                    valid_kws = [
                        k.strip()
                        for k in kws
                        if k
                        and isinstance(k, str)
                        and k.strip()
                        and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                    ]
                    negative_keywords.extend(valid_kws)

            sentiment_dist = [
                {"name": "æ­£é¢", "value": sentiment_counts["æ­£é¢"]},
                {"name": "ä¸­ç«‹", "value": sentiment_counts["ä¸­ç«‹"]},
                {"name": "è´Ÿé¢", "value": sentiment_counts["è´Ÿé¢"]},
            ]
            neg_keywords = [k for k, _ in Counter(negative_keywords).most_common(10)]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(æƒ…æ„Ÿ)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | è¡Œæ•°={len(rows)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return {"sentiment_dist": sentiment_dist, "neg_keywords": neg_keywords}

    async def _get_list_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        async with AsyncSessionLocal() as db:
            stmt = (
                select(
                    News.id,
                    News.title,
                    News.url,
                    News.source,
                    News.heat_score,
                    News.publish_date,
                    News.summary,
                    News.sources,
                    News.category,
                    News.region,
                    News.sentiment_label,
                    News.sentiment_score,
                )
                .select_from(News)
                .order_by(desc(News.heat_score), desc(News.publish_date))
                .limit(limit)
            )
            if filters:
                stmt = stmt.where(and_(*filters))
            result = await db.execute(stmt)
            rows = result.all()

        data = []
        for (
            nid,
            title,
            url,
            source,
            heat_score,
            publish_date,
            summary,
            sources,
            cat,
            reg,
            sentiment_label,
            sentiment_score,
        ) in rows:
            data.append(
                {
                    "id": int(nid),
                    "title": title,
                    "url": url,
                    "source": source,
                    "heat": heat_score,
                    "time": publish_date.isoformat() if publish_date else None,
                    "summary": summary,
                    "sources": sources,
                    "category": cat,
                    "region": reg,
                    "sentiment_label": sentiment_label,
                    "sentiment_score": sentiment_score,
                }
            )

        elapsed = monotonic() - t0
        if elapsed > 0.5:
            logger.info(
                f"chart-data list æ…¢æŸ¥è¯¢: {elapsed:.2f}s | items={len(data)} | category={category or ''} | range={start_date or ''}~{end_date or ''}"
            )
        return data

    async def generate_and_cache_global_report(self, period: str = "weekly") -> None:
        """
        è¾“å…¥:
        - `period`: å‘¨æœŸï¼ˆdaily/weekly/monthlyï¼‰

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - ç”ŸæˆæŒ‡å®šå‘¨æœŸçš„å…¨å±€æŠ¥è¡¨å¹¶å†™å…¥ç¼“å­˜
        """
        
        if not await check_db_connection(verbose=False):
            logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡ç”Ÿæˆå…¨å±€æŠ¥è¡¨ç¼“å­˜")
            return

        logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆå…¨å±€å¤§ç›˜æŠ¥è¡¨ç¼“å­˜ ({period})...")
        try:
            today = datetime.now()
            end_date_str = today.strftime("%Y-%m-%d")

            if period == "daily":
                start_date_str = today.strftime("%Y-%m-%d")
                # Add 6 days lookback for trend chart in daily report
                data = await self._generate_analysis_data(
                    keyword="", 
                    start_date=start_date_str, 
                    end_date=end_date_str, 
                    generate_ai=True,
                    trend_lookback_days=6
                )
            elif period == "monthly":
                start_date_str = today.replace(day=1).strftime("%Y-%m-%d")
                data = await self._generate_analysis_data(
                    keyword="", start_date=start_date_str, end_date=end_date_str, generate_ai=True
                )
            else:
                start_date_str = (today - timedelta(days=6)).strftime("%Y-%m-%d")
                data = await self._generate_analysis_data(
                    keyword="", start_date=start_date_str, end_date=end_date_str, generate_ai=True
                )

            await self.save_report_cache("global", period, data)
            logger.info(f"âœ… å…¨å±€æŠ¥è¡¨ç¼“å­˜ ({period}) å·²æ›´æ–°")
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥è¡¨ç¼“å­˜å¤±è´¥: {e}")

    async def save_report_cache(self, r_type: str, keyword: str, data: Dict[str, Any]) -> Optional[int]:
        """
        è¾“å…¥:
        - `r_type`: æŠ¥è¡¨ç±»å‹ï¼ˆglobal/keywordï¼‰
        - `keyword`: å…³é”®è¯æˆ–å‘¨æœŸæ ‡è¯†ï¼ˆglobal æ—¶ç”¨ daily/weekly/monthlyï¼‰
        - `data`: æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - å†™å…¥æŠ¥è¡¨ç¼“å­˜ï¼Œå¹¶å¯¹æ•°é‡ä¸åŒæ—¥é‡å¤ç¼“å­˜è¿›è¡Œæ§åˆ¶
        """
        if not await check_db_connection(verbose=False):
            return None

        async with AsyncSessionLocal() as db:
            if r_type == "global":
                today_start = datetime.combine(datetime.now().date(), datetime.min.time())
                await db.execute(
                    delete(ReportCache).where(
                        ReportCache.report_type == "global",
                        ReportCache.keyword == keyword,
                        ReportCache.created_at >= today_start,
                    )
                )

            if r_type == "keyword" and keyword:
                stmt = (
                    select(ReportCache.id)
                    .where(ReportCache.report_type == "keyword", ReportCache.keyword == keyword)
                    .order_by(desc(ReportCache.created_at))
                )
                result = await db.execute(stmt)
                ids = result.scalars().all()

                if len(ids) >= 10:
                    ids_to_delete = ids[9:]
                    await db.execute(delete(ReportCache).where(ReportCache.id.in_(ids_to_delete)))

            cache = ReportCache(report_type=r_type, keyword=keyword, data=data, created_at=datetime.now())
            db.add(cache)
            await db.commit()
            await db.refresh(cache)
            return int(cache.id)
        return None

    async def get_recent_reports(self, limit: int = 10, keyword: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `limit`: è¿”å›æ•°é‡ä¸Šé™

        è¾“å‡º:
        - æœ€è¿‘å…³é”®è¯æŠ¥è¡¨åˆ—è¡¨ï¼ˆæŒ‰å…³é”®è¯å»é‡ï¼‰

        ä½œç”¨:
        - ä¸ºå‰ç«¯å±•ç¤ºæœ€è¿‘ç”Ÿæˆçš„å…³é”®è¯æŠ¥è¡¨å…¥å£
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = select(ReportCache).where(ReportCache.report_type == "keyword")
            if keyword:
                stmt = stmt.where(ReportCache.keyword == keyword)
            stmt = stmt.order_by(desc(ReportCache.created_at)).limit(limit * 5)
            result = await db.execute(stmt)
            reports = result.scalars().all()

            if keyword:
                return [
                    {
                        "keyword": r.keyword,
                        "date": r.created_at.strftime("%Y-%m-%d %H:%M"),
                        "created_at": r.created_at.isoformat(),
                        "id": r.id,
                    }
                    for r in reports[:limit]
                    if r.keyword
                ]

            seen = set()
            recent = []
            for r in reports:
                if r.keyword and r.keyword not in seen:
                    recent.append(
                        {
                            "keyword": r.keyword,
                            "date": r.created_at.strftime("%Y-%m-%d %H:%M"),
                            "created_at": r.created_at.isoformat(),
                            "id": r.id,
                        }
                    )
                    seen.add(r.keyword)
                    if len(recent) >= limit:
                        break
            return recent

    async def get_report_history(self, keyword: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯

        è¾“å‡º:
        - è¯¥å…³é”®è¯ä¸‹çš„å†å²æŠ¥è¡¨è®°å½•åˆ—è¡¨

        ä½œç”¨:
        - æ”¯æŒå…³é”®è¯æŠ¥è¡¨å†å²å›æº¯
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = (
                select(ReportCache)
                .where(ReportCache.report_type == "keyword", ReportCache.keyword == keyword)
                .order_by(desc(ReportCache.created_at))
            )
            if limit:
                stmt = stmt.limit(limit)
            result = await db.execute(stmt)
            reports = result.scalars().all()
            return [
                {
                    "id": r.id, 
                    "date": r.created_at.strftime("%Y-%m-%d %H:%M"), 
                    "created_at": r.created_at.isoformat(),
                    "keyword": r.keyword
                }
                for r in reports
            ]

    async def get_global_history(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å…¨å±€æŠ¥è¡¨å†å²è®°å½•åˆ—è¡¨

        ä½œç”¨:
        - æ”¯æŒå…¨å±€æŠ¥è¡¨å†å²å›æº¯
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = (
                select(ReportCache)
                .where(ReportCache.report_type == "global")
                .order_by(desc(ReportCache.created_at))
            )
            stmt = stmt.limit(limit or 100)
            result = await db.execute(stmt)
            reports = result.scalars().all()
            return [
                {
                    "id": r.id,
                    "date": r.created_at.strftime("%Y-%m-%d %H:%M"),
                    "created_at": r.created_at.isoformat(),
                    "type": r.keyword or "weekly",
                }
                for r in reports
            ]

    async def delete_report_cache(self, report_id: int) -> bool:
        if not await check_db_connection(verbose=False):
            return False
        async with AsyncSessionLocal() as db:
            result = await db.execute(select(ReportCache).where(ReportCache.id == report_id))
            cached = result.scalar_one_or_none()
            if not cached:
                return False
            await db.delete(cached)
            await db.commit()
            return True

    async def find_latest_report_id(self, keyword: str = "") -> Optional[int]:
        if not await check_db_connection(verbose=False):
            return None
        async with AsyncSessionLocal() as db:
            if keyword:
                stmt = select(ReportCache.id).where(ReportCache.report_type == "keyword", ReportCache.keyword == keyword)
            else:
                stmt = select(ReportCache.id).where(ReportCache.report_type == "global").order_by(desc(ReportCache.created_at))
                # Just take the latest global report
            
            stmt = stmt.order_by(desc(ReportCache.created_at)).limit(1)
            result = await db.execute(stmt)
            return result.scalar_one_or_none()

    async def stream_ai_analysis(self, report_id: int) -> AsyncIterator[str]:
        logger.info(f"ğŸ”„ stream_ai_analysis è¿›å…¥: report_id={report_id}")
        if not await check_db_connection(verbose=False):
            logger.error(f"âŒ DB è¿æ¥å¤±è´¥: report_id={report_id}")
            yield "æ•°æ®åº“è¿æ¥å¤±è´¥"
            return

        async with AsyncSessionLocal() as db:
            result = await db.execute(select(ReportCache).where(ReportCache.id == report_id))
            cached = result.scalar_one_or_none()
            if not cached:
                logger.warning(f"âš ï¸ æŠ¥è¡¨æœªæ‰¾åˆ°: report_id={report_id}")
                yield "æŠ¥è¡¨æœªæ‰¾åˆ°"
                return

            data = dict(cached.data or {})
            
            # If already done or has content (legacy support), yield result directly
            ai_status = data.get("ai_status")
            ai_analysis = data.get("ai_analysis")
            logger.info(f"â„¹ï¸ æŠ¥è¡¨çŠ¶æ€: id={report_id} status={ai_status} len={len(ai_analysis or '')}")
            
            if (ai_status == "done" or (ai_analysis and len(ai_analysis) > 100)) and ai_analysis:
                # Update status if missing
                if ai_status != "done":
                    data["ai_status"] = "done"
                    cached.data = data
                    await db.commit()
                yield ai_analysis
                return

            # Prepare context
            params = dict(data.get("params") or {})
            keyword = (params.get("keyword") or cached.keyword or "") if cached.report_type == "keyword" else ""
            start_date = params.get("start_date")
            end_date = params.get("end_date")
            category = params.get("category")
            region = params.get("region")
            source = params.get("source")

            scope_parts = []
            if keyword:
                scope_parts.append(f"å…³é”®è¯={keyword}")
            if category and category != "all":
                scope_parts.append(f"é¢†åŸŸ={category}")
            if region and region != "all":
                scope_parts.append(f"åœ°åŒº={region}")
            if source and source != "all":
                scope_parts.append(f"æ¥æº={source}")
            scope_str = "ï¼›".join(scope_parts) if scope_parts else "å…¨é‡æ ·æœ¬"

            ai_start = start_date or (datetime.now().date() - timedelta(days=0)).strftime("%Y-%m-%d")
            ai_end = end_date or datetime.now().date().strftime("%Y-%m-%d")
            time_range_label = f"{ai_start} è‡³ {ai_end}" if ai_start != ai_end else ai_end

            ai_filters = self._build_news_filters(
                keyword=keyword,
                start_date=ai_start,
                end_date=ai_end,
                category=category,
                region=region,
                source=source,
            )

            ai_stmt = (
                select(
                    News.title,
                    func.substr(News.content, 1, 1000).label("content"),
                    News.summary,
                    News.heat_score,
                    News.source,
                    News.url,
                    News.publish_date,
                )
                .select_from(News)
                .order_by(desc(News.heat_score), desc(News.publish_date))
                .limit(100)
            )
            if ai_filters:
                ai_stmt = ai_stmt.where(and_(*ai_filters))
            ai_result = await db.execute(ai_stmt)
            ai_rows = ai_result.all()

            if not ai_rows:
                yield "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚"
                data["ai_status"] = "done"
                data["ai_analysis"] = "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚"
                cached.data = data
                await db.commit()
                return

            def compact_text(text: Any, max_len: int = 180) -> str:
                t = (text or "").replace("\r", " ").replace("\n", " ").strip()
                if len(t) > max_len:
                    return t[:max_len] + "â€¦"
                return t

            news_lines = []
            for idx, (title, content, summary, heat, src, url, pub_dt) in enumerate(ai_rows, start=1):
                body = content if content else summary
                body_str = compact_text(body, 220)
                title_str = compact_text(title, 80)
                src_str = compact_text(src, 30)
                time_str = pub_dt.isoformat() if pub_dt else ""
                news_lines.append(
                    f"{idx}. [çƒ­åº¦:{(heat or 0):.1f}] [{src_str}] {title_str}\n   æ—¶é—´: {time_str}\n   æ­£æ–‡: {body_str}\n   é“¾æ¥: {url}"
                )

            # Build prompt
            if keyword:
                prompt = prompt_manager.get_user_prompt(
                    "report_keyword_analysis",
                    keyword=keyword,
                    time_range_label=time_range_label,
                    scope_str=scope_str,
                    news_lines="\n\n".join(news_lines)
                )
            else:
                prompt = prompt_manager.get_user_prompt(
                    "report_global_analysis",
                    time_range_label=time_range_label,
                    scope_str=scope_str,
                    news_lines="\n\n".join(news_lines)
                )

            # Mark as running
            data["ai_status"] = "running"
            cached.data = data
            await db.commit()

            # Stream
            full_text = ""
            last_flush = monotonic()
            
            try:
                async for chunk in ai_service.stream_completion(prompt, route_key="REPORT"):
                    full_text += chunk
                    yield chunk

                    if len(full_text) > 100000:
                        yield "\n[ç³»ç»Ÿæç¤º: ç”Ÿæˆå†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
                        break
                    
                    # Incremental update
                    now = monotonic()
                    if (now - last_flush) >= 2.0:
                        data["ai_analysis"] = full_text
                        cached.data = dict(data)
                        await db.commit()
                        last_flush = now
                
                # Update DB with full result
                data["ai_analysis"] = full_text
                data["ai_status"] = "done"
                cached.data = dict(data)
                await db.commit()

            except Exception as e:
                logger.error(f"AI Stream failed: {e}")
                yield f"\n[AI ç”Ÿæˆå¤±è´¥: {e}]"
                data["ai_status"] = "error"
                cached.data = dict(data)
                await db.commit()
            finally:
                try:
                    # Ensure status is not running
                    await db.refresh(cached)
                    current_data = dict(cached.data or {})
                    if current_data.get("ai_status") == "running":
                        current_data["ai_status"] = "done"
                        if full_text:
                            current_data["ai_analysis"] = full_text
                        cached.data = current_data
                        await db.commit()
                except Exception as e:
                    logger.error(f"Error in finally block: {e}")

    async def generate_report_and_stream_ai(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> None:
        logger.info(f"ğŸ“„ åå°ç”ŸæˆæŠ¥è¡¨å¼€å§‹: keyword={keyword or '-'}")
        data = await self._generate_analysis_data(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=source,
            limit=limit,
            generate_ai=False,
        )

        news_count = data.get("summary", {}).get("total", 0)
        if news_count == 0:
            data["ai_analysis"] = "æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ï¼Œæ— æ³•è¿›è¡Œ AI ç»¼è¿°ã€‚"
            data["ai_status"] = "done"
            logger.info(f"ğŸ“„ æ— æ–°é—»ï¼Œè·³è¿‡ AI ç»¼è¿°: keyword={keyword or '-'}")
        else:
            data["ai_analysis"] = ""
            data["ai_status"] = "pending"

        if keyword:
            report_id = await self.save_report_cache("keyword", keyword, data)
        else:
            report_id = await self.save_report_cache("global", "weekly", data)

        if not report_id:
            logger.warning("âš ï¸ æŠ¥è¡¨ç¼“å­˜å†™å…¥å¤±è´¥ï¼Œåå°ä»»åŠ¡ç»“æŸ")
            return

        logger.info(f"ğŸ“„ æŠ¥è¡¨ç¼“å­˜å·²å†™å…¥: id={report_id} keyword={keyword or '-'}")
        
        # Only start background analysis if we have news AND it's not a keyword report (which streams on-demand)
        # OR if we want to support background generation for all.
        # User feedback: "Frontend sees generating, but needs refresh".
        # This means frontend streaming is not working or not connecting to the background stream.
        # To fix this, we disable background generation for keyword reports (interactive),
        # so the frontend triggers it via stream_ai_analysis endpoint.
        # But wait, if we disable it here, the status is 'pending'.
        # The frontend sees 'pending' and calls streamAiAnalysis.
        # stream_ai_analysis endpoint needs to handle 'pending' by STARTING generation.
        
        if news_count > 0 and not keyword:
            # For global reports (usually scheduled or slow), run in background?
            # Or just disable background for all interactive generation?
            # Let's disable for all interactive for now to ensure streaming works.
            # But "generate_report_and_stream_ai" implies it MIGHT be background.
            # If we comment it out, the frontend MUST be open to generate.
            # Let's try commenting it out as requested to fix the "stuck" issue.
            # await self._stream_ai_analysis_to_cache(report_id)
            pass
        
        # Actually, if I comment it out, existing logic in stream_ai_analysis (endpoint) needs to pick it up.
        # Let's check stream_ai_analysis implementation again.
        
        logger.info(f"ğŸ“„ åå°ç”ŸæˆæŠ¥è¡¨ç»“æŸ: id={report_id} keyword={keyword or '-'}")

    async def _stream_ai_analysis_to_cache(self, report_id: int) -> None:
        if not await check_db_connection(verbose=False):
            return

        async with AsyncSessionLocal() as db:
            result = await db.execute(select(ReportCache).where(ReportCache.id == report_id))
            cached = result.scalar_one_or_none()
            if not cached:
                return

            data = dict(cached.data or {})
            params = dict(data.get("params") or {})
            keyword = (params.get("keyword") or cached.keyword or "") if cached.report_type == "keyword" else ""
            start_date = params.get("start_date")
            end_date = params.get("end_date")
            category = params.get("category")
            region = params.get("region")
            source = params.get("source")

            logger.info(f"ğŸ¤– AI ç»¼è¿°ç”Ÿæˆå¼€å§‹: report_id={report_id} keyword={keyword or '-'}")

            scope_parts = []
            if keyword:
                scope_parts.append(f"å…³é”®è¯={keyword}")
            if category and category != "all":
                scope_parts.append(f"é¢†åŸŸ={category}")
            if region and region != "all":
                scope_parts.append(f"åœ°åŒº={region}")
            if source and source != "all":
                scope_parts.append(f"æ¥æº={source}")
            scope_str = "ï¼›".join(scope_parts) if scope_parts else "å…¨é‡æ ·æœ¬"

            ai_start = start_date or (datetime.now().date() - timedelta(days=0)).strftime("%Y-%m-%d")
            ai_end = end_date or datetime.now().date().strftime("%Y-%m-%d")
            time_range_label = f"{ai_start} è‡³ {ai_end}" if ai_start != ai_end else ai_end

            ai_filters = self._build_news_filters(
                keyword=keyword,
                start_date=ai_start,
                end_date=ai_end,
                category=category,
                region=region,
                source=source,
            )

            ai_stmt = (
                select(
                    News.title,
                    func.substr(News.content, 1, 1000).label("content"),
                    News.summary,
                    News.heat_score,
                    News.source,
                    News.url,
                    News.publish_date,
                )
                .select_from(News)
                .order_by(desc(News.heat_score), desc(News.publish_date))
                .limit(100)
            )
            if ai_filters:
                ai_stmt = ai_stmt.where(and_(*ai_filters))
            ai_result = await db.execute(ai_stmt)
            ai_rows = ai_result.all()

            def compact_text(text: Any, max_len: int = 180) -> str:
                t = (text or "").replace("\r", " ").replace("\n", " ").strip()
                if len(t) > max_len:
                    return t[:max_len] + "â€¦"
                return t

            news_lines = []
            for idx, (title, content, summary, heat, src, url, pub_dt) in enumerate(ai_rows, start=1):
                body = content if content else summary
                body_str = compact_text(body, 220)
                title_str = compact_text(title, 80)
                src_str = compact_text(src, 30)
                time_str = pub_dt.isoformat() if pub_dt else ""
                news_lines.append(
                    f"{idx}. [çƒ­åº¦:{(heat or 0):.1f}] [{src_str}] {title_str}\n   æ—¶é—´: {time_str}\n   æ­£æ–‡: {body_str}\n   é“¾æ¥: {url}"
                )

            if keyword:
                prompt = prompt_manager.get_user_prompt(
                    "report_keyword_analysis",
                    keyword=keyword,
                    time_range_label=time_range_label,
                    scope_str=scope_str,
                    news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬",
                )
            else:
                prompt = prompt_manager.get_user_prompt(
                    "report_global_analysis",
                    time_range_label=time_range_label,
                    scope_str=scope_str,
                    news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬",
                )

            data["ai_status"] = "running"
            data["ai_analysis"] = ""
            cached.data = data
            await db.commit()

            full_text = ""
            last_flush = monotonic()
            last_len = 0
            try:
                logger.info(f"DEBUG: Entering stream loop for report {report_id}")
                async for chunk in ai_service.stream_completion(prompt, route_key="REPORT"):
                    # logger.info(f"DEBUG: Got chunk len={len(chunk)} content={chunk[:20]!r}")
                    full_text += chunk
                    
                    if len(full_text) > 100000:
                         logger.warning(f"âš ï¸ Report {report_id} AI output too long, truncated.")
                         full_text += "\n[æˆªæ–­]"
                         break

                    now = monotonic()
                    if (now - last_flush) >= 1.5 and (len(full_text) - last_len) >= 120:
                        data["ai_analysis"] = full_text
                        cached.data = dict(data)
                        await db.commit()
                        last_flush = now
                        last_len = len(full_text)

                if not full_text.strip():
                    logger.warning(f"âš ï¸ AI æµå¼è¿”å›ä¸ºç©ºï¼Œå°è¯•é™çº§ä¸ºéæµå¼ç”Ÿæˆ: report_id={report_id}")
                    fallback = await ai_service.chat_completion(prompt, route_key="REPORT")
                    full_text = (fallback or "").strip()

                # Create a new dict to ensure SQLAlchemy detects the change
                final_data = dict(data)
                final_data["ai_analysis"] = full_text.strip()
                final_data["ai_status"] = "done" if final_data["ai_analysis"] else "error"
                cached.data = final_data
                await db.commit()
            except AIConfigurationError as e:
                logger.warning(f"âš ï¸ AI é…ç½®ä¸å¯ç”¨: {e}")
                data["ai_analysis"] = str(e)
                data["ai_status"] = "error"
                cached.data = data
                await db.commit()
            except Exception as e:
                logger.error(f"AIåˆ†æå¤±è´¥: {e}")
                data["ai_analysis"] = (full_text.strip() or "AI åˆ†æå¤±è´¥").strip()
                data["ai_status"] = "error"
                cached.data = data
                await db.commit()
            finally:
                try:
                    await db.refresh(cached)
                    final_data = dict(cached.data or {})
                    final_status = str(final_data.get("ai_status") or "")
                    final_text = str(final_data.get("ai_analysis") or "")
                    logger.info(
                        f"ğŸ¤– AI ç»¼è¿°ç”Ÿæˆç»“æŸ: report_id={report_id} status={final_status or '-'} chars={len(final_text)}"
                    )
                except Exception:
                    pass

    async def load_report(self, report_id: int) -> Optional[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `report_id`: æŠ¥è¡¨ç¼“å­˜ ID

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼›ä¸å­˜åœ¨è¿”å› None

        ä½œç”¨:
        - è¯»å–æŒ‡å®šå†å²æŠ¥è¡¨ç¼“å­˜ä¾›å‰ç«¯æ¸²æŸ“
        """
        if not await check_db_connection(verbose=False):
            return None

        async with AsyncSessionLocal() as db:
            result = await db.execute(select(ReportCache).where(ReportCache.id == report_id))
            cached = result.scalar_one_or_none()
            if cached:
                data = dict(cached.data)
                data["id"] = cached.id
                if cached.report_type == "global":
                    data["period"] = cached.keyword or "weekly"
                return data
            return None


    async def get_analysis_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
        limit: Optional[int] = None,
        generate_ai: bool = False,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯ï¼ˆä¸ºç©ºä»£è¡¨å…¨å±€ï¼‰
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        - `category`/`region`/`source`: è¿‡æ»¤æ¡ä»¶
        - `limit`: å–æ ·ä¸Šé™
        - `generate_ai`: æ˜¯å¦ç”Ÿæˆ AI æ–‡æœ¬åˆ†æ
        - `use_cache`: æ˜¯å¦ä¼˜å…ˆè¯»å–ç¼“å­˜

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼ˆsummary/charts/top_news/ai_analysisï¼‰

        ä½œç”¨:
        - å¯¹å¤–æä¾›ç»Ÿä¸€æŠ¥è¡¨æ•°æ®å…¥å£ï¼Œå¹¶åœ¨æ¡ä»¶å…è®¸æ—¶å‘½ä¸­ç¼“å­˜æå‡æ€§èƒ½
        """

        if use_cache and not keyword and not start_date and not end_date and not category and not region and not source:
            now_ts = monotonic()
            if self._global_cache and (now_ts - self._global_cache[0]) <= 15:
                return self._global_cache[2]
            
            if await check_db_connection(verbose=False):
                async with AsyncSessionLocal() as db:
                    t0 = monotonic()
                    stmt = (
                        select(ReportCache.id, ReportCache.keyword, ReportCache.data)
                        .where(ReportCache.report_type == "global", ReportCache.keyword == "daily")
                        .order_by(desc(ReportCache.created_at))
                        .limit(1)
                    )
                    result = await db.execute(stmt)
                    row = result.first()

                    if not row:
                        stmt = (
                            select(ReportCache.id, ReportCache.keyword, ReportCache.data)
                            .where(ReportCache.report_type == "global")
                            .order_by(desc(ReportCache.created_at))
                            .limit(1)
                        )
                        result = await db.execute(stmt)
                        row = result.first()

                    if row:
                        rid, kw, data = row
                        data = dict(data)
                        data["id"] = rid
                        elapsed_ms = int((monotonic() - t0) * 1000)
                        logger.info(f"ğŸ“– è¯»å–å…¨å±€æŠ¥è¡¨æ•°æ®åº“ç¼“å­˜ ({kw or 'æœªçŸ¥'}) {elapsed_ms}ms")
                        self._global_cache = (monotonic(), str(kw or ""), data)
                        return data

        data = await self._generate_analysis_data(keyword, start_date, end_date, category, region, source, limit, generate_ai)

        if keyword:
            report_id = await self.save_report_cache("keyword", keyword, data)
            if report_id:
                data["id"] = report_id

        return data

    async def _generate_analysis_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
        limit: Optional[int] = None,
        generate_ai: bool = False,
        trend_lookback_days: int = 0, # New parameter
    ) -> Dict[str, Any]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯è¿‡æ»¤
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸè¿‡æ»¤
        - `category`/`region`/`source`: ç»´åº¦è¿‡æ»¤
        - `limit`: æ•°æ®é‡ä¸Šé™
        - `generate_ai`: æ˜¯å¦ç”Ÿæˆ AI åˆ†ææ–‡å­—

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼ˆæ‘˜è¦ã€è¶‹åŠ¿ã€æ¥æºã€è¯äº‘ã€æƒ…ç»ªåˆ†å¸ƒã€Top æ–°é—»ç­‰ï¼‰

        ä½œç”¨:
        - åœ¨æ•°æ®åº“ä¸­æŒ‰æ¡ä»¶æŸ¥è¯¢æ–°é—»ï¼Œå¹¶èšåˆè®¡ç®—å„ç±»ç»Ÿè®¡æŒ‡æ ‡
        """
        
        # ç»Ÿä¸€è¿”å›ç©ºæ•°æ®çš„é—­åŒ…å‡½æ•°
        def empty_result(msg: str = "æ•°æ®åº“è¿æ¥ä¸å¯ç”¨"):
             return {
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "region": region,
                    "source": source,
                    "limit": limit
                },
                "summary": {"total": 0, "avg_heat": 0, "max_heat": 0, "sentiment_idx": 0, "risk_count": 0, "time_range": "-"},
                    "charts": {
                        "trend": {},
                        "source": {},
                        "word_cloud": [],
                        "sentiment_dist": [],
                        "neg_keywords": [],
                        "correlation": [],
                        "freq_trend": [],
                        "sentiment_trend": [],
                    },
                    "top_news": [],
                    "ai_analysis": f"{msg}ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚",
                }

        if not await check_db_connection(verbose=False):
            return empty_result()

        async with AsyncSessionLocal() as db:
            # ä¼˜åŒ–ï¼šå»¶è¿ŸåŠ è½½å¤§å­—æ®µï¼ˆcontent, embeddingï¼‰ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨
            stmt = select(News).options(
                defer(News.content),
                defer(News.embedding)
            )
            filters = []

            if keyword:
                filters.append(News.title.ilike(f"%{keyword}%"))
            if category and category != "all":
                if "," in category:
                    filters.append(News.category.in_(category.split(",")))
                else:
                    filters.append(News.category == category)
            if region and region != "all":
                selected_regions = region.split(",")
                region_conditions = [News.region.ilike(f"%{r}%") for r in selected_regions]
                filters.append(or_(*region_conditions))
            if source and source != "all":
                if "," in source:
                    filters.append(News.source.in_(source.split(",")))
                else:
                    filters.append(News.source == source)
            if start_date:
                try:
                    s_dt = datetime.strptime(start_date, "%Y-%m-%d")
                    filters.append(News.publish_date >= s_dt)
                except Exception:
                    pass
            if end_date:
                try:
                    e_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
                    filters.append(News.publish_date < e_dt)
                except Exception:
                    pass

            count_stmt = select(func.count()).select_from(News)
            if filters:
                # stmt = stmt.where(and_(*filters))  <-- REMOVED: stmt is not defined yet for charts
                count_stmt = count_stmt.where(and_(*filters))

            real_total_count = await db.scalar(count_stmt) or 0
            
            if real_total_count == 0:
                return empty_result(msg="æœªæ‰¾åˆ°ç›¸å…³æ•°æ®")

            # -------------------------------------------------------------------------
            # 2. ç»Ÿè®¡æŒ‡æ ‡ (èšåˆæŸ¥è¯¢ï¼Œä¸åŠ è½½å¯¹è±¡)
            # -------------------------------------------------------------------------
            stats_stmt = select(
                func.avg(News.heat_score),
                func.max(News.heat_score),
                func.min(News.publish_date),
                func.max(News.publish_date)
            )
            if filters:
                stats_stmt = stats_stmt.where(and_(*filters))
            
            stats_res = await db.execute(stats_stmt)
            avg_heat, max_heat, min_date, max_date = stats_res.one()
            avg_heat = float(avg_heat or 0)
            max_heat = float(max_heat or 0)
            time_range = f"{(min_date or datetime.now()).strftime('%Y-%m-%d')} ~ {(max_date or datetime.now()).strftime('%Y-%m-%d')}"

            # -------------------------------------------------------------------------
            # 3. æƒ…æ„Ÿåˆ†å¸ƒ (èšåˆæŸ¥è¯¢)
            # -------------------------------------------------------------------------
            sentiment_stmt = (
                select(News.sentiment_label, func.count(), func.avg(News.sentiment_score))
                .group_by(News.sentiment_label)
            )
            if filters:
                sentiment_stmt = sentiment_stmt.where(and_(*filters))
            
            sent_res = await db.execute(sentiment_stmt)
            sent_rows = sent_res.all()
            
            sentiment_counts = {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0}
            total_score_sum = 0.0
            total_score_count = 0
            
            for label, count, avg_score in sent_rows:
                l_str = label if label in sentiment_counts else "ä¸­ç«‹"
                sentiment_counts[l_str] += count
                if avg_score is not None:
                    total_score_sum += float(avg_score) * count
                    total_score_count += count

            sentiment_idx = (total_score_sum / total_score_count) if total_score_count else 50.0
            risk_count = sentiment_counts["è´Ÿé¢"]

            sentiment_dist = [
                {"name": "æ­£é¢", "value": sentiment_counts["æ­£é¢"]},
                {"name": "ä¸­ç«‹", "value": sentiment_counts["ä¸­ç«‹"]},
                {"name": "è´Ÿé¢", "value": sentiment_counts["è´Ÿé¢"]},
            ]

            # -------------------------------------------------------------------------
            # 4. æ¥æºåˆ†å¸ƒ (Top 10 èšåˆ)
            # -------------------------------------------------------------------------
            source_stmt = (
                select(News.source, func.count().label("cnt"))
                .group_by(News.source)
                .order_by(desc("cnt"))
                .limit(10)
            )
            if filters:
                source_stmt = source_stmt.where(and_(*filters))
            
            source_res = await db.execute(source_stmt)
            chart_source = [{"name": str(src), "value": cnt} for src, cnt in source_res.all() if src]

            # -------------------------------------------------------------------------
            # 5. è¶‹åŠ¿å›¾ (èšåˆæŸ¥è¯¢)
            # -------------------------------------------------------------------------
            # å¦‚æœæœ‰ trend_lookback_daysï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•æ—¶é—´èŒƒå›´æŸ¥è¯¢è¶‹åŠ¿
            # ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å…ˆç”¨ filters çš„èŒƒå›´ï¼Œå¦‚æœéœ€è¦æ‰©å±•ï¼Œéœ€é‡æ–°æ„å»º filters
            
            trend_map = defaultdict(lambda: {"count": 0, "heat_sum": 0.0, "categories": Counter()})
            
            # ä½¿ç”¨ cast(News.publish_date, Date) åœ¨æŸäº› DB å¯èƒ½ä¸å…¼å®¹ï¼Œä½†åœ¨ PG/SQLite é€šå¸¸å¯è¡Œ
            # æˆ–è€…ç›´æ¥å– publish_date å¹¶åœ¨ Python ç«¯æˆªå–æ—¥æœŸï¼ˆèšåˆåçš„æ•°æ®é‡è¾ƒå°ï¼‰
            # è¿™é‡Œä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬æŒ‰å¤©èšåˆ
            # æ³¨æ„ï¼šSQLite ä¸æ”¯æŒ cast(..., Date) åŒæ ·è¯­æ³•ï¼ŒPostgres æ”¯æŒ
            # æˆ‘ä»¬å‡è®¾ç¯å¢ƒæ˜¯ Postgres (TrendSonar çœ‹èµ·æ¥åƒ) æˆ–è€…å…¼å®¹
            
            # å°è¯•ä½¿ç”¨ func.date_trunc('day', News.publish_date) for PG, or just fetch date
            # å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬æ‹‰å– (date, category, heat, count) èšåˆ
            # ä½† func.date(...) ä¾èµ–æ–¹è¨€ã€‚
            # æ—¢ç„¶å·²ç»ä¼˜åŒ–äº†ï¼Œæˆ‘ä»¬å¯ä»¥ç¨å¾®æ‹‰å–å¤šä¸€ç‚¹æ•°æ®ï¼š(publish_date, category, heat_score) 
            # ä½†è¿˜æ˜¯ä¸æ‹‰å–å…¨é‡ã€‚
            
            # æ›´å¥½æ–¹æ¡ˆï¼šæŒ‰å¤©åˆ†ç»„
            # é’ˆå¯¹ä¸åŒæ•°æ®åº“ï¼Œæ—¥æœŸæˆªæ–­å†™æ³•ä¸åŒã€‚ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ Python åšæ—¥æœŸèšåˆï¼Œ
            # ä½†åªæ‹‰å– (publish_date, heat_score, category)ï¼Œä¸æ‹‰å–å…¶ä»–å­—æ®µã€‚
            
            # ç¡®å®šè¶‹åŠ¿æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´
            trend_filters = list(filters)
            if trend_lookback_days > 0 and start_date:
                # ç§»é™¤åŸæœ‰çš„æ—¥æœŸ filterï¼Œæ·»åŠ æ–°çš„
                trend_start = (datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=trend_lookback_days)).strftime("%Y-%m-%d")
                # é‡æ–°æ„å»ºæ—¶é—´ filter (æ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸º filters æ˜¯ list)
                # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å•ç‹¬æ„å»º trend_stmt
                trend_range_filters = self._build_news_filters(
                     keyword=keyword,
                     start_date=trend_start,
                     end_date=end_date,
                     category=category,
                     region=region,
                     source=source
                )
                trend_filters = trend_range_filters

            trend_stmt = select(
                News.publish_date, 
                News.heat_score, 
                News.category
            ).order_by(News.publish_date)
            
            if trend_filters:
                trend_stmt = trend_stmt.where(and_(*trend_filters))
                
            # è¿™é‡Œå¦‚æœä¸åš SQL group byï¼Œæ•°æ®é‡å¯èƒ½è¿˜æ˜¯å¾ˆå¤§ (æ¯”å¦‚ 10ä¸‡æ¡è®°å½•çš„æ—¥æœŸå’Œçƒ­åº¦)
            # 10ä¸‡æ¡ (date, float, str) å¤§æ¦‚å ç”¨ 100,000 * (10+8+10) bytes â‰ˆ 3MBï¼Œå®Œå…¨å¯æ¥å—ã€‚
            # æ¯”åŠ è½½ 10ä¸‡ä¸ªå¯¹è±¡ (å‡ ç™¾ MB) å°å¾—å¤šã€‚
            
            trend_res = await db.execute(trend_stmt)
            trend_rows = trend_res.all()
            
            for pub_date, heat, cat in trend_rows:
                if not pub_date: continue
                d_str = pub_date.strftime("%Y-%m-%d")
                trend_map[d_str]["count"] += 1
                trend_map[d_str]["heat_sum"] += (heat or 0.0)
                trend_map[d_str]["categories"][cat or "å…¶ä»–"] += 1

            sorted_dates = sorted(trend_map.keys())
            category_names = set()
            for d in sorted_dates:
                category_names.update(trend_map[d]["categories"].keys())
            category_series = [
                {"name": cat, "type": "bar", "stack": "æ–‡ç« æ•°", "data": [trend_map[d]["categories"][cat] for d in sorted_dates]}
                for cat in sorted(category_names)
            ]

            chart_trend = {
                "dates": sorted_dates,
                "counts": [trend_map[d]["count"] for d in sorted_dates],
                "avg_heats": [
                    (trend_map[d]["heat_sum"] / trend_map[d]["count"]) if trend_map[d]["count"] else 0 for d in sorted_dates
                ],
                "category_series": category_series,
            }

            # -------------------------------------------------------------------------
            # 6. å…³é”®è¯/å…±ç°/æ¯æ—¥æƒ…æ„Ÿ (é‡‡æ · Top 2000)
            # -------------------------------------------------------------------------
            # ä¸ºäº†è®¡ç®—è¯äº‘ã€å…±ç°å’Œæ¯æ—¥æƒ…æ„Ÿè¶‹åŠ¿ï¼ˆéœ€è¦å…³é”®è¯å’Œæƒ…æ„Ÿæ˜ç»†ï¼‰ï¼Œæˆ‘ä»¬åªå–çƒ­åº¦æœ€é«˜çš„ 2000 æ¡
            sample_limit = 2000
            sample_stmt = (
                select(News.keywords, News.sentiment_label, News.sentiment_score, News.publish_date)
                .order_by(desc(News.heat_score))
                .limit(sample_limit)
            )
            if filters:
                sample_stmt = sample_stmt.where(and_(*filters))
                
            sample_res = await db.execute(sample_stmt)
            sample_rows = sample_res.all()

            all_keywords = []
            negative_keywords = []
            co_occurrence = defaultdict(int)
            keyword_freq = defaultdict(int)
            daily_kw_freq = defaultdict(Counter)
            # é‡æ–°è®¡ç®— daily_sentiment ç”¨äºè¶‹åŠ¿å›¾ (åŸºäºé‡‡æ ·ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥å†åšä¸€ä¸ª SQL èšåˆï¼Œ
            # ä½†ä¸ºäº†è¶‹åŠ¿å›¾çš„å¹³æ»‘æ€§ï¼Œé‡‡æ · Top 2000 å¯èƒ½ä¹Ÿå¤Ÿäº†ï¼Œä¸è¿‡ä¸ºäº†å‡†ç¡®ï¼Œæœ€å¥½ç”¨ SQL)
            
            # è®©æˆ‘ä»¬ç”¨ SQL èšåˆæ¯æ—¥æƒ…æ„Ÿï¼Œä»¥ä¿è¯å‡†ç¡®æ€§ (step 3 å·²ç»æ˜¯èšåˆäº†ï¼Œä½†é‚£æ˜¯æ€»çš„ï¼Œæˆ‘ä»¬éœ€è¦æ¯æ—¥çš„)
            # daily_sentiment_stmt = select(date, label, count, sum(score))...
            # ä¸ºäº†é¿å…å¤æ‚ SQLï¼Œè¿™é‡Œå…ˆç”¨é‡‡æ ·æ•°æ®è¿‘ä¼¼æ¯æ—¥æƒ…æ„Ÿè¶‹åŠ¿ï¼Œæˆ–è€…æ¥å—é‡‡æ ·è¯¯å·®ã€‚
            # è€ƒè™‘åˆ°æ€§èƒ½ï¼Œé‡‡æ · 2000 æ¡çƒ­ç‚¹æ–°é—»çš„æƒ…æ„Ÿè¶‹åŠ¿é€šå¸¸èƒ½ä»£è¡¨æ•´ä½“è¶‹åŠ¿ã€‚
            
            daily_sentiment = defaultdict(lambda: {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0, "score_sum": 0.0, "score_count": 0})

            for kws, label, score, pub_date in sample_rows:
                if not pub_date: continue
                day = pub_date.strftime("%Y-%m-%d")
                
                # ç»Ÿè®¡æ¯æ—¥æƒ…æ„Ÿ (Sampled)
                l_str = label if label in ["æ­£é¢", "ä¸­ç«‹", "è´Ÿé¢"] else "ä¸­ç«‹"
                daily_sentiment[day][l_str] += 1
                if score is not None:
                    daily_sentiment[day]["score_sum"] += float(score)
                    daily_sentiment[day]["score_count"] += 1
                
                # ç»Ÿè®¡å…³é”®è¯
                if kws and isinstance(kws, list):
                    valid_kws = [
                        k.strip() for k in kws
                        if k and k.strip() and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                    ]
                    all_keywords.extend(valid_kws)
                    if l_str == "è´Ÿé¢":
                        negative_keywords.extend(valid_kws)
                    
                    unique_kws = list(set(valid_kws))
                    daily_kw_freq[day].update(unique_kws)
                    for kw in unique_kws:
                        keyword_freq[kw] += 1
                    for i in range(len(unique_kws)):
                        for j in range(i + 1, len(unique_kws)):
                            pair = tuple(sorted((unique_kws[i], unique_kws[j])))
                            co_occurrence[pair] += 1

            word_counts = Counter(all_keywords)
            word_cloud = [{"name": k, "value": v} for k, v in word_counts.most_common(50)]
            neg_keywords = [k for k, _ in Counter(negative_keywords).most_common(10)]

            top_pairs = sorted(co_occurrence.items(), key=lambda x: x[1], reverse=True)[:80]
            node_names = set()
            links = []
            for (a, b), cnt in top_pairs:
                node_names.add(a)
                node_names.add(b)
                links.append({"source": a, "target": b, "value": cnt})
            nodes = [
                {"name": kw, "value": int(keyword_freq.get(kw, 0)), "symbolSize": min(60, 10 + int(keyword_freq.get(kw, 0)) * 2)}
                for kw in node_names
            ]
            correlation = {"nodes": nodes, "links": links}

            top_keywords = [k for k, _ in sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:8]]
            freq_trend = {
                "dates": sorted_dates,
                "series": [{"name": kw, "type": "line", "smooth": True, "showSymbol": False, "data": [daily_kw_freq[d][kw] for d in sorted_dates]} for kw in top_keywords],
            }

            # ä¿®æ­£æ¯æ—¥æƒ…æ„Ÿè¶‹åŠ¿æ•°æ® (å¯¹é½ sorted_dates)
            sentiment_trend = {
                "dates": sorted_dates,
                "series": [
                    {"name": "æ­£é¢", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["æ­£é¢"] for d in sorted_dates]},
                    {"name": "ä¸­ç«‹", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["ä¸­ç«‹"] for d in sorted_dates]},
                    {"name": "è´Ÿé¢", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["è´Ÿé¢"] for d in sorted_dates]},
                    {
                        "name": "å¹³å‡æƒ…æ„Ÿåˆ†",
                        "type": "line",
                        "yAxisIndex": 1,
                        "smooth": True,
                        "showSymbol": False,
                        "data": [
                            (daily_sentiment[d]["score_sum"] / daily_sentiment[d]["score_count"]) if daily_sentiment[d]["score_count"] else 50
                            for d in sorted_dates
                        ],
                    },
                ],
            }

            # -------------------------------------------------------------------------
            # 7. Top News (Limit 10)
            # -------------------------------------------------------------------------
            top_stmt = (
                select(News)
                .options(defer(News.content), defer(News.embedding))
                .order_by(desc(News.heat_score))
                .limit(10)
            )
            if filters:
                top_stmt = top_stmt.where(and_(*filters))
            
            top_res = await db.execute(top_stmt)
            top_news_list = top_res.scalars().all()
            
            top_news = []
            for n in top_news_list:
                top_news.append(
                    {
                        "id": n.id,
                        "title": n.title,
                        "url": n.url,
                        "source": n.source,
                        "heat": n.heat_score,
                        "time": n.publish_date.isoformat() if n.publish_date else "",
                        "summary": n.summary,
                        "sources": n.sources,
                        "category": n.category,
                        "region": n.region,
                        "sentiment_label": n.sentiment_label,
                        "sentiment_score": n.sentiment_score,
                    }
                )



            ai_analysis = ""
            if generate_ai:
                try:
                    scope_parts = []
                    if keyword:
                        scope_parts.append(f"å…³é”®è¯={keyword}")
                    if category and category != "all":
                        scope_parts.append(f"é¢†åŸŸ={category}")
                    if region and region != "all":
                        scope_parts.append(f"åœ°åŒº={region}")
                    if source and source != "all":
                        scope_parts.append(f"æ¥æº={source}")
                    scope_str = "ï¼›".join(scope_parts) if scope_parts else "å…¨é‡æ ·æœ¬"

                    # AI åˆ†æçš„æ—¶é—´èŒƒå›´ logic optimization
                    ai_start = start_date if start_date else (max_date.date() - timedelta(days=0)).strftime("%Y-%m-%d")
                    ai_end = end_date if end_date else max_date.date().strftime("%Y-%m-%d")
                    
                    time_range_label = f"{ai_start} è‡³ {ai_end}" if ai_start != ai_end else ai_end

                    ai_filters = self._build_news_filters(
                        keyword=keyword,
                        start_date=ai_start,
                        end_date=ai_end,
                        category=category,
                        region=region,
                        source=source,
                    )

                    ai_stmt = (
                        select(
                            News.title,
                            func.substr(News.content, 1, 1000).label("content"),
                            News.summary,
                            News.heat_score,
                            News.source,
                            News.url,
                            News.publish_date,
                        )
                        .select_from(News)
                        .order_by(desc(News.heat_score), desc(News.publish_date))
                        .limit(100)
                    )
                    if ai_filters:
                        ai_stmt = ai_stmt.where(and_(*ai_filters))

                    ai_result = await db.execute(ai_stmt)
                    ai_rows = ai_result.all()

                    def compact_text(text: Any, max_len: int = 180) -> str:
                        t = (text or "").replace("\r", " ").replace("\n", " ").strip()
                        if len(t) > max_len:
                            return t[:max_len] + "â€¦"
                        return t

                    news_lines = []
                    for idx, (title, content, summary, heat, src, url, pub_dt) in enumerate(ai_rows, start=1):
                        body = content if content else summary
                        body_str = compact_text(body, 220)
                        title_str = compact_text(title, 80)
                        src_str = compact_text(src, 30)
                        time_str = pub_dt.isoformat() if pub_dt else ""
                        news_lines.append(
                            f"{idx}. [çƒ­åº¦:{(heat or 0):.1f}] [{src_str}] {title_str}\n   æ—¶é—´: {time_str}\n   æ­£æ–‡: {body_str}\n   é“¾æ¥: {url}"
                        )

                    # --- AI æç¤ºè¯åˆ†æµé€»è¾‘ ---
                    # åœºæ™¯1ï¼šå…³é”®è¯æ·±åº¦åˆ†æ (Keyword Depth Analysis)
                    if keyword:
                        prompt = prompt_manager.get_user_prompt(
                            "report_keyword_analysis",
                            keyword=keyword,
                            time_range_label=time_range_label,
                            scope_str=scope_str,
                            news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬"
                        )
                    # åœºæ™¯2ï¼šå…¨å±€/å¤§ç›˜ç»¼è¿° (Global Overview)
                    else:
                        prompt = prompt_manager.get_user_prompt(
                            "report_global_analysis",
                            time_range_label=time_range_label,
                            scope_str=scope_str,
                            news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬"
                        )

                    ai_analysis = await ai_service.chat_completion(prompt, route_key="REPORT")
                except Exception as e:
                    logger.error(f"AIåˆ†æå¤±è´¥: {e}")
                    ai_analysis = "AI åˆ†æå¤±è´¥"
            else:
                ai_analysis = "æœªå¼€å¯ AI åˆ†æ"

            result_data = {
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "region": region,
                    "source": source,
                    "limit": limit
                },
                "summary": {
                    "total": int(real_total_count or 0),
                    "avg_heat": float(avg_heat),
                    "max_heat": float(max_heat),
                    "sentiment_idx": float(sentiment_idx),
                    "risk_count": int(risk_count),
                    "time_range": time_range,
                },
                "charts": {
                    "trend": chart_trend,
                    "source": chart_source,
                    "word_cloud": word_cloud,
                    "sentiment_dist": sentiment_dist,
                    "neg_keywords": neg_keywords,
                    "correlation": correlation,
                    "freq_trend": freq_trend,
                    "sentiment_trend": sentiment_trend,
                },
                "top_news": top_news,
                "ai_analysis": ai_analysis,
            }
            
            # æ˜¾å¼é‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢å¤§é‡ News å¯¹è±¡æ»ç•™
            if 'news_list' in locals():
                del news_list
            gc.collect()
            
            return result_data

    async def get_chart_data(
        self,
        type: str,
        category: Optional[str] = None,
        region: Optional[str] = None,
        q: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Any:
        """
        è¾“å…¥:
        - `type`: å›¾è¡¨ç±»å‹
        - `category`/`region`: è¿‡æ»¤æ¡ä»¶
        - `q`: å…³é”®è¯
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸ

        è¾“å‡º:
        - å¯¹åº”å›¾è¡¨çš„æ•°æ®ç»“æ„

        ä½œç”¨:
        - ä¸ºå‰ç«¯æŒ‰éœ€åˆ·æ–°å•ä¸€å›¾è¡¨æä¾›æ•°æ®è£å‰ª
        """

        if type == "source":
            cache_key = ("source", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_source_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "sentiment":
            cache_key = ("sentiment", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_sentiment_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "list":
            cache_key = ("list", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 15:
                return cached[1]

            data = await self._get_list_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
                limit=50,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "word_cloud":
            cache_key = ("word_cloud", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_word_cloud_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        data = await self.get_analysis_data(
            keyword=q,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
            limit=None,
            generate_ai=False,
            use_cache=False,
        )

        charts = data.get("charts", {})
        if type == "source":
            return charts.get("source", [])
        if type == "sentiment":
            return {"sentiment_dist": charts.get("sentiment_dist", []), "neg_keywords": charts.get("neg_keywords", [])}
        if type == "list":
            return data.get("top_news", [])
        return charts.get(type, {})


report_service = ReportService()
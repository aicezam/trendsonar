"""
æœ¬æ–‡ä»¶ç”¨äºç”Ÿæˆèˆ†æƒ…æŠ¥è¡¨ï¼ˆå…¨å±€/å…³é”®è¯ï¼‰ï¼Œå¹¶æä¾›æŠ¥è¡¨ç¼“å­˜ã€å†å²è®°å½•ä¸å›¾è¡¨æ•°æ®èšåˆèƒ½åŠ›ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `ReportService`: æŠ¥è¡¨ç”Ÿæˆä¸ç¼“å­˜æœåŠ¡
- `report_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import asyncio
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from time import monotonic
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from sqlalchemy import and_, case, cast, delete, desc, func, literal, or_, select, true
from sqlalchemy.dialects.postgresql import JSONB

from app.core.database import AsyncSessionLocal, check_db_connection
from app.core.logger import logger
from app.core.prompts import prompt_manager
from app.models.news import News
from app.models.report import ReportCache
from app.services.ai_service import ai_service


class ReportService:
    """
    è¾“å…¥:
    - æ•°æ®åº“ä¸­çš„æ–°é—»æ•°æ®ä¸åˆ†ææ¡ä»¶ï¼ˆæ—¶é—´/åˆ†ç±»/åœ°åŒº/æ¥æºç­‰ï¼‰

    è¾“å‡º:
    - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ä¸å†å²ç¼“å­˜è®°å½•

    ä½œç”¨:
    - ç”Ÿæˆèˆ†æƒ…åˆ†ææŠ¥è¡¨ï¼ˆå…¨å±€/å…³é”®è¯ï¼‰ï¼Œå¹¶å†™å…¥æ•°æ®åº“ç¼“å­˜ä¾›å‰ç«¯å±•ç¤º
    """

    def __init__(self) -> None:
        self._chart_cache: Dict[Tuple[Any, ...], Tuple[float, Any]] = {}
        self._global_cache: Optional[Tuple[float, str, Dict[str, Any]]] = None

    def clear_local_cache(self) -> None:
        """
        æ¸…ç©ºæœ¬åœ°å†…å­˜ç¼“å­˜
        """
        self._chart_cache.clear()
        self._global_cache = None
        
    def _cleanup_chart_cache(self) -> None:
        """
        æ¸…ç†å›¾è¡¨ç¼“å­˜ï¼šä¼˜å…ˆç§»é™¤è¿‡æœŸé¡¹ï¼Œè‹¥ä»è¶…é™åˆ™ç§»é™¤æœ€æ—©çš„é¡¹
        """
        now = monotonic()
        # 1. ç§»é™¤è¶…è¿‡ 300s çš„è€æ—§ç¼“å­˜
        keys_to_remove = [k for k, v in self._chart_cache.items() if now - v[0] > 300]
        for k in keys_to_remove:
            self._chart_cache.pop(k, None)

        # 2. è‹¥ä»è¶…é™ï¼Œä¿ç•™æœ€æ–°çš„ 128 æ¡
        if len(self._chart_cache) > 256:
            sorted_items = sorted(self._chart_cache.items(), key=lambda x: x[1][0])
            self._chart_cache = dict(sorted_items[-128:])

    def _build_news_filters(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
    ) -> List[Any]:
        filters: List[Any] = []
        if keyword:
            filters.append(News.title.ilike(f"%{keyword}%"))
        if category and category != "all":
            if "," in category:
                filters.append(News.category.in_(category.split(",")))
            else:
                filters.append(News.category == category)
        if region and region != "all":
            selected_regions = region.split(",")
            region_conditions = [News.region.ilike(f"%{r}%") for r in selected_regions]
            filters.append(or_(*region_conditions))
        if source and source != "all":
            if "," in source:
                filters.append(News.source.in_(source.split(",")))
            else:
                filters.append(News.source == source)
        if start_date:
            try:
                s_dt = datetime.strptime(start_date, "%Y-%m-%d")
                filters.append(News.publish_date >= s_dt)
            except Exception:
                pass
        if end_date:
            try:
                e_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
                filters.append(News.publish_date < e_dt)
            except Exception:
                pass
        return filters

    async def _get_word_cloud_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        try:
            jsonb_keywords = case(
                (func.jsonb_typeof(cast(News.keywords, JSONB)) == "array", cast(News.keywords, JSONB)),
                else_=cast(literal("[]"), JSONB),
            )
            kw = func.jsonb_array_elements_text(jsonb_keywords).table_valued("value").alias("kw")

            async with AsyncSessionLocal() as db:
                stmt = (
                    select(kw.c.value.label("name"), func.count().label("value"))
                    .select_from(News)
                    .join(kw, true())
                )
                if filters:
                    stmt = stmt.where(and_(*filters))
                stmt = stmt.where(func.length(func.trim(kw.c.value)) > 0)
                stmt = stmt.where(func.lower(func.trim(kw.c.value)).notin_(["æ— å†…å®¹", "null", "ç©º", "none", ""]))
                stmt = stmt.group_by(kw.c.value).order_by(desc("value")).limit(50)

                result = await db.execute(stmt)
                rows = result.all()

            data = [{"name": str(name), "value": int(value)} for name, value in rows]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(è¯äº‘)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | æ•°é‡={len(data)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data
        except Exception:
            async with AsyncSessionLocal() as db:
                stmt = select(News.keywords)
                if filters:
                    stmt = stmt.where(and_(*filters))
                result = await db.execute(stmt)
                keywords_values = result.scalars().all()

            all_keywords: List[str] = []
            for kws in keywords_values:
                if not kws or not isinstance(kws, list):
                    continue
                valid_kws = [
                    k.strip()
                    for k in kws
                    if k
                    and isinstance(k, str)
                    and k.strip()
                    and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                ]
                all_keywords.extend(valid_kws)

            word_counts = Counter(all_keywords)
            data = [{"name": k, "value": v} for k, v in word_counts.most_common(50)]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(è¯äº‘)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | è¡Œæ•°={len(keywords_values)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data

    async def _get_source_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        async with AsyncSessionLocal() as db:
            stmt = select(News.source.label("name"), func.count().label("value")).select_from(News)
            if filters:
                stmt = stmt.where(and_(*filters))
            stmt = stmt.where(func.length(func.trim(News.source)) > 0)
            stmt = stmt.group_by(News.source).order_by(desc("value")).limit(10)
            result = await db.execute(stmt)
            rows = result.all()

        data = [{"name": str(name), "value": int(value)} for name, value in rows]
        elapsed = monotonic() - t0
        if elapsed > 0.5:
            logger.info(
                f"å›¾è¡¨æ•°æ®(æ¥æº)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | æ•°é‡={len(data)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
            )
        return data

    async def _get_sentiment_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
    ) -> Dict[str, Any]:
        if not await check_db_connection(verbose=False):
             return {"sentiment_dist": [], "neg_keywords": []}

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        label_expr = case(
            (News.sentiment_label.in_(["æ­£é¢", "ä¸­ç«‹", "è´Ÿé¢"]), News.sentiment_label),
            else_=literal("ä¸­ç«‹"),
        ).label("name")

        try:
            jsonb_keywords = case(
                (func.jsonb_typeof(cast(News.keywords, JSONB)) == "array", cast(News.keywords, JSONB)),
                else_=cast(literal("[]"), JSONB),
            )
            kw = func.jsonb_array_elements_text(jsonb_keywords).table_valued("value").alias("kw")

            async with AsyncSessionLocal() as db:
                stmt_dist = select(label_expr, func.count().label("value")).select_from(News)
                if filters:
                    stmt_dist = stmt_dist.where(and_(*filters))
                stmt_dist = stmt_dist.group_by(label_expr)
                dist_rows = (await db.execute(stmt_dist)).all()

                stmt_neg = select(kw.c.value.label("name"), func.count().label("value")).select_from(News).join(kw, true())
                if filters:
                    stmt_neg = stmt_neg.where(and_(*filters))
                stmt_neg = stmt_neg.where(News.sentiment_label == "è´Ÿé¢")
                stmt_neg = stmt_neg.where(func.length(func.trim(kw.c.value)) > 0)
                stmt_neg = stmt_neg.where(func.lower(func.trim(kw.c.value)).notin_(["æ— å†…å®¹", "null", "ç©º", "none", ""]))
                stmt_neg = stmt_neg.group_by(kw.c.value).order_by(desc("value")).limit(10)
                neg_rows = (await db.execute(stmt_neg)).all()

            dist_map = {str(name): int(value) for name, value in dist_rows}
            sentiment_dist = [
                {"name": "æ­£é¢", "value": int(dist_map.get("æ­£é¢", 0))},
                {"name": "ä¸­ç«‹", "value": int(dist_map.get("ä¸­ç«‹", 0))},
                {"name": "è´Ÿé¢", "value": int(dist_map.get("è´Ÿé¢", 0))},
            ]
            neg_keywords = [str(name) for name, _ in neg_rows]

            data = {"sentiment_dist": sentiment_dist, "neg_keywords": neg_keywords}
            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(æƒ…æ„Ÿåˆ†å¸ƒ)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | negN={len(neg_keywords)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return data
        except Exception:
            async with AsyncSessionLocal() as db:
                stmt = select(News.sentiment_label, News.keywords).select_from(News)
                if filters:
                    stmt = stmt.where(and_(*filters))
                result = await db.execute(stmt)
                rows = result.all()

            sentiment_counts = {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0}
            negative_keywords: List[str] = []
            for label, kws in rows:
                label_str = label if label in sentiment_counts else "ä¸­ç«‹"
                sentiment_counts[label_str] += 1
                if label_str == "è´Ÿé¢" and kws and isinstance(kws, list):
                    valid_kws = [
                        k.strip()
                        for k in kws
                        if k
                        and isinstance(k, str)
                        and k.strip()
                        and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                    ]
                    negative_keywords.extend(valid_kws)

            sentiment_dist = [
                {"name": "æ­£é¢", "value": sentiment_counts["æ­£é¢"]},
                {"name": "ä¸­ç«‹", "value": sentiment_counts["ä¸­ç«‹"]},
                {"name": "è´Ÿé¢", "value": sentiment_counts["è´Ÿé¢"]},
            ]
            neg_keywords = [k for k, _ in Counter(negative_keywords).most_common(10)]

            elapsed = monotonic() - t0
            if elapsed > 0.5:
                logger.info(
                    f"å›¾è¡¨æ•°æ®(æƒ…æ„Ÿ)æ…¢æŸ¥è¯¢: {elapsed:.2f}s | è¡Œæ•°={len(rows)} | åˆ†ç±»={category or ''} | èŒƒå›´={start_date or ''}~{end_date or ''}"
                )
            return {"sentiment_dist": sentiment_dist, "neg_keywords": neg_keywords}

    async def _get_list_chart_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        if not await check_db_connection(verbose=False):
            return []

        t0 = monotonic()
        filters = self._build_news_filters(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
        )

        async with AsyncSessionLocal() as db:
            stmt = (
                select(
                    News.id,
                    News.title,
                    News.url,
                    News.source,
                    News.heat_score,
                    News.publish_date,
                    News.summary,
                    News.sources,
                    News.category,
                    News.region,
                    News.sentiment_label,
                    News.sentiment_score,
                )
                .select_from(News)
                .order_by(desc(News.heat_score), desc(News.publish_date))
                .limit(limit)
            )
            if filters:
                stmt = stmt.where(and_(*filters))
            result = await db.execute(stmt)
            rows = result.all()

        data = []
        for (
            nid,
            title,
            url,
            source,
            heat_score,
            publish_date,
            summary,
            sources,
            cat,
            reg,
            sentiment_label,
            sentiment_score,
        ) in rows:
            data.append(
                {
                    "id": int(nid),
                    "title": title,
                    "url": url,
                    "source": source,
                    "heat": heat_score,
                    "time": publish_date.isoformat() if publish_date else None,
                    "summary": summary,
                    "sources": sources,
                    "category": cat,
                    "region": reg,
                    "sentiment_label": sentiment_label,
                    "sentiment_score": sentiment_score,
                }
            )

        elapsed = monotonic() - t0
        if elapsed > 0.5:
            logger.info(
                f"chart-data list æ…¢æŸ¥è¯¢: {elapsed:.2f}s | items={len(data)} | category={category or ''} | range={start_date or ''}~{end_date or ''}"
            )
        return data

    async def generate_and_cache_global_report(self, period: str = "weekly") -> None:
        """
        è¾“å…¥:
        - `period`: å‘¨æœŸï¼ˆdaily/weekly/monthlyï¼‰

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - ç”ŸæˆæŒ‡å®šå‘¨æœŸçš„å…¨å±€æŠ¥è¡¨å¹¶å†™å…¥ç¼“å­˜
        """
        
        if not await check_db_connection(verbose=False):
            logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡ç”Ÿæˆå…¨å±€æŠ¥è¡¨ç¼“å­˜")
            return

        logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆå…¨å±€å¤§ç›˜æŠ¥è¡¨ç¼“å­˜ ({period})...")
        try:
            today = datetime.now()
            end_date_str = today.strftime("%Y-%m-%d")

            if period == "daily":
                start_date_str = today.strftime("%Y-%m-%d")
            elif period == "monthly":
                start_date_str = today.replace(day=1).strftime("%Y-%m-%d")
            else:
                start_date_str = (today - timedelta(days=6)).strftime("%Y-%m-%d")

            data = await self._generate_analysis_data(
                keyword="", start_date=start_date_str, end_date=end_date_str, generate_ai=True
            )

            await self.save_report_cache("global", period, data)
            logger.info(f"âœ… å…¨å±€æŠ¥è¡¨ç¼“å­˜ ({period}) å·²æ›´æ–°")
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥è¡¨ç¼“å­˜å¤±è´¥: {e}")

    async def save_report_cache(self, r_type: str, keyword: str, data: Dict[str, Any]) -> None:
        """
        è¾“å…¥:
        - `r_type`: æŠ¥è¡¨ç±»å‹ï¼ˆglobal/keywordï¼‰
        - `keyword`: å…³é”®è¯æˆ–å‘¨æœŸæ ‡è¯†ï¼ˆglobal æ—¶ç”¨ daily/weekly/monthlyï¼‰
        - `data`: æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - å†™å…¥æŠ¥è¡¨ç¼“å­˜ï¼Œå¹¶å¯¹æ•°é‡ä¸åŒæ—¥é‡å¤ç¼“å­˜è¿›è¡Œæ§åˆ¶
        """
        if not await check_db_connection(verbose=False):
            return

        async with AsyncSessionLocal() as db:
            if r_type == "global":
                today_start = datetime.combine(datetime.now().date(), datetime.min.time())
                await db.execute(
                    delete(ReportCache).where(
                        ReportCache.report_type == "global",
                        ReportCache.keyword == keyword,
                        ReportCache.created_at >= today_start,
                    )
                )

            if r_type == "keyword" and keyword:
                stmt = (
                    select(ReportCache.id)
                    .where(ReportCache.report_type == "keyword", ReportCache.keyword == keyword)
                    .order_by(desc(ReportCache.created_at))
                )
                result = await db.execute(stmt)
                ids = result.scalars().all()

                if len(ids) >= 10:
                    ids_to_delete = ids[9:]
                    await db.execute(delete(ReportCache).where(ReportCache.id.in_(ids_to_delete)))

            cache = ReportCache(report_type=r_type, keyword=keyword, data=data, created_at=datetime.now())
            db.add(cache)
            await db.commit()

    async def get_recent_reports(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `limit`: è¿”å›æ•°é‡ä¸Šé™

        è¾“å‡º:
        - æœ€è¿‘å…³é”®è¯æŠ¥è¡¨åˆ—è¡¨ï¼ˆæŒ‰å…³é”®è¯å»é‡ï¼‰

        ä½œç”¨:
        - ä¸ºå‰ç«¯å±•ç¤ºæœ€è¿‘ç”Ÿæˆçš„å…³é”®è¯æŠ¥è¡¨å…¥å£
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = (
                select(ReportCache)
                .where(ReportCache.report_type == "keyword")
                .order_by(desc(ReportCache.created_at))
                .limit(limit * 5)
            )
            result = await db.execute(stmt)
            reports = result.scalars().all()

            seen = set()
            recent = []
            for r in reports:
                if r.keyword and r.keyword not in seen:
                    recent.append({"keyword": r.keyword, "date": r.created_at.strftime("%Y-%m-%d %H:%M"), "id": r.id})
                    seen.add(r.keyword)
                    if len(recent) >= limit:
                        break
            return recent

    async def get_report_history(self, keyword: str) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯

        è¾“å‡º:
        - è¯¥å…³é”®è¯ä¸‹çš„å†å²æŠ¥è¡¨è®°å½•åˆ—è¡¨

        ä½œç”¨:
        - æ”¯æŒå…³é”®è¯æŠ¥è¡¨å†å²å›æº¯
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = (
                select(ReportCache)
                .where(ReportCache.report_type == "keyword", ReportCache.keyword == keyword)
                .order_by(desc(ReportCache.created_at))
            )
            result = await db.execute(stmt)
            reports = result.scalars().all()
            return [{"id": r.id, "date": r.created_at.strftime("%Y-%m-%d %H:%M")} for r in reports]

    async def get_global_history(self) -> List[Dict[str, Any]]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å…¨å±€æŠ¥è¡¨å†å²è®°å½•åˆ—è¡¨

        ä½œç”¨:
        - æ”¯æŒå…¨å±€æŠ¥è¡¨å†å²å›æº¯
        """
        if not await check_db_connection(verbose=False):
            return []

        async with AsyncSessionLocal() as db:
            stmt = (
                select(ReportCache)
                .where(ReportCache.report_type == "global")
                .order_by(desc(ReportCache.created_at))
                .limit(100)
            )
            result = await db.execute(stmt)
            reports = result.scalars().all()
            return [
                {"id": r.id, "date": r.created_at.strftime("%Y-%m-%d %H:%M"), "type": r.keyword or "weekly"}
                for r in reports
            ]

    async def load_report(self, report_id: int) -> Optional[Dict[str, Any]]:
        """
        è¾“å…¥:
        - `report_id`: æŠ¥è¡¨ç¼“å­˜ ID

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼›ä¸å­˜åœ¨è¿”å› None

        ä½œç”¨:
        - è¯»å–æŒ‡å®šå†å²æŠ¥è¡¨ç¼“å­˜ä¾›å‰ç«¯æ¸²æŸ“
        """
        if not await check_db_connection(verbose=False):
            return None

        async with AsyncSessionLocal() as db:
            result = await db.execute(select(ReportCache).where(ReportCache.id == report_id))
            cached = result.scalar_one_or_none()
            if cached:
                data = cached.data
                if cached.report_type == "global":
                    return {**data, "period": cached.keyword or "weekly"}
                return data
            return None


    async def get_analysis_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
        limit: Optional[int] = None,
        generate_ai: bool = False,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯ï¼ˆä¸ºç©ºä»£è¡¨å…¨å±€ï¼‰
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        - `category`/`region`/`source`: è¿‡æ»¤æ¡ä»¶
        - `limit`: å–æ ·ä¸Šé™
        - `generate_ai`: æ˜¯å¦ç”Ÿæˆ AI æ–‡æœ¬åˆ†æ
        - `use_cache`: æ˜¯å¦ä¼˜å…ˆè¯»å–ç¼“å­˜

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼ˆsummary/charts/top_news/ai_analysisï¼‰

        ä½œç”¨:
        - å¯¹å¤–æä¾›ç»Ÿä¸€æŠ¥è¡¨æ•°æ®å…¥å£ï¼Œå¹¶åœ¨æ¡ä»¶å…è®¸æ—¶å‘½ä¸­ç¼“å­˜æå‡æ€§èƒ½
        """

        if use_cache and not keyword and not start_date and not end_date and not category and not region and not source:
            now_ts = monotonic()
            if self._global_cache and (now_ts - self._global_cache[0]) <= 15:
                return self._global_cache[2]
            
            if await check_db_connection(verbose=False):
                async with AsyncSessionLocal() as db:
                    t0 = monotonic()
                    stmt = (
                        select(ReportCache.keyword, ReportCache.data)
                        .where(ReportCache.report_type == "global", ReportCache.keyword == "daily")
                        .order_by(desc(ReportCache.created_at))
                        .limit(1)
                    )
                    result = await db.execute(stmt)
                    row = result.first()

                    if not row:
                        stmt = (
                            select(ReportCache.keyword, ReportCache.data)
                            .where(ReportCache.report_type == "global")
                            .order_by(desc(ReportCache.created_at))
                            .limit(1)
                        )
                        result = await db.execute(stmt)
                        row = result.first()

                    if row:
                        kw, data = row
                        elapsed_ms = int((monotonic() - t0) * 1000)
                        logger.info(f"ğŸ“– è¯»å–å…¨å±€æŠ¥è¡¨æ•°æ®åº“ç¼“å­˜ ({kw or 'æœªçŸ¥'}) {elapsed_ms}ms")
                        self._global_cache = (monotonic(), str(kw or ""), data)
                        return data

        data = await self._generate_analysis_data(keyword, start_date, end_date, category, region, source, limit, generate_ai)

        if keyword:
            await self.save_report_cache("keyword", keyword, data)

        return data

    async def _generate_analysis_data(
        self,
        keyword: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        category: Optional[str] = None,
        region: Optional[str] = None,
        source: Optional[str] = None,
        limit: Optional[int] = None,
        generate_ai: bool = False,
    ) -> Dict[str, Any]:
        """
        è¾“å…¥:
        - `keyword`: å…³é”®è¯è¿‡æ»¤
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸè¿‡æ»¤
        - `category`/`region`/`source`: ç»´åº¦è¿‡æ»¤
        - `limit`: æ•°æ®é‡ä¸Šé™
        - `generate_ai`: æ˜¯å¦ç”Ÿæˆ AI åˆ†ææ–‡å­—

        è¾“å‡º:
        - æŠ¥è¡¨ç»“æ„åŒ–æ•°æ®ï¼ˆæ‘˜è¦ã€è¶‹åŠ¿ã€æ¥æºã€è¯äº‘ã€æƒ…ç»ªåˆ†å¸ƒã€Top æ–°é—»ç­‰ï¼‰

        ä½œç”¨:
        - åœ¨æ•°æ®åº“ä¸­æŒ‰æ¡ä»¶æŸ¥è¯¢æ–°é—»ï¼Œå¹¶èšåˆè®¡ç®—å„ç±»ç»Ÿè®¡æŒ‡æ ‡
        """
        
        # ç»Ÿä¸€è¿”å›ç©ºæ•°æ®çš„é—­åŒ…å‡½æ•°
        def empty_result(msg: str = "æ•°æ®åº“è¿æ¥ä¸å¯ç”¨"):
             return {
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "region": region,
                    "source": source,
                    "limit": limit
                },
                "summary": {"total": 0, "avg_heat": 0, "max_heat": 0, "sentiment_idx": 0, "risk_count": 0, "time_range": "-"},
                    "charts": {
                        "trend": {},
                        "source": {},
                        "word_cloud": [],
                        "sentiment_dist": [],
                        "neg_keywords": [],
                        "correlation": [],
                        "freq_trend": [],
                        "sentiment_trend": [],
                    },
                    "top_news": [],
                    "ai_analysis": f"{msg}ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚",
                }

        if not await check_db_connection(verbose=False):
            return empty_result()

        async with AsyncSessionLocal() as db:
            stmt = select(News)
            filters = []

            if keyword:
                filters.append(News.title.ilike(f"%{keyword}%"))
            if category and category != "all":
                if "," in category:
                    filters.append(News.category.in_(category.split(",")))
                else:
                    filters.append(News.category == category)
            if region and region != "all":
                selected_regions = region.split(",")
                region_conditions = [News.region.ilike(f"%{r}%") for r in selected_regions]
                filters.append(or_(*region_conditions))
            if source and source != "all":
                if "," in source:
                    filters.append(News.source.in_(source.split(",")))
                else:
                    filters.append(News.source == source)
            if start_date:
                try:
                    s_dt = datetime.strptime(start_date, "%Y-%m-%d")
                    filters.append(News.publish_date >= s_dt)
                except Exception:
                    pass
            if end_date:
                try:
                    e_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
                    filters.append(News.publish_date < e_dt)
                except Exception:
                    pass

            count_stmt = select(func.count()).select_from(News)
            if filters:
                stmt = stmt.where(and_(*filters))
                count_stmt = count_stmt.where(and_(*filters))

            real_total_count = await db.scalar(count_stmt)

            if limit and limit > 0:
                stmt = stmt.order_by(desc(News.heat_score)).limit(limit)
            else:
                stmt = stmt.order_by(desc(News.publish_date))

            result = await db.execute(stmt)
            news_list = result.scalars().all()

            if not news_list:
                return {
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "region": region,
                    "source": source,
                    "limit": limit
                },
                "summary": {"total": 0, "avg_heat": 0, "max_heat": 0, "sentiment_idx": 0, "risk_count": 0, "time_range": "-"},
                    "charts": {
                        "trend": {},
                        "source": {},
                        "word_cloud": [],
                        "sentiment_dist": [],
                        "neg_keywords": [],
                        "correlation": [],
                        "freq_trend": [],
                        "sentiment_trend": [],
                    },
                    "top_news": [],
                    "ai_analysis": "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚",
                }

            heats = [n.heat_score for n in news_list]
            avg_heat = sum(heats) / len(news_list) if news_list else 0
            max_heat = max(heats) if heats else 0

            sentiment_scores = [n.sentiment_score for n in news_list if n.sentiment_score is not None]
            sentiment_idx = (sum(sentiment_scores) / len(sentiment_scores)) if sentiment_scores else 50
            risk_count = sum(1 for n in news_list if n.sentiment_label == "è´Ÿé¢")

            min_date = min(n.publish_date for n in news_list)
            max_date = max(n.publish_date for n in news_list)
            time_range = f"{min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}"

            trend_map = defaultdict(lambda: {"count": 0, "heat_sum": 0.0, "categories": Counter()})
            source_map = Counter()

            all_keywords = []
            negative_keywords = []
            sentiment_counts = {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0}

            co_occurrence = defaultdict(int)
            keyword_freq = defaultdict(int)
            daily_kw_freq = defaultdict(Counter)
            daily_sentiment = defaultdict(lambda: {"æ­£é¢": 0, "ä¸­ç«‹": 0, "è´Ÿé¢": 0, "score_sum": 0.0, "score_count": 0})

            for n in news_list:
                day = n.publish_date.strftime("%Y-%m-%d")
                trend_map[day]["count"] += 1
                trend_map[day]["heat_sum"] += n.heat_score or 0.0
                trend_map[day]["categories"][n.category or "å…¶ä»–"] += 1

                if n.source:
                    source_map[n.source] += 1

                label = n.sentiment_label if n.sentiment_label in sentiment_counts else "ä¸­ç«‹"
                sentiment_counts[label] += 1
                daily_sentiment[day][label] += 1
                if n.sentiment_score is not None:
                    daily_sentiment[day]["score_sum"] += float(n.sentiment_score)
                    daily_sentiment[day]["score_count"] += 1

                if n.keywords and isinstance(n.keywords, list):
                    valid_kws = [
                        k.strip()
                        for k in n.keywords
                        if k and k.strip() and k.strip().lower() not in {"æ— å†…å®¹", "null", "ç©º", "none", ""}
                    ]
                    all_keywords.extend(valid_kws)
                    if label == "è´Ÿé¢":
                        negative_keywords.extend(valid_kws)

                    unique_kws = list(set(valid_kws))
                    daily_kw_freq[day].update(unique_kws)
                    for kw in unique_kws:
                        keyword_freq[kw] += 1
                    for i in range(len(unique_kws)):
                        for j in range(i + 1, len(unique_kws)):
                            pair = tuple(sorted((unique_kws[i], unique_kws[j])))
                            co_occurrence[pair] += 1

            sorted_dates = sorted(trend_map.keys())
            category_names = set()
            for d in sorted_dates:
                category_names.update(trend_map[d]["categories"].keys())
            category_series = [
                {"name": cat, "type": "bar", "stack": "æ–‡ç« æ•°", "data": [trend_map[d]["categories"][cat] for d in sorted_dates]}
                for cat in sorted(category_names)
            ]

            chart_trend = {
                "dates": sorted_dates,
                "counts": [trend_map[d]["count"] for d in sorted_dates],
                "avg_heats": [
                    (trend_map[d]["heat_sum"] / trend_map[d]["count"]) if trend_map[d]["count"] else 0 for d in sorted_dates
                ],
                "category_series": category_series,
            }

            sorted_sources = sorted(source_map.items(), key=lambda x: x[1], reverse=True)
            chart_source = [{"name": k, "value": v} for k, v in sorted_sources[:10]]

            word_counts = Counter(all_keywords)
            word_cloud = [{"name": k, "value": v} for k, v in word_counts.most_common(50)]

            neg_keywords = [k for k, _ in Counter(negative_keywords).most_common(10)]

            sentiment_dist = [
                {"name": "æ­£é¢", "value": sentiment_counts["æ­£é¢"]},
                {"name": "ä¸­ç«‹", "value": sentiment_counts["ä¸­ç«‹"]},
                {"name": "è´Ÿé¢", "value": sentiment_counts["è´Ÿé¢"]},
            ]

            top_pairs = sorted(co_occurrence.items(), key=lambda x: x[1], reverse=True)[:80]
            node_names = set()
            links = []
            for (a, b), cnt in top_pairs:
                node_names.add(a)
                node_names.add(b)
                links.append({"source": a, "target": b, "value": cnt})
            nodes = [
                {"name": kw, "value": int(keyword_freq.get(kw, 0)), "symbolSize": min(60, 10 + int(keyword_freq.get(kw, 0)) * 2)}
                for kw in node_names
            ]
            correlation = {"nodes": nodes, "links": links}

            top_keywords = [k for k, _ in sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:8]]
            freq_trend = {
                "dates": sorted_dates,
                "series": [{"name": kw, "type": "line", "smooth": True, "showSymbol": False, "data": [daily_kw_freq[d][kw] for d in sorted_dates]} for kw in top_keywords],
            }

            sentiment_trend = {
                "dates": sorted_dates,
                "series": [
                    {"name": "æ­£é¢", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["æ­£é¢"] for d in sorted_dates]},
                    {"name": "ä¸­ç«‹", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["ä¸­ç«‹"] for d in sorted_dates]},
                    {"name": "è´Ÿé¢", "type": "bar", "stack": "æƒ…æ„Ÿ", "data": [daily_sentiment[d]["è´Ÿé¢"] for d in sorted_dates]},
                    {
                        "name": "å¹³å‡æƒ…æ„Ÿåˆ†",
                        "type": "line",
                        "yAxisIndex": 1,
                        "smooth": True,
                        "showSymbol": False,
                        "data": [
                            (daily_sentiment[d]["score_sum"] / daily_sentiment[d]["score_count"]) if daily_sentiment[d]["score_count"] else 50
                            for d in sorted_dates
                        ],
                    },
                ],
            }

            top_news = []
            for n in sorted(news_list, key=lambda x: x.heat_score or 0.0, reverse=True)[:10]:
                top_news.append(
                    {
                        "id": n.id,
                        "title": n.title,
                        "url": n.url,
                        "source": n.source,
                        "heat": n.heat_score,
                        "time": n.publish_date.isoformat(),
                        "summary": n.summary,
                        "sources": n.sources,
                        "category": n.category,
                        "region": n.region,
                        "sentiment_label": n.sentiment_label,
                        "sentiment_score": n.sentiment_score,
                    }
                )

            ai_analysis = ""
            if generate_ai:
                try:
                    scope_parts = []
                    if keyword:
                        scope_parts.append(f"å…³é”®è¯={keyword}")
                    if category and category != "all":
                        scope_parts.append(f"é¢†åŸŸ={category}")
                    if region and region != "all":
                        scope_parts.append(f"åœ°åŒº={region}")
                    if source and source != "all":
                        scope_parts.append(f"æ¥æº={source}")
                    scope_str = "ï¼›".join(scope_parts) if scope_parts else "å…¨é‡æ ·æœ¬"

                    # AI åˆ†æçš„æ—¶é—´èŒƒå›´ logic optimization
                    ai_start = start_date if start_date else (max_date.date() - timedelta(days=0)).strftime("%Y-%m-%d")
                    ai_end = end_date if end_date else max_date.date().strftime("%Y-%m-%d")
                    
                    time_range_label = f"{ai_start} è‡³ {ai_end}" if ai_start != ai_end else ai_end

                    ai_filters = self._build_news_filters(
                        keyword=keyword,
                        start_date=ai_start,
                        end_date=ai_end,
                        category=category,
                        region=region,
                        source=source,
                    )

                    ai_stmt = (
                        select(
                            News.title,
                            News.content,
                            News.summary,
                            News.heat_score,
                            News.source,
                            News.url,
                            News.publish_date,
                        )
                        .select_from(News)
                        .order_by(desc(News.heat_score), desc(News.publish_date))
                        .limit(100)
                    )
                    if ai_filters:
                        ai_stmt = ai_stmt.where(and_(*ai_filters))

                    ai_result = await db.execute(ai_stmt)
                    ai_rows = ai_result.all()

                    def compact_text(text: Any, max_len: int = 180) -> str:
                        t = (text or "").replace("\r", " ").replace("\n", " ").strip()
                        if len(t) > max_len:
                            return t[:max_len] + "â€¦"
                        return t

                    news_lines = []
                    for idx, (title, content, summary, heat, src, url, pub_dt) in enumerate(ai_rows, start=1):
                        body = content if content else summary
                        body_str = compact_text(body, 220)
                        title_str = compact_text(title, 80)
                        src_str = compact_text(src, 30)
                        time_str = pub_dt.isoformat() if pub_dt else ""
                        news_lines.append(
                            f"{idx}. [çƒ­åº¦:{(heat or 0):.1f}] [{src_str}] {title_str}\n   æ—¶é—´: {time_str}\n   æ­£æ–‡: {body_str}\n   é“¾æ¥: {url}"
                        )

                    # --- AI æç¤ºè¯åˆ†æµé€»è¾‘ ---
                    # åœºæ™¯1ï¼šå…³é”®è¯æ·±åº¦åˆ†æ (Keyword Depth Analysis)
                    if keyword:
                        prompt = prompt_manager.get_user_prompt(
                            "report_keyword_analysis",
                            keyword=keyword,
                            time_range_label=time_range_label,
                            scope_str=scope_str,
                            news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬"
                        )
                    # åœºæ™¯2ï¼šå…¨å±€/å¤§ç›˜ç»¼è¿° (Global Overview)
                    else:
                        prompt = prompt_manager.get_user_prompt(
                            "report_global_analysis",
                            time_range_label=time_range_label,
                            scope_str=scope_str,
                            news_lines="\n\n".join(news_lines) if news_lines else "æ— å¯ç”¨æ–°é—»æ ·æœ¬"
                        )

                    ai_analysis = await ai_service.chat_completion(prompt, route_key="REPORT")
                except Exception as e:
                    logger.error(f"AIåˆ†æå¤±è´¥: {e}")
                    ai_analysis = "AI åˆ†æå¤±è´¥"
            else:
                ai_analysis = "æœªå¼€å¯ AI åˆ†æ"

            return {
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "region": region,
                    "source": source,
                    "limit": limit
                },
                "summary": {
                    "total": int(real_total_count or 0),
                    "avg_heat": float(avg_heat),
                    "max_heat": float(max_heat),
                    "sentiment_idx": float(sentiment_idx),
                    "risk_count": int(risk_count),
                    "time_range": time_range,
                },
                "charts": {
                    "trend": chart_trend,
                    "source": chart_source,
                    "word_cloud": word_cloud,
                    "sentiment_dist": sentiment_dist,
                    "neg_keywords": neg_keywords,
                    "correlation": correlation,
                    "freq_trend": freq_trend,
                    "sentiment_trend": sentiment_trend,
                },
                "top_news": top_news,
                "ai_analysis": ai_analysis,
            }

    async def get_chart_data(
        self,
        type: str,
        category: Optional[str] = None,
        region: Optional[str] = None,
        q: str = "",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Any:
        """
        è¾“å…¥:
        - `type`: å›¾è¡¨ç±»å‹
        - `category`/`region`: è¿‡æ»¤æ¡ä»¶
        - `q`: å…³é”®è¯
        - `start_date`/`end_date`: èµ·æ­¢æ—¥æœŸ

        è¾“å‡º:
        - å¯¹åº”å›¾è¡¨çš„æ•°æ®ç»“æ„

        ä½œç”¨:
        - ä¸ºå‰ç«¯æŒ‰éœ€åˆ·æ–°å•ä¸€å›¾è¡¨æä¾›æ•°æ®è£å‰ª
        """

        if type == "source":
            cache_key = ("source", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_source_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "sentiment":
            cache_key = ("sentiment", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_sentiment_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "list":
            cache_key = ("list", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 15:
                return cached[1]

            data = await self._get_list_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
                limit=50,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        if type == "word_cloud":
            cache_key = ("word_cloud", q or "", start_date or "", end_date or "", category or "", region or "")
            cached = self._chart_cache.get(cache_key)
            if cached and (monotonic() - cached[0]) < 30:
                return cached[1]

            data = await self._get_word_cloud_chart_data(
                keyword=q,
                start_date=start_date,
                end_date=end_date,
                category=category,
                region=region,
            )
            self._chart_cache[cache_key] = (monotonic(), data)
            self._cleanup_chart_cache()
            return data

        data = await self.get_analysis_data(
            keyword=q,
            start_date=start_date,
            end_date=end_date,
            category=category,
            region=region,
            source=None,
            limit=None,
            generate_ai=False,
            use_cache=False,
        )

        charts = data.get("charts", {})
        if type == "source":
            return charts.get("source", [])
        if type == "sentiment":
            return {"sentiment_dist": charts.get("sentiment_dist", []), "neg_keywords": charts.get("neg_keywords", [])}
        if type == "list":
            return data.get("top_news", [])
        return charts.get(type, {})


report_service = ReportService()
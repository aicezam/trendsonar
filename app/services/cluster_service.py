"""
æœ¬æ–‡ä»¶ç”¨äºå®ç°æ–°é—»èšç±»ä¸åˆå¹¶é€»è¾‘ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ä¸å¤§æ¨¡å‹æ ¸éªŒé™ä½é‡å¤æ–°é—»ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `ClusteringService`: èšç±»æœåŠ¡å®ç°
- `cluster_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import json
import gc
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
from sqlalchemy import delete, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import BASE_DIR, get_settings
from app.core.database import AsyncSessionLocal
from app.core.logger import setup_logger
from app.models.news import News
from app.models.clustering_history import ClusteringHistory
from app.services.ai_service import AIService, ai_service

settings = get_settings()
logger = setup_logger("ClusteringService")


class ClusteringService:
    """
    è¾“å…¥:
    - æ•°æ®åº“ä¸­çš„æ–°é—»å‘é‡ä¸ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®

    è¾“å‡º:
    - èšç±»åˆå¹¶åçš„æ–°é—»è®°å½•ï¼ˆæ›´æ–° sources ä¸ heat_scoreï¼‰

    ä½œç”¨:
    - é€šè¿‡å‘é‡ç›¸ä¼¼åº¦ä¸ AI æ ¸éªŒï¼Œå°†åŒä¸€äº‹ä»¶çš„å¤šæ¥æºæ–°é—»åˆå¹¶
    """

    def __init__(self, ai: AIService) -> None:
        """
        è¾“å…¥:
        - `ai`: AI æœåŠ¡å®ä¾‹

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - æ³¨å…¥ AI èƒ½åŠ›ï¼Œç”¨äºå‘é‡ç”Ÿæˆä¸èšç±»æ ¸éªŒ
        """

        self.ai = ai

    def calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è¾“å…¥:
        - `vec1`: å‘é‡1
        - `vec2`: å‘é‡2

        è¾“å‡º:
        - ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ0~1ï¼‰

        ä½œç”¨:
        - è®¡ç®—ä¸¤æ¡æ–°é—»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œç”¨äºå€™é€‰èšç±»åˆ¤æ–­
        """

        if not vec1 or not vec2:
            return 0.0
        v1, v2 = np.array(vec1), np.array(vec2)
        norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return float(np.dot(v1, v2) / (norm1 * norm2))

    def _get_sources_path_candidates(self) -> List[Path]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å¯èƒ½çš„æ–°é—»æºé…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

        ä½œç”¨:
        - è¯»å–æ–°é—»æºæƒé‡æ—¶ï¼Œå…¼å®¹ `data/` ä¸é¡¹ç›®æ ¹ç›®å½•ä¸¤ç§æ”¾ç½®æ–¹å¼
        """

        return [
            BASE_DIR / "data" / "news_sources.json",
            BASE_DIR / "news_sources.json",
        ]

    def _get_source_weight_by_name(self, source_name: str) -> float:
        """
        è¾“å…¥:
        - `source_name`: æ–°é—»æ¥æºåç§°

        è¾“å‡º:
        - æ¥æºæƒé‡ï¼ˆæœªæ‰¾åˆ°è¿”å› 1.0ï¼‰

        ä½œç”¨:
        - ç”¨æ–°é—»æºé…ç½®çš„æƒé‡é‡ç®—åˆå¹¶åçš„æ€»çƒ­åº¦
        """

        try:
            for path in self._get_sources_path_candidates():
                if not path.exists():
                    continue
                with path.open("r", encoding="utf-8") as f:
                    sources: List[Dict] = json.load(f)
                for src in sources:
                    if src.get("name") == source_name:
                        return float(src.get("weight", 1.0))
        except Exception as e:
            logger.error(f"è¯»å–æ–°é—»æºæƒé‡å¤±è´¥: {e}")
        return 1.0

    def _needs_summary_regeneration(self, summary: str) -> bool:
        """
        åˆ¤æ–­æ‘˜è¦æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ (e.g. åŒ…å« HTML æ ‡ç­¾)
        """
        if not summary:
            return False
        # ç®€å•åˆ¤æ–­æ˜¯å¦åŒ…å« HTML æ ‡ç­¾
        return "<" in summary and ">" in summary

    async def execute_clustering(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - å¯¹æ—¶é—´çª—å£å†…æ–°é—»è¿›è¡Œèšç±»åˆå¹¶ï¼Œå¹¶å†™å›æ•°æ®åº“
        """

        logger.info("ğŸ§© å¼€å§‹æ‰§è¡Œèšç±»ä»»åŠ¡...")
        async with AsyncSessionLocal() as db:
            from datetime import datetime, timedelta

            time_window = datetime.now() - timedelta(hours=settings.CLUSTERING_TIME_WINDOW_HOURS)
            logger.info(f"   â±ï¸ èšç±»æ—¶é—´çª—å£: {settings.CLUSTERING_TIME_WINDOW_HOURS}h (Cutoff: {time_window.strftime('%Y-%m-%d %H:%M:%S')})")
            
            result = await db.execute(
                select(News).where(News.publish_date >= time_window).order_by(News.heat_score.desc())
            )
            items = result.scalars().all()
            if not items:
                return

            items_to_embed = [item for item in items if not item.embedding]
            if items_to_embed:
                logger.info(f"   ğŸ§  è®¡ç®— {len(items_to_embed)} æ¡ç¼ºå¤±å‘é‡...")
                titles = [i.title for i in items_to_embed]
                embeddings = await self.ai.get_embeddings(titles)
                for item, emb in zip(items_to_embed, embeddings):
                    if emb:
                        item.embedding = emb
                await db.commit()

            # åŠ è½½å†å²æ ¸éªŒè®°å½• (æ”¹ä¸ºåŠ è½½æœ€è¿‘çš„ 5000 æ¡è®°å½•)
            history_result = await db.execute(
                select(ClusteringHistory.news_id_a, ClusteringHistory.news_id_b)
                .order_by(ClusteringHistory.id.desc())
                .limit(5000)
            )
            failed_pairs = set((row.news_id_a, row.news_id_b) for row in history_result)
            if failed_pairs:
                logger.info(f"   ğŸ“œ å·²åŠ è½½ {len(failed_pairs)} æ¡æœ€è¿‘çš„å†å²æ ¸éªŒè®°å½•")

            pool = []
            for item in items:
                if not item.embedding:
                    continue
                pool.append({"id": item.id, "obj": item, "vec": item.embedding, "merged": False})

            logger.info(f"   ğŸ“Š èšç±»æ± å¤§å°: {len(pool)}")

            candidates: Dict[int, List] = {j: [] for j in range(len(pool))}
            total_items = len(pool)
            logger.info(f"   ğŸ” å¼€å§‹é¢„æ‰«æ {total_items} æ¡æ•°æ®...")

            skipped_count = 0
            for i in range(len(pool)):
                if i > 0 and i % 500 == 0:
                    logger.debug(f"      [é¢„æ‰«æ] å·²å¤„ç† {i}/{total_items} ...")
                for j in range(i + 1, len(pool)):
                    # æ£€æŸ¥æ˜¯å¦åœ¨å†å²æ‹’ç»åˆ—è¡¨ä¸­
                    id_a, id_b = pool[i]["id"], pool[j]["id"]
                    if id_a > id_b:
                        id_a, id_b = id_b, id_a
                    
                    if (id_a, id_b) in failed_pairs:
                        skipped_count += 1
                        continue

                    sim = self.calculate_cosine_similarity(pool[i]["vec"], pool[j]["vec"])
                    if sim >= settings.CLUSTERING_THRESHOLD:
                        candidates[j].append((i, sim))

            total_candidates = sum(len(v) for v in candidates.values())
            processed_candidates = 0
            logger.info(f"   âœ… é¢„æ‰«æå®Œæˆ (å…±å‘ç° {total_candidates} ä¸ªæ½œåœ¨åˆå¹¶å¯¹, ğŸ›¡ï¸ åŸºäºå†å²æ‹¦æˆªè·³è¿‡ {skipped_count} å¯¹)")

            # ------------------------------------------------

            loop_count = 0
            while True:
                loop_count += 1
                if loop_count % 10 == 0:
                    logger.debug(f"      [èšç±»å¾ªç¯] ç¬¬ {loop_count} è½®æ‰«æ...")

                batch_requests = []
                merged_indices_in_this_round = []
                processed_in_this_round = False

                for j in range(len(pool)):
                    follower = pool[j]
                    if follower["merged"]:
                        continue
                    if not candidates[j]:
                        continue

                    leader_idx, sim = candidates[j][0]
                    
                    # å°è¯•æ‰¾åˆ°æœ€ç»ˆçš„ Leader
                    real_l_idx = self._get_active_leader_idx(pool, leader_idx)
                    if real_l_idx is not None and real_l_idx != j:
                        leader_idx = real_l_idx
                    else:
                        candidates[j].pop(0)
                        processed_in_this_round = True
                        continue

                    leader = pool[leader_idx]

                    if leader["merged"]:
                        candidates[j].pop(0)
                        processed_in_this_round = True
                        continue

                    if sim >= 0.98:
                        processed_candidates += len(candidates[j])
                        logger.debug(f"      ğŸ”— é«˜ç›¸ä¼¼åº¦è‡ªåŠ¨åˆå¹¶: [{leader['obj'].title}] <== [{follower['obj'].title}] (Sim: {sim:.4f})")
                        await self._merge_news(db, leader, follower)
                        
                        pool[j]["merged_to"] = leader_idx
                        merged_indices_in_this_round.append(j)
                        
                        candidates[j] = []
                        processed_in_this_round = True
                    else:
                        batch_requests.append(
                            {
                                "leader": leader["obj"].title,
                                "candidate": follower["obj"].title,
                                "leader_idx": leader_idx,
                                "candidate_idx": j,
                            }
                        )
                        # ä½¿ç”¨é…ç½®çš„ batch size
                        if len(batch_requests) >= settings.ANALYSIS_BATCH_SIZE:
                            break

                if not batch_requests and not processed_in_this_round:
                    break

                if batch_requests:
                    req_count = len(batch_requests)
                    current_progress_start = processed_candidates + 1
                    current_progress_end = min(processed_candidates + req_count, total_candidates)
                    logger.info(f"   ğŸ¤– æ‰¹é‡æ ¸éªŒ {req_count} å¯¹ (è¿›åº¦: {current_progress_start}-{current_progress_end}/{total_candidates})...")
                    
                    # æ‰¹é‡è°ƒç”¨ AI æ¥å£
                    verify_results = await self.ai.verify_cluster_batch(batch_requests)
                    
                    if len(verify_results) != len(batch_requests):
                        logger.error(f"      âŒ AI è¿”å›ç»“æœæ•°é‡ä¸åŒ¹é…! è¯·æ±‚: {len(batch_requests)}, è¿”å›: {len(verify_results)}")
                    else:
                        logger.debug(f"      ğŸ¤– AI è¿”å› {len(verify_results)} æ¡æ ¸éªŒç»“æœ")

                    for idx, (req, is_match) in enumerate(zip(batch_requests, verify_results), 1):
                        l_idx = req["leader_idx"]
                        f_idx = req["candidate_idx"]
                        progress_prefix = f"({idx}/{req_count})"

                        if is_match:
                            processed_candidates += len(candidates[f_idx])
                            logger.debug(
                                f"      {progress_prefix} ğŸ”— AIç¡®è®¤: [{pool[l_idx]['obj'].title}] <== [{pool[f_idx]['obj'].title}]"
                            )
                            await self._merge_news(db, pool[l_idx], pool[f_idx])
                            
                            pool[f_idx]["merged_to"] = l_idx
                            merged_indices_in_this_round.append(f_idx)
                            
                            candidates[f_idx] = []
                        else:
                            processed_candidates += 1
                            logger.debug(
                                f"      {progress_prefix} ğŸ›¡ï¸ AIæ‹¦æˆª: [{pool[l_idx]['obj'].title}] vs [{pool[f_idx]['obj'].title}]"
                            )
                            
                            # è®°å½•åˆ°å†å²è¡¨ï¼Œé˜²æ­¢ä¸‹æ¬¡é‡å¤æ ¸éªŒ
                            l_id = pool[l_idx]["id"]
                            f_id = pool[f_idx]["id"]
                            if l_id > f_id:
                                l_id, f_id = f_id, l_id
                            
                            if (l_id, f_id) not in failed_pairs:
                                # ç¡®ä¿ä½¿ç”¨ session ä¸­çš„å¯¹è±¡æ·»åŠ 
                                try:
                                    history_record = ClusteringHistory(news_id_a=l_id, news_id_b=f_id)
                                    db.add(history_record)
                                    # ç«‹å³ flush ç¡®ä¿å†™å…¥ sessionï¼Œå¹¶æ£€æµ‹æ½œåœ¨çº¦æŸå†²çª
                                    await db.flush()
                                    failed_pairs.add((l_id, f_id))
                                except Exception as e:
                                    logger.error(f"      âŒ å†å²è®°å½•å†™å…¥å¤±è´¥ ({l_id}, {f_id}): {e}")

                            if candidates[f_idx]:
                                candidates[f_idx].pop(0)

                # æ¯ä¸€æ‰¹æ¬¡åç«‹å³æäº¤ï¼Œé˜²æ­¢ä»»åŠ¡ä¸­æ–­å¯¼è‡´è¿›åº¦ä¸¢å¤±
                try:
                    await db.commit()
                    if batch_requests or merged_indices_in_this_round:
                        logger.debug("      âœ… æ‰¹æ¬¡æäº¤æˆåŠŸ")
                except Exception as e:
                    logger.error(f"      âŒ æ‰¹æ¬¡æäº¤å¤±è´¥: {e}")
                    await db.rollback()
                    # å›æ»šå†…å­˜çŠ¶æ€
                    for idx in merged_indices_in_this_round:
                        pool[idx]["merged"] = False
                        if "merged_to" in pool[idx]:
                            del pool[idx]["merged_to"]

                processed_in_this_round = True

                if not processed_in_this_round:
                    break

            await db.commit()
            logger.info("âœ… èšç±»å®Œæˆ")

            # ä¸»åŠ¨é‡Šæ”¾å¤§å¯¹è±¡ï¼Œé˜²æ­¢æ®‹ç•™
            del pool
            del candidates
            del items
            gc.collect()

    def _get_active_leader_idx(self, pool: List[Dict], idx: int) -> Optional[int]:
        """é€’å½’æŸ¥æ‰¾å½“å‰èŠ‚ç‚¹è¢«åˆå¹¶åˆ°çš„æœ€ç»ˆ Leader"""
        current = idx
        path = set()
        while pool[current].get("merged"):
            if current in path:
                return None # ç¯è·¯æ£€æµ‹
            path.add(current)
            if "merged_to" not in pool[current]:
                return None # æ— æ³•è¿½è¸ª
            current = pool[current]["merged_to"]
        return current

    async def _merge_news(self, db: AsyncSession, leader: Dict[str, Any], follower: Dict[str, Any]) -> None:
        """
        è¾“å…¥:
        - `db`: æ•°æ®åº“ä¼šè¯
        - `leader`: ä¸»æ–°é—»ï¼ˆåˆå¹¶ç›®æ ‡ï¼‰
        - `follower`: ä»æ–°é—»ï¼ˆè¢«åˆå¹¶å¯¹è±¡ï¼‰

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå¹¶æ¥æºåˆ—è¡¨ä¸çƒ­åº¦ï¼Œå¹¶åˆ é™¤è¢«åˆå¹¶çš„æ–°é—»è®°å½•
        """

        leader_obj = leader["obj"]
        f_obj = follower["obj"]
        follower["merged"] = True

        master_sources = list(leader_obj.sources) if leader_obj.sources else []
        if not master_sources:
            master_sources = [{"name": leader_obj.source, "url": leader_obj.url}]

        seen_urls = set(s["url"] for s in master_sources if "url" in s)

        f_sources = list(f_obj.sources) if f_obj.sources else [{"name": f_obj.source, "url": f_obj.url}]
        for s in f_sources:
            if s.get("url") and s["url"] not in seen_urls:
                master_sources.append(s)
                seen_urls.add(s["url"])

        new_total_heat = 0.0
        for s in master_sources:
            w = self._get_source_weight_by_name(s.get("name", ""))
            new_total_heat += w

        if not leader_obj.summary and f_obj.summary:
            leader_obj.summary = f_obj.summary

        leader_obj.sources = master_sources
        leader_obj.heat_score = new_total_heat

        await db.execute(delete(News).where(News.id == f_obj.id))


cluster_service = ClusteringService(ai_service)

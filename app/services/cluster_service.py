"""
æœ¬æ–‡ä»¶ç”¨äºå®ç°æ–°é—»èšç±»ä¸åˆå¹¶é€»è¾‘ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ä¸å¤§æ¨¡å‹æ ¸éªŒé™ä½é‡å¤æ–°é—»ã€‚
ä¸»è¦ç±»/å¯¹è±¡:
- `ClusteringService`: èšç±»æœåŠ¡å®ç°
- `cluster_service`: å…¨å±€æœåŠ¡å•ä¾‹
"""

import json
import gc
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from sqlalchemy import delete, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import BASE_DIR, get_settings
from app.core.database import AsyncSessionLocal
from app.core.logger import setup_logger
from app.models.news import News
from app.services.ai_service import AIService, ai_service

settings = get_settings()
logger = setup_logger("ClusteringService")


class ClusteringService:
    """
    è¾“å…¥:
    - æ•°æ®åº“ä¸­çš„æ–°é—»å‘é‡ä¸ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®

    è¾“å‡º:
    - èšç±»åˆå¹¶åçš„æ–°é—»è®°å½•ï¼ˆæ›´æ–° sources ä¸ heat_scoreï¼‰

    ä½œç”¨:
    - é€šè¿‡å‘é‡ç›¸ä¼¼åº¦ä¸ AI æ ¸éªŒï¼Œå°†åŒä¸€äº‹ä»¶çš„å¤šæ¥æºæ–°é—»åˆå¹¶
    """

    def __init__(self, ai: AIService) -> None:
        """
        è¾“å…¥:
        - `ai`: AI æœåŠ¡å®ä¾‹

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - æ³¨å…¥ AI èƒ½åŠ›ï¼Œç”¨äºå‘é‡ç”Ÿæˆä¸èšç±»æ ¸éªŒ
        """

        self.ai = ai

    def calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è¾“å…¥:
        - `vec1`: å‘é‡1
        - `vec2`: å‘é‡2

        è¾“å‡º:
        - ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ0~1ï¼‰

        ä½œç”¨:
        - è®¡ç®—ä¸¤æ¡æ–°é—»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œç”¨äºå€™é€‰èšç±»åˆ¤æ–­
        """

        if not vec1 or not vec2:
            return 0.0
        v1, v2 = np.array(vec1), np.array(vec2)
        norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return float(np.dot(v1, v2) / (norm1 * norm2))

    def _get_sources_path_candidates(self) -> List[Path]:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - å¯èƒ½çš„æ–°é—»æºé…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

        ä½œç”¨:
        - è¯»å–æ–°é—»æºæƒé‡æ—¶ï¼Œå…¼å®¹ `data/` ä¸é¡¹ç›®æ ¹ç›®å½•ä¸¤ç§æ”¾ç½®æ–¹å¼
        """

        return [
            BASE_DIR / "data" / "news_sources.json",
            BASE_DIR / "news_sources.json",
        ]

    def _get_source_weight_by_name(self, source_name: str) -> float:
        """
        è¾“å…¥:
        - `source_name`: æ–°é—»æ¥æºåç§°

        è¾“å‡º:
        - æ¥æºæƒé‡ï¼ˆæœªæ‰¾åˆ°è¿”å› 1.0ï¼‰

        ä½œç”¨:
        - ç”¨æ–°é—»æºé…ç½®çš„æƒé‡é‡ç®—åˆå¹¶åçš„æ€»çƒ­åº¦
        """

        try:
            for path in self._get_sources_path_candidates():
                if not path.exists():
                    continue
                with path.open("r", encoding="utf-8") as f:
                    sources: List[Dict] = json.load(f)
                for src in sources:
                    if src.get("name") == source_name:
                        return float(src.get("weight", 1.0))
        except Exception as e:
            logger.error(f"è¯»å–æ–°é—»æºæƒé‡å¤±è´¥: {e}")
        return 1.0

    def _needs_summary_regeneration(self, summary: str) -> bool:
        """
        åˆ¤æ–­æ‘˜è¦æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ (e.g. åŒ…å« HTML æ ‡ç­¾)
        """
        if not summary:
            return False
        # ç®€å•åˆ¤æ–­æ˜¯å¦åŒ…å« HTML æ ‡ç­¾
        return "<" in summary and ">" in summary

    async def execute_clustering(self) -> None:
        """
        è¾“å…¥:
        - æ— 

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - å¯¹æ—¶é—´çª—å£å†…æ–°é—»è¿›è¡Œèšç±»åˆå¹¶ï¼Œå¹¶å†™å›æ•°æ®åº“
        """

        logger.info("ğŸ§© å¼€å§‹æ‰§è¡Œèšç±»ä»»åŠ¡...")
        async with AsyncSessionLocal() as db:
            from datetime import datetime, timedelta

            time_window = datetime.now() - timedelta(hours=settings.CLUSTERING_TIME_WINDOW_HOURS)
            result = await db.execute(
                select(News).where(News.publish_date >= time_window).order_by(News.heat_score.desc())
            )
            items = result.scalars().all()
            if not items:
                return

            items_to_embed = [item for item in items if not item.embedding]
            if items_to_embed:
                logger.info(f"   ğŸ§  è®¡ç®— {len(items_to_embed)} æ¡ç¼ºå¤±å‘é‡...")
                titles = [i.title for i in items_to_embed]
                embeddings = await self.ai.get_embeddings(titles)
                for item, emb in zip(items_to_embed, embeddings):
                    if emb:
                        item.embedding = emb
                await db.commit()

            pool = []
            for item in items:
                if not item.embedding:
                    continue
                pool.append({"id": item.id, "obj": item, "vec": item.embedding, "merged": False})

            logger.info(f"   ğŸ“Š èšç±»æ± å¤§å°: {len(pool)}")

            candidates: Dict[int, List] = {j: [] for j in range(len(pool))}
            total_items = len(pool)
            logger.info(f"   ğŸ” å¼€å§‹é¢„æ‰«æ {total_items} æ¡æ•°æ®...")

            for i in range(len(pool)):
                if i > 0 and i % 500 == 0:
                    logger.debug(f"      [é¢„æ‰«æ] å·²å¤„ç† {i}/{total_items} ...")
                for j in range(i + 1, len(pool)):
                    sim = self.calculate_cosine_similarity(pool[i]["vec"], pool[j]["vec"])
                    if sim >= settings.CLUSTERING_THRESHOLD:
                        candidates[j].append((i, sim))

            logger.info("   âœ… é¢„æ‰«æå®Œæˆ")

            # ------------------------------------------------

            loop_count = 0
            while True:
                loop_count += 1
                if loop_count % 10 == 0:
                    logger.debug(f"      [èšç±»å¾ªç¯] ç¬¬ {loop_count} è½®æ‰«æ...")

                batch_requests = []
                processed_in_this_round = False

                for j in range(len(pool)):
                    follower = pool[j]
                    if follower["merged"]:
                        continue
                    if not candidates[j]:
                        continue

                    leader_idx, sim = candidates[j][0]
                    leader = pool[leader_idx]

                    if leader["merged"]:
                        candidates[j].pop(0)
                        processed_in_this_round = True
                        continue

                    if sim >= 0.98:
                        await self._merge_news(db, leader, follower)
                        candidates[j] = []
                        processed_in_this_round = True
                    else:
                        batch_requests.append(
                            {
                                "leader": leader["obj"].title,
                                "candidate": follower["obj"].title,
                                "leader_idx": leader_idx,
                                "candidate_idx": j,
                            }
                        )
                        # ä½¿ç”¨é…ç½®çš„ batch size
                        if len(batch_requests) >= settings.ANALYSIS_BATCH_SIZE:
                            break

                if not batch_requests and not processed_in_this_round:
                    break

                if batch_requests:
                    req_count = len(batch_requests)
                    logger.info(f"   ğŸ¤– æ‰¹é‡æ ¸éªŒ {req_count} å¯¹...")
                    verify_results = await self.ai.verify_cluster_batch(batch_requests)

                    for idx, (req, is_match) in enumerate(zip(batch_requests, verify_results), 1):
                        l_idx = req["leader_idx"]
                        f_idx = req["candidate_idx"]
                        progress_prefix = f"({idx}/{req_count})"

                        if is_match:
                            logger.debug(
                                f"      {progress_prefix} ğŸ”— AIç¡®è®¤: [{pool[l_idx]['obj'].title}] <== [{pool[f_idx]['obj'].title}]"
                            )
                            await self._merge_news(db, pool[l_idx], pool[f_idx])
                            candidates[f_idx] = []
                        else:
                            logger.debug(
                                f"      {progress_prefix} ğŸ›¡ï¸ AIæ‹¦æˆª: [{pool[l_idx]['obj'].title}] vs [{pool[f_idx]['obj'].title}]"
                            )
                            if candidates[f_idx]:
                                candidates[f_idx].pop(0)

                    processed_in_this_round = True

                if not processed_in_this_round:
                    break

            await db.commit()
            logger.info("âœ… èšç±»å®Œæˆ")

            # ä¸»åŠ¨é‡Šæ”¾å¤§å¯¹è±¡ï¼Œé˜²æ­¢æ®‹ç•™
            del pool
            del candidates
            del items
            gc.collect()

    async def _merge_news(self, db: AsyncSession, leader: Dict[str, Any], follower: Dict[str, Any]) -> None:
        """
        è¾“å…¥:
        - `db`: æ•°æ®åº“ä¼šè¯
        - `leader`: ä¸»æ–°é—»ï¼ˆåˆå¹¶ç›®æ ‡ï¼‰
        - `follower`: ä»æ–°é—»ï¼ˆè¢«åˆå¹¶å¯¹è±¡ï¼‰

        è¾“å‡º:
        - æ— 

        ä½œç”¨:
        - åˆå¹¶æ¥æºåˆ—è¡¨ä¸çƒ­åº¦ï¼Œå¹¶åˆ é™¤è¢«åˆå¹¶çš„æ–°é—»è®°å½•
        """

        leader_obj = leader["obj"]
        f_obj = follower["obj"]
        follower["merged"] = True

        master_sources = list(leader_obj.sources) if leader_obj.sources else []
        if not master_sources:
            master_sources = [{"name": leader_obj.source, "url": leader_obj.url}]

        seen_urls = set(s["url"] for s in master_sources if "url" in s)

        f_sources = list(f_obj.sources) if f_obj.sources else [{"name": f_obj.source, "url": f_obj.url}]
        for s in f_sources:
            if s.get("url") and s["url"] not in seen_urls:
                master_sources.append(s)
                seen_urls.add(s["url"])

        new_total_heat = 0.0
        for s in master_sources:
            w = self._get_source_weight_by_name(s.get("name", ""))
            new_total_heat += w

        if not leader_obj.summary and f_obj.summary:
            leader_obj.summary = f_obj.summary

        leader_obj.sources = master_sources
        leader_obj.heat_score = new_total_heat

        await db.execute(delete(News).where(News.id == f_obj.id))


cluster_service = ClusteringService(ai_service)

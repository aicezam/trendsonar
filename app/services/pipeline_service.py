"""
æœ¬æ–‡ä»¶ç”¨äºç¼–æ’æŠ“å–ã€èšç±»ã€æ‘˜è¦ä¸æŠ¥è¡¨ç”Ÿæˆç­‰å…¨æµç¨‹ä»»åŠ¡ï¼Œå¹¶æä¾›å®šæ—¶è°ƒåº¦å…¥å£ã€‚
ä¸»è¦å‡½æ•°:
- `scheduled_task`: å®šæ—¶è°ƒåº¦å¾ªç¯
- `run_manual`: æ‰‹åŠ¨è§¦å‘å…¨æµç¨‹
"""

import asyncio
import gc
from datetime import datetime, time, timedelta
from typing import Dict, List, Optional

import numpy as np
from sqlalchemy import delete, desc, or_, select

from app.core.config import get_settings
from app.core.database import AsyncSessionLocal, check_db_connection
from app.core.logger import logger
from app.core.exceptions import AIConfigurationError
from app.models.news import News
from app.services.ai_service import ai_service
from app.services.cluster_service import cluster_service
from app.services.crawler_service import crawler_service
from app.services.report_service import report_service
from app.services.topic_service import topic_service
from app.utils.tools import normalize_regions_to_countries

settings = get_settings()


async def auto_batch_analyze_new_news() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - å¯¹æ—¶é—´çª—å£å†…å°šæœªåˆ†ç±»çš„æ–°é—»è¿›è¡Œæ‰¹é‡æƒ…æ„Ÿä¸åˆ†ç±»åˆ†æ
    """

    logger.info("ğŸ¤– å¼€å§‹æ‰¹é‡åˆæ­¥åˆ†ææ–°æ–°é—»...")
    async with AsyncSessionLocal() as db:
        time_window = datetime.now() - timedelta(hours=settings.CLUSTERING_TIME_WINDOW_HOURS)
        stmt = select(News).where(News.publish_date >= time_window, News.category == "å…¶ä»–").order_by(News.id.desc())

        result = await db.execute(stmt)
        news_list = result.scalars().all()

        if not news_list:
            logger.info("âœ… æ²¡æœ‰å¾…å¤„ç†çš„æ–°é—»")
            return

        total = len(news_list)
        logger.debug(f"   ğŸ“Š å¾…åˆ†ææ–°é—»æ•°: {total}")

        batch_size = settings.ANALYSIS_BATCH_SIZE
        processed_count = 0

        for i in range(0, total, batch_size):
            batch = news_list[i : i + batch_size]
            batch_data = [{"id": n.id, "title": n.title} for n in batch]

            logger.debug(f"   ğŸš€ æ­£åœ¨åˆ†ææ‰¹æ¬¡ {i // batch_size + 1} (å¤§å°: {len(batch)})...")
            results = await ai_service.batch_analyze_sentiment(batch_data)

            updates = 0
            for news in batch:
                if news.id in results:
                    res = results[news.id]
                    news.sentiment_label = res.get("label", "ä¸­ç«‹")
                    news.sentiment_score = res.get("score", 50)
                    news.category = res.get("category", "å…¶ä»–")
                    news.region = normalize_regions_to_countries(res.get("region", "å…¶ä»–"))
                    updates += 1

            await db.commit()
            processed_count += updates

        logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±æ›´æ–° {processed_count} æ¡")


async def auto_generate_summaries_top_n() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - ä¸ºå½“æ—¥çƒ­åº¦ TopN æ–°é—»ç”Ÿæˆæ‘˜è¦ï¼Œå¹¶å°½é‡è¡¥å…¨å‘é‡ä¸æ·±åº¦åˆ†æå­—æ®µ
    """

    top_n = settings.AUTO_SUMMARY_TOP_N
    logger.info(f"ğŸ¤– å¼€å§‹ä¸ºä»Šæ—¥çƒ­åº¦Top{top_n}è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦...")
    async with AsyncSessionLocal() as db:
        today_start = datetime.combine(datetime.now().date(), time.min)

        stmt = (
            select(News).where(News.publish_date >= today_start).order_by(desc(News.heat_score)).limit(top_n)
        )
        result = await db.execute(stmt)
        top_news = result.scalars().all()

        # ç­›é€‰å‡ºå°šæœªç»è¿‡ AI æ‘˜è¦ç”Ÿæˆçš„æ–°é—» (is_ai_summary == False)
        # æ³¨æ„ï¼šå³ä¾¿ news.summary æœ‰å€¼ï¼ˆRSSè‡ªå¸¦æ‘˜è¦ï¼‰ï¼Œåªè¦ is_ai_summary ä¸º Falseï¼Œä¹Ÿéœ€è¦é‡æ–°ç”Ÿæˆ
        news_to_process = [n for n in top_news if not n.is_ai_summary]
        total_task = len(news_to_process)
        logger.debug(f"   ğŸ“‹ éœ€ç”Ÿæˆæ‘˜è¦: {total_task} æ¡")

        count = 0
        for idx, news in enumerate(news_to_process, 1):
            progress_str = f"({idx}/{total_task})"
            try:
                content = news.content
                if not content or len(content) < 50:
                    logger.debug(f"   {progress_str} ğŸ•·ï¸ è¡¥æŠ“æ­£æ–‡: {news.title}")
                    content = await crawler_service.crawl_content(news.url)
                    if content:
                        news.content = content
                        db.add(news)
                        await db.commit()
                    else:
                        logger.warning(f"   {progress_str} âŒ æ— æ³•è·å–æ­£æ–‡ï¼Œè·³è¿‡: {news.title}")
                        continue

                if content:
                    logger.debug(f"   {progress_str} ğŸ“ ç”Ÿæˆæ‘˜è¦: {news.title}")
                    
                    # ç»„åˆè¾“å…¥ï¼šå¦‚æœæœ‰åŸå§‹æ‘˜è¦ï¼ˆRSSï¼‰ï¼Œåˆ™ä¸€èµ·æä¾›ç»™ AI
                    input_content = content
                    if news.summary:
                        input_content = f"åŸå§‹æ‘˜è¦ï¼š{news.summary}\n\næ­£æ–‡å†…å®¹ï¼š{content}"

                    summary = await ai_service.generate_summary(news.title, input_content)
                    if summary:
                        news.summary = summary
                        news.is_ai_summary = True

                        try:
                            txt_to_embed = f"{news.title} {summary} {content[:1000]}"
                            embs = await ai_service.get_embeddings([txt_to_embed])
                            if embs and embs[0]:
                                news.embedding = embs[0]
                        except Exception as e:
                            logger.error(f"   {progress_str} âš ï¸ å‘é‡æ›´æ–°å¤±è´¥: {e}")

                        if not news.keywords:
                            try:
                                logger.debug(f"   {progress_str} ğŸ§  åŒæ­¥æ·±åº¦åˆ†æ: {news.title}")
                                res = await ai_service.analyze_sentiment(news.title, summary)
                                if res:
                                    news.sentiment_score = res["score"]
                                    news.sentiment_label = res["label"]
                                    news.category = res.get("category", "å…¶ä»–")
                                    news.region = res.get("region", "å…¶ä»–")
                                    news.keywords = res["keywords"]
                                    news.entities = res["entities"]
                            except Exception as e:
                                logger.error(f"   {progress_str} âš ï¸ åŒæ­¥åˆ†æå¤±è´¥: {e}")

                        db.add(news)
                        await db.commit()
                        count += 1
            except Exception as e:
                logger.error(f"   {progress_str} âš ï¸ å¤„ç†å¼‚å¸¸ ({news.title}): {e}")

        logger.info(f"âœ… è‡ªåŠ¨æ‘˜è¦å®Œæˆï¼Œå…±å¤„ç† {count} æ¡")


async def auto_analyze_sentiment_top_n() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - ä¸ºå½“æ—¥çƒ­åº¦ TopN æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æï¼ˆæƒ…æ„Ÿ/å…³é”®è¯/å®ä½“/åœ°åŒºï¼‰
    """

    top_n = settings.AUTO_ANALYSIS_TOP_N
    logger.info(f"ğŸ§  å¼€å§‹ä¸ºä»Šæ—¥çƒ­åº¦Top{top_n}è¿›è¡Œæ·±åº¦åˆ†æ...")
    async with AsyncSessionLocal() as db:
        today_start = datetime.combine(datetime.now().date(), time.min)

        stmt = (
            select(News).where(News.publish_date >= today_start).order_by(desc(News.heat_score)).limit(top_n)
        )
        result = await db.execute(stmt)
        top_news = result.scalars().all()

        items_to_process = []
        for news in top_news:
            if not news.keywords or len(news.keywords) == 0:
                items_to_process.append(news)

        if not items_to_process:
            logger.info("âœ… æ‰€æœ‰Topæ–°é—»å‡å·²åˆ†æï¼Œæ— éœ€å¤„ç†")
            return

        logger.debug(f"   ğŸ“Š å¾…åˆ†ææ–°é—»æ•°: {len(items_to_process)}")

        sem = asyncio.Semaphore(5)
        total_items = len(items_to_process)

        async def analyze_task(news_item, index):
            async with sem:
                try:
                    if not news_item.content or len(news_item.content) < 50:
                        logger.debug(f"   ({index}/{total_items}) ğŸ•·ï¸ è¡¥æŠ“æ­£æ–‡: {news_item.title}")
                        try:
                            content = await crawler_service.crawl_content(news_item.url)
                            if content:
                                news_item.content = content
                        except Exception as e:
                            logger.error(f"   ({index}/{total_items}) âš ï¸ è¡¥æŠ“å¤±è´¥: {e}")

                    text = news_item.summary or news_item.content or ""
                    logger.debug(f"   ({index}/{total_items}) ğŸ§  åˆ†æä¸­: {news_item.title}")
                    return await ai_service.analyze_sentiment(news_item.title, text)
                except Exception as e:
                    logger.error(f"   ({index}/{total_items}) âš ï¸ åˆ†æå¤±è´¥ ({news_item.title}): {e}")
                    return None

        tasks = [analyze_task(n, i + 1) for i, n in enumerate(items_to_process)]
        results = await asyncio.gather(*tasks)

        count = 0
        for news, res in zip(items_to_process, results):
            if res:
                news.sentiment_score = res["score"]
                news.sentiment_label = res["label"]
                news.category = res.get("category", "å…¶ä»–")
                news.keywords = res["keywords"]
                news.entities = res["entities"]
                db.add(news)
                count += 1

        await db.commit()
        logger.info(f"âœ… æ·±åº¦åˆ†æå®Œæˆï¼Œå…±æ›´æ–° {count} æ¡")


async def cleanup_old_data() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - æ¸…ç†è¿‡æœŸä¸”ä½çƒ­åº¦çš„æ•°æ®ï¼Œæ§åˆ¶æ•°æ®åº“ä½“é‡
    """

    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸæ•°æ®...")
    async with AsyncSessionLocal() as db:
        deadline = datetime.now() - timedelta(days=3)
        stmt = delete(News).where(News.publish_date < deadline, News.heat_score < 1.0)
        result = await db.execute(stmt)
        await db.commit()
        logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ {result.rowcount} æ¡è¿‡æœŸä½çƒ­åº¦æ•°æ®")


async def run_pipeline_task(generate_daily: bool = True, run_topic_task: bool = True) -> None:
    """
    è¾“å…¥:
    - `generate_daily`: æ˜¯å¦åœ¨æµç¨‹ä¸­ç”Ÿæˆæ¯æ—¥å¤§ç›˜æŠ¥è¡¨
    - `run_topic_task`: æ˜¯å¦åœ¨æµç¨‹ä¸­è¿è¡Œä¸“é¢˜è¿½è¸ªä»»åŠ¡

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - ä¸²è”æŠ“å–ã€å…¥åº“ã€èšç±»ã€åˆ†æã€æ‘˜è¦ä¸æ¸…ç†çš„å…¨æµç¨‹ä»»åŠ¡
    """

    try:
        logger.info(f"ğŸš€ å¼€å§‹æ–°ä¸€è½®å…¨æµç¨‹ä»»åŠ¡ (generate_daily={generate_daily}, run_topic_task={run_topic_task})...")
        news_items = await crawler_service.fetch_all_sources()
        await crawler_service.save_raw_news(news_items)

        await cluster_service.execute_clustering()

        await auto_batch_analyze_new_news()

        await auto_generate_summaries_top_n()

        await auto_analyze_sentiment_top_n()

        if generate_daily:
            await report_service.generate_and_cache_global_report("daily")

        if run_topic_task:
            try:
                await topic_service.refresh_topics()
            except AIConfigurationError:
                raise
            except Exception as e:
                logger.error(f"âŒ ä¸“é¢˜åˆ·æ–°å¼‚å¸¸: {e}")
        else:
            logger.info("â© è·³è¿‡ä¸“é¢˜åˆ·æ–° (æœªåˆ°é…ç½®çš„æ—¶é—´é—´éš”)")

        await cleanup_old_data()

        logger.info("âœ… æœ¬è½®å…¨æµç¨‹ä»»åŠ¡ç»“æŸ")
    except AIConfigurationError:
        raise
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
    finally:
        gc.collect()


async def scheduled_task() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - å®šæ—¶è°ƒåº¦å…¥å£ï¼šæŒ‰å›ºå®šé—´éš”è¿è¡Œå…¨æµç¨‹ï¼Œå¹¶åœ¨ç‰¹å®šæ—¶åˆ»ç”Ÿæˆæ—¥æŠ¥/å‘¨æŠ¥/æœˆæŠ¥
    """

    logger.info("â° å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨...")

    last_periodic_run = datetime.min
    last_topic_run = datetime.min
    last_daily_final = None
    last_weekly_final = None
    last_monthly_final = None

    while True:
        try:
            if not await check_db_connection():
                logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥å¼‚å¸¸ï¼Œå®šæ—¶ä»»åŠ¡æš‚åœè¿è¡Œï¼Œç­‰å¾…æ¢å¤...")
                await asyncio.sleep(60)
                continue

            if not (settings.DATABASE_URL or "").strip():
                logger.warning("âš ï¸ æœªé…ç½® DATABASE_URLï¼Œå®šæ—¶ä»»åŠ¡æš‚åœè¿è¡Œ")
                await asyncio.sleep(60)
                continue
            now = datetime.now()

            interval_seconds = settings.SCHEDULE_INTERVAL_MINUTES * 60
            if (now - last_periodic_run).total_seconds() >= interval_seconds:
                # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿è¡Œä¸“é¢˜ä»»åŠ¡
                topic_interval_seconds = settings.TOPIC_SCHEDULE_INTERVAL_HOURS * 3600
                should_run_topics = (now - last_topic_run).total_seconds() >= topic_interval_seconds
                
                await run_pipeline_task(generate_daily=True, run_topic_task=should_run_topics)
                
                last_periodic_run = datetime.now()
                if should_run_topics:
                    last_topic_run = datetime.now()

            if now.hour == 23 and now.minute == 58:
                if last_daily_final != now.date():
                    logger.info("â° [Schedule] è§¦å‘æ¯æ—¥æœ€ç»ˆæŠ¥è¡¨ (23:58)...")
                    await report_service.generate_and_cache_global_report("daily")
                    last_daily_final = now.date()
                    gc.collect()

            if now.weekday() == 6 and now.hour == 23 and now.minute == 55:
                if last_weekly_final != now.date():
                    logger.info("â° [Schedule] è§¦å‘æ¯å‘¨æœ€ç»ˆæŠ¥è¡¨ (å‘¨æ—¥ 23:55)...")
                    await report_service.generate_and_cache_global_report("weekly")
                    last_weekly_final = now.date()
                    gc.collect()

            tomorrow = now + timedelta(days=1)
            if tomorrow.day == 1 and now.hour == 23 and now.minute == 50:
                if last_monthly_final != now.date():
                    logger.info("â° [Schedule] è§¦å‘æ¯æœˆæœ€ç»ˆæŠ¥è¡¨ (æœˆæœ« 23:50)...")
                    await report_service.generate_and_cache_global_report("monthly")
                    last_monthly_final = now.date()
                    gc.collect()

        except AIConfigurationError as e:
            logger.error(f"ğŸ›‘ é…ç½®é”™è¯¯: {e} è¯·æ£€æŸ¥ config.yaml æ˜¯å¦é…ç½®æ­£ç¡®")
            logger.warning("âš ï¸ ç³»ç»Ÿå°†è¿›å…¥ç»´æŠ¤æ¨¡å¼ï¼Œæ¯ 5 åˆ†é’Ÿè‡ªåŠ¨é‡å¯æœåŠ¡æ£€æŸ¥ä¸€æ¬¡...")
            await asyncio.sleep(300)
            
            # é‡æ–°åŠ è½½é…ç½®
            from app.core.config import reload_settings
            reload_settings()
            
            # é‡æ–°åŠ è½½ AI æœåŠ¡ä¸­çš„é…ç½®å¼•ç”¨
            from app.services.ai_service import ai_service
            ai_service.reload_config()
            
            logger.info("ğŸ”„ é…ç½®å·²å°è¯•é‡æ–°åŠ è½½")
            
            continue

        except Exception as e:
            logger.error(f"âŒ è°ƒåº¦å¾ªç¯å¼‚å¸¸: {e}")

        await asyncio.sleep(30)


async def run_manual() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æŠ“å–ä¸åˆ†æä»»åŠ¡ï¼Œå¹¶è¡¥é½æ—¥æŠ¥/å‘¨æŠ¥/æœˆæŠ¥ç¼“å­˜
    """

    logger.info("ğŸš€ æ‰‹åŠ¨ä»»åŠ¡å¼€å§‹...")
    try:
        items = await crawler_service.fetch_all_sources()
        await crawler_service.save_raw_news(items)
        await cluster_service.execute_clustering()
        await auto_generate_summaries_top_n()
        await auto_analyze_sentiment_top_n()

        await report_service.generate_and_cache_global_report("daily")
        await report_service.generate_and_cache_global_report("weekly")
        await report_service.generate_and_cache_global_report("monthly")

        logger.info("âœ… æ‰‹åŠ¨ä»»åŠ¡ç»“æŸ")
    finally:
        try:
            await topic_service.refresh_topics()
        except Exception as e:
            logger.error(f"âŒ ä¸“é¢˜åˆ·æ–°å¼‚å¸¸: {e}")
        
        gc.collect()


async def reanalyze_all_categories() -> Dict:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - ä»»åŠ¡çŠ¶æ€ä¸æˆåŠŸæ›´æ–°æ¡æ•°

    ä½œç”¨:
    - å¯¹å…¨é‡æ–°é—»é€æ¡è°ƒç”¨ AI è¿›è¡Œé‡æ–°åˆ†æï¼Œç”¨äºä¿®å¤å†å²åˆ†ç±»æˆ–ç­–ç•¥è°ƒæ•´
    """

    logger.info("ğŸ”„ å¼€å§‹å…¨é‡æ•°æ®é‡æ–°åˆ†æä»»åŠ¡...")

    async with AsyncSessionLocal() as db:
        result = await db.execute(select(News.id))
        all_ids = result.scalars().all()

        logger.info(f"   ğŸ“Š å¾…å¤„ç†æ–°é—»æ€»æ•°: {len(all_ids)}")

        sem = asyncio.Semaphore(5)

        async def analyze_task(news_item):
            async with sem:
                try:
                    text = news_item.summary or news_item.content or ""
                    res = await ai_service.analyze_sentiment(news_item.title, text)
                    if res:
                        news_item.sentiment_score = res["score"]
                        news_item.sentiment_label = res["label"]
                        news_item.category = res.get("category", "å…¶ä»–")
                        news_item.keywords = res["keywords"]
                        news_item.entities = res["entities"]
                        return True
                except Exception as e:
                    logger.error(f"   âš ï¸ åˆ†æå¤±è´¥ ({news_item.title}): {e}")
                return False

        tasks = []
        batch_size = 50
        count = 0

        for i in range(0, len(all_ids), batch_size):
            batch_ids = all_ids[i : i + batch_size]

            async with AsyncSessionLocal() as db:
                result = await db.execute(select(News).where(News.id.in_(batch_ids)))
                current_batch_news = result.scalars().all()

                batch_tasks = [analyze_task(n) for n in current_batch_news]
                results = await asyncio.gather(*batch_tasks)

                try:
                    await db.commit()
                    success_count = sum(1 for r in results if r)
                    count += success_count
                    logger.debug(f"   å¤„ç†æ‰¹æ¬¡ {i} - {i + batch_size}ï¼ŒæˆåŠŸ: {success_count}")
                except Exception as e:
                    logger.error(f"   âŒ æ‰¹æ¬¡æäº¤å¤±è´¥: {e}")
                    await db.rollback()
                
                # ä¸»åŠ¨å›æ”¶å†…å­˜
                del current_batch_news
                del results
                gc.collect()

        logger.info(f"âœ… å…¨é‡é‡åˆ†æå®Œæˆï¼Œå…±æ›´æ–° {count} æ¡")
        return {"status": "finished", "count": count}


async def background_analyze_all() -> None:
    """
    è¾“å…¥:
    - æ— 

    è¾“å‡º:
    - æ— 

    ä½œç”¨:
    - ä»¥æ‰¹å¤„ç†æ–¹å¼è¡¥å…¨å†å²æ–°é—»çš„æƒ…æ„Ÿä¸å…³é”®è¯ï¼Œå¹¶åœ¨ç»“æŸååˆ·æ–°æŠ¥è¡¨ç¼“å­˜
    """

    logger.info("ğŸš€ å¼€å§‹å…¨é‡æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼ˆå¤„ç†æ‰€æœ‰æœªåˆ†æçš„å†å²æ•°æ®ï¼‰...")

    total_processed = 0
    batch_size = 50

    while True:
        processed_in_batch = 0
        async with AsyncSessionLocal() as db:
            stmt = select(News).where(News.keywords.is_(None)).limit(batch_size)
            result = await db.execute(stmt)
            items = result.scalars().all()

            if not items:
                stmt = select(News).limit(batch_size * 5)
                result = await db.execute(stmt)
                all_candidates = result.scalars().all()
                items = [
                    n for n in all_candidates if not n.keywords or n.keywords == [] or n.keywords == "[]"
                ][:batch_size]

            if not items:
                logger.info("   âš ï¸ æœªå‘ç°æ›´å¤šå¾…åˆ†ææ•°æ®")
                break

            logger.info(f"   ğŸ“¦ æœ¬æ‰¹æ¬¡å¤„ç† {len(items)} æ¡...")

            sem = asyncio.Semaphore(10)

            async def analyze_task(news_item):
                async with sem:
                    try:
                        text = news_item.summary or news_item.content or ""
                        if len(text) < 10:
                            return {
                                "score": 50,
                                "label": "ä¸­ç«‹",
                                "category": "å…¶ä»–",
                                "keywords": ["æ— å†…å®¹"],
                                "entities": [],
                            }
                        return await ai_service.analyze_sentiment(news_item.title, text)
                    except Exception as e:
                        logger.error(f"   âš ï¸ åˆ†æå¤±è´¥ ({news_item.id}): {e}")
                        return None

            tasks = [analyze_task(n) for n in items]
            results = await asyncio.gather(*tasks)

            for news, res in zip(items, results):
                if res:
                    news.sentiment_score = res["score"]
                    news.sentiment_label = res["label"]
                    news.category = res.get("category", "å…¶ä»–")
                    news.keywords = res["keywords"]
                    news.entities = res["entities"]
                    db.add(news)
                    processed_in_batch += 1
                else:
                    news.keywords = ["åˆ†æå¤±è´¥"]
                    db.add(news)

            await db.commit()
            
            # ä¸»åŠ¨å›æ”¶å†…å­˜
            del items
            del results
            gc.collect()
            total_processed += processed_in_batch
            logger.info(f"   âœ… å·²æ›´æ–° {processed_in_batch} æ¡ï¼Œç´¯è®¡ {total_processed} æ¡")

        await asyncio.sleep(1)

    await report_service.generate_and_cache_global_report("daily")
    await report_service.generate_and_cache_global_report("weekly")
    await report_service.generate_and_cache_global_report("monthly")
    logger.info(f"ğŸ‰ å…¨é‡åˆ†æä»»åŠ¡ç»“æŸï¼Œå…±å¤„ç† {total_processed} æ¡")
